{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as Data\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import imageio\n",
        "from torch import nn\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "EWMkx-ozuwH-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.unsqueeze(torch.linspace(-3, 3, 100), dim=1) \n",
        "y = torch.sin(x)"
      ],
      "metadata": {
        "id": "fB1Ug-jouxvb"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def snake(x):\n",
        "  a=3\n",
        "  #return torch.sin(a*x)\n",
        "  return  (1/a)*torch.pow(torch.sin(a*x), 2)\n",
        "  #return 1/(1+torch.exp(-x))\n",
        "\n",
        "def snake_grad(x):\n",
        "  a=3\n",
        "  #return a*torch.cos(a*x) \n",
        "  return 2* torch.sin(a*x)*torch.cos(a*x)\n",
        "  #return sigmoid(x)*(1- sigmoid(x))\n",
        "\n",
        "def neural_network(x,weights,bias):\n",
        "    s_z = snake(torch.matmul(x,weights[0])+bias[0])\n",
        "    return torch.matmul(s_z, weights[1]) + bias[1]\n",
        "\n",
        "def dN_dx(weights, x):\n",
        "    s_z_grad = snake_grad(torch.matmul(x,weights[0])+bias[0]) #100x10\n",
        "    mul=torch.mul(weights[0].T,weights[1])\n",
        "    return torch.matmul(s_z_grad,mul)"
      ],
      "metadata": {
        "id": "hMcLcemeuzoa"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def error(pred,target): return ((pred-target)**2).mean() "
      ],
      "metadata": {
        "id": "2z61R0mku6hK"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize weights and biases\n",
        "weights = [torch.randn((1,20),  requires_grad=True), torch.randn((20,1),  requires_grad=True)]\n",
        "bias = [torch.randn(20,  requires_grad=True), torch.randn(1,  requires_grad=True)]"
      ],
      "metadata": {
        "id": "VSklBn59vAMs"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training(x,epochs,lr):\n",
        "  for i in range(epochs):\n",
        "\n",
        "    pred = neural_network(x,weights,bias)\n",
        "    loss = error(pred,y)\n",
        "    loss.backward()\n",
        "    weights[0].data -= lr*weights[0].grad.data\n",
        "    weights[1].data -= lr*weights[1].grad.data\n",
        "    bias[0].data -= lr*bias[0].grad.data\n",
        "    bias[1].data -= lr*bias[1].grad.data\n",
        "\n",
        " \n",
        "    weights[0].grad.zero_()\n",
        "    weights[1].grad.zero_()\n",
        "    bias[0].grad.zero_()\n",
        "    bias[1].grad.zero_()\n",
        "\n",
        "    print(\"Loss: \", loss.item())\n"
      ],
      "metadata": {
        "id": "VPCobxVJ-ma_"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " training(x,5000,0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQyYaFhE-z1M",
        "outputId": "9f5f3aa8-1273-43f3-d6d7-d209348302b6"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss:  7.200637340545654\n",
            "Loss:  6.758741855621338\n",
            "Loss:  6.3449506759643555\n",
            "Loss:  5.958456039428711\n",
            "Loss:  5.598080635070801\n",
            "Loss:  5.262348175048828\n",
            "Loss:  4.949708461761475\n",
            "Loss:  4.658670425415039\n",
            "Loss:  4.387819766998291\n",
            "Loss:  4.135769367218018\n",
            "Loss:  3.9011354446411133\n",
            "Loss:  3.6825592517852783\n",
            "Loss:  3.4787466526031494\n",
            "Loss:  3.288511276245117\n",
            "Loss:  3.1108007431030273\n",
            "Loss:  2.944699764251709\n",
            "Loss:  2.7894182205200195\n",
            "Loss:  2.644266128540039\n",
            "Loss:  2.508620500564575\n",
            "Loss:  2.3819005489349365\n",
            "Loss:  2.263543128967285\n",
            "Loss:  2.152998685836792\n",
            "Loss:  2.0497264862060547\n",
            "Loss:  1.9532058238983154\n",
            "Loss:  1.862938404083252\n",
            "Loss:  1.7784576416015625\n",
            "Loss:  1.6993321180343628\n",
            "Loss:  1.6251660585403442\n",
            "Loss:  1.555597186088562\n",
            "Loss:  1.4902958869934082\n",
            "Loss:  1.4289610385894775\n",
            "Loss:  1.371317744255066\n",
            "Loss:  1.317113995552063\n",
            "Loss:  1.2661184072494507\n",
            "Loss:  1.2181174755096436\n",
            "Loss:  1.172914981842041\n",
            "Loss:  1.1303285360336304\n",
            "Loss:  1.0901896953582764\n",
            "Loss:  1.052342414855957\n",
            "Loss:  1.0166420936584473\n",
            "Loss:  0.9829549193382263\n",
            "Loss:  0.9511558413505554\n",
            "Loss:  0.9211291670799255\n",
            "Loss:  0.892768144607544\n",
            "Loss:  0.8659732937812805\n",
            "Loss:  0.8406524658203125\n",
            "Loss:  0.8167198300361633\n",
            "Loss:  0.7940962314605713\n",
            "Loss:  0.7727075219154358\n",
            "Loss:  0.7524853348731995\n",
            "Loss:  0.7333654761314392\n",
            "Loss:  0.7152881026268005\n",
            "Loss:  0.6981974840164185\n",
            "Loss:  0.6820414066314697\n",
            "Loss:  0.6667705774307251\n",
            "Loss:  0.6523386240005493\n",
            "Loss:  0.6387019753456116\n",
            "Loss:  0.6258190870285034\n",
            "Loss:  0.6136510968208313\n",
            "Loss:  0.6021605134010315\n",
            "Loss:  0.5913116335868835\n",
            "Loss:  0.5810712575912476\n",
            "Loss:  0.5714064240455627\n",
            "Loss:  0.5622865557670593\n",
            "Loss:  0.5536822080612183\n",
            "Loss:  0.5455650091171265\n",
            "Loss:  0.5379079580307007\n",
            "Loss:  0.5306853652000427\n",
            "Loss:  0.5238727331161499\n",
            "Loss:  0.5174465775489807\n",
            "Loss:  0.511384129524231\n",
            "Loss:  0.5056642889976501\n",
            "Loss:  0.5002668499946594\n",
            "Loss:  0.4951726794242859\n",
            "Loss:  0.49036338925361633\n",
            "Loss:  0.4858216345310211\n",
            "Loss:  0.481531023979187\n",
            "Loss:  0.47747620940208435\n",
            "Loss:  0.4736425280570984\n",
            "Loss:  0.47001636028289795\n",
            "Loss:  0.46658459305763245\n",
            "Loss:  0.46333518624305725\n",
            "Loss:  0.4602564573287964\n",
            "Loss:  0.45733803510665894\n",
            "Loss:  0.45456957817077637\n",
            "Loss:  0.45194151997566223\n",
            "Loss:  0.4494454264640808\n",
            "Loss:  0.4470725655555725\n",
            "Loss:  0.4448155164718628\n",
            "Loss:  0.4426668584346771\n",
            "Loss:  0.44061988592147827\n",
            "Loss:  0.43866822123527527\n",
            "Loss:  0.4368058741092682\n",
            "Loss:  0.43502748012542725\n",
            "Loss:  0.4333278238773346\n",
            "Loss:  0.43170201778411865\n",
            "Loss:  0.43014541268348694\n",
            "Loss:  0.4286540150642395\n",
            "Loss:  0.42722374200820923\n",
            "Loss:  0.42585110664367676\n",
            "Loss:  0.42453238368034363\n",
            "Loss:  0.4232645332813263\n",
            "Loss:  0.4220444858074188\n",
            "Loss:  0.420869380235672\n",
            "Loss:  0.4197366237640381\n",
            "Loss:  0.4186439514160156\n",
            "Loss:  0.4175886809825897\n",
            "Loss:  0.4165688455104828\n",
            "Loss:  0.4155822992324829\n",
            "Loss:  0.41462722420692444\n",
            "Loss:  0.4137018918991089\n",
            "Loss:  0.4128044843673706\n",
            "Loss:  0.4119336009025574\n",
            "Loss:  0.4110877215862274\n",
            "Loss:  0.41026535630226135\n",
            "Loss:  0.4094651937484741\n",
            "Loss:  0.4086860716342926\n",
            "Loss:  0.4079267978668213\n",
            "Loss:  0.407186359167099\n",
            "Loss:  0.40646377205848694\n",
            "Loss:  0.405757874250412\n",
            "Loss:  0.405068039894104\n",
            "Loss:  0.4043932259082794\n",
            "Loss:  0.4037327468395233\n",
            "Loss:  0.4030855894088745\n",
            "Loss:  0.40245139598846436\n",
            "Loss:  0.40182918310165405\n",
            "Loss:  0.401218444108963\n",
            "Loss:  0.4006185829639435\n",
            "Loss:  0.40002909302711487\n",
            "Loss:  0.3994493782520294\n",
            "Loss:  0.3988787531852722\n",
            "Loss:  0.3983170986175537\n",
            "Loss:  0.3977636694908142\n",
            "Loss:  0.39721810817718506\n",
            "Loss:  0.39667999744415283\n",
            "Loss:  0.39614906907081604\n",
            "Loss:  0.39562466740608215\n",
            "Loss:  0.39510658383369446\n",
            "Loss:  0.3945949077606201\n",
            "Loss:  0.3940887153148651\n",
            "Loss:  0.39358794689178467\n",
            "Loss:  0.3930922746658325\n",
            "Loss:  0.39260154962539673\n",
            "Loss:  0.39211541414260864\n",
            "Loss:  0.3916335701942444\n",
            "Loss:  0.3911559581756592\n",
            "Loss:  0.3906821310520172\n",
            "Loss:  0.3902120888233185\n",
            "Loss:  0.3897455334663391\n",
            "Loss:  0.3892822265625\n",
            "Loss:  0.38882210850715637\n",
            "Loss:  0.3883648216724396\n",
            "Loss:  0.387910395860672\n",
            "Loss:  0.3874584436416626\n",
            "Loss:  0.3870092034339905\n",
            "Loss:  0.3865622282028198\n",
            "Loss:  0.3861173987388611\n",
            "Loss:  0.38567447662353516\n",
            "Loss:  0.38523364067077637\n",
            "Loss:  0.38479453325271606\n",
            "Loss:  0.3843570649623871\n",
            "Loss:  0.3839212656021118\n",
            "Loss:  0.38348686695098877\n",
            "Loss:  0.38305389881134033\n",
            "Loss:  0.3826220631599426\n",
            "Loss:  0.3821915090084076\n",
            "Loss:  0.38176196813583374\n",
            "Loss:  0.38133344054222107\n",
            "Loss:  0.38090571761131287\n",
            "Loss:  0.38047873973846436\n",
            "Loss:  0.38005274534225464\n",
            "Loss:  0.37962737679481506\n",
            "Loss:  0.37920239567756653\n",
            "Loss:  0.3787780702114105\n",
            "Loss:  0.37835416197776794\n",
            "Loss:  0.3779306709766388\n",
            "Loss:  0.3775075078010559\n",
            "Loss:  0.37708455324172974\n",
            "Loss:  0.3766618072986603\n",
            "Loss:  0.3762393295764923\n",
            "Loss:  0.3758167624473572\n",
            "Loss:  0.3753942847251892\n",
            "Loss:  0.3749718964099884\n",
            "Loss:  0.37454935908317566\n",
            "Loss:  0.37412673234939575\n",
            "Loss:  0.37370383739471436\n",
            "Loss:  0.37328094244003296\n",
            "Loss:  0.3728576898574829\n",
            "Loss:  0.37243422865867615\n",
            "Loss:  0.3720102608203888\n",
            "Loss:  0.3715861439704895\n",
            "Loss:  0.371161550283432\n",
            "Loss:  0.3707364797592163\n",
            "Loss:  0.37031090259552\n",
            "Loss:  0.3698849081993103\n",
            "Loss:  0.3694581985473633\n",
            "Loss:  0.3690311312675476\n",
            "Loss:  0.36860334873199463\n",
            "Loss:  0.3681749105453491\n",
            "Loss:  0.3677457571029663\n",
            "Loss:  0.3673158586025238\n",
            "Loss:  0.3668854534626007\n",
            "Loss:  0.366454154253006\n",
            "Loss:  0.366021990776062\n",
            "Loss:  0.36558911204338074\n",
            "Loss:  0.36515548825263977\n",
            "Loss:  0.3647208511829376\n",
            "Loss:  0.36428534984588623\n",
            "Loss:  0.3638489544391632\n",
            "Loss:  0.36341163516044617\n",
            "Loss:  0.36297333240509033\n",
            "Loss:  0.3625341057777405\n",
            "Loss:  0.3620937466621399\n",
            "Loss:  0.3616524934768677\n",
            "Loss:  0.36121031641960144\n",
            "Loss:  0.36076685786247253\n",
            "Loss:  0.3603225350379944\n",
            "Loss:  0.3598770499229431\n",
            "Loss:  0.35943031311035156\n",
            "Loss:  0.358982652425766\n",
            "Loss:  0.35853368043899536\n",
            "Loss:  0.35808369517326355\n",
            "Loss:  0.35763245820999146\n",
            "Loss:  0.35718002915382385\n",
            "Loss:  0.35672640800476074\n",
            "Loss:  0.3562716245651245\n",
            "Loss:  0.3558155298233032\n",
            "Loss:  0.3553582727909088\n",
            "Loss:  0.3548998236656189\n",
            "Loss:  0.3544400930404663\n",
            "Loss:  0.35397911071777344\n",
            "Loss:  0.3535168766975403\n",
            "Loss:  0.3530532419681549\n",
            "Loss:  0.3525884747505188\n",
            "Loss:  0.3521222770214081\n",
            "Loss:  0.35165491700172424\n",
            "Loss:  0.35118621587753296\n",
            "Loss:  0.3507162928581238\n",
            "Loss:  0.35024508833885193\n",
            "Loss:  0.349772572517395\n",
            "Loss:  0.3492986559867859\n",
            "Loss:  0.3488236665725708\n",
            "Loss:  0.3483472764492035\n",
            "Loss:  0.3478696346282959\n",
            "Loss:  0.3473907709121704\n",
            "Loss:  0.34691062569618225\n",
            "Loss:  0.3464292585849762\n",
            "Loss:  0.345946729183197\n",
            "Loss:  0.3454630374908447\n",
            "Loss:  0.3449781537055969\n",
            "Loss:  0.3444920778274536\n",
            "Loss:  0.3440049886703491\n",
            "Loss:  0.34351661801338196\n",
            "Loss:  0.343027263879776\n",
            "Loss:  0.3425368070602417\n",
            "Loss:  0.34204551577568054\n",
            "Loss:  0.3415531814098358\n",
            "Loss:  0.3410598039627075\n",
            "Loss:  0.34056556224823\n",
            "Loss:  0.3400706350803375\n",
            "Loss:  0.3395746946334839\n",
            "Loss:  0.33907806873321533\n",
            "Loss:  0.33858078718185425\n",
            "Loss:  0.33808284997940063\n",
            "Loss:  0.33758413791656494\n",
            "Loss:  0.3370850682258606\n",
            "Loss:  0.3365854322910309\n",
            "Loss:  0.33608534932136536\n",
            "Loss:  0.33558493852615356\n",
            "Loss:  0.33508414030075073\n",
            "Loss:  0.33458325266838074\n",
            "Loss:  0.3340820372104645\n",
            "Loss:  0.3335808217525482\n",
            "Loss:  0.3330794870853424\n",
            "Loss:  0.33257821202278137\n",
            "Loss:  0.33207714557647705\n",
            "Loss:  0.3315761685371399\n",
            "Loss:  0.3310754895210266\n",
            "Loss:  0.3305751383304596\n",
            "Loss:  0.3300752341747284\n",
            "Loss:  0.3295758366584778\n",
            "Loss:  0.3290769159793854\n",
            "Loss:  0.3285786509513855\n",
            "Loss:  0.3280811607837677\n",
            "Loss:  0.3275843858718872\n",
            "Loss:  0.32708844542503357\n",
            "Loss:  0.3265935778617859\n",
            "Loss:  0.32609957456588745\n",
            "Loss:  0.3256067633628845\n",
            "Loss:  0.3251149654388428\n",
            "Loss:  0.3246244788169861\n",
            "Loss:  0.3241353929042816\n",
            "Loss:  0.32364752888679504\n",
            "Loss:  0.32316112518310547\n",
            "Loss:  0.3226761519908905\n",
            "Loss:  0.3221927583217621\n",
            "Loss:  0.3217109441757202\n",
            "Loss:  0.3212306499481201\n",
            "Loss:  0.32075226306915283\n",
            "Loss:  0.3202754557132721\n",
            "Loss:  0.319800466299057\n",
            "Loss:  0.31932732462882996\n",
            "Loss:  0.3188561201095581\n",
            "Loss:  0.3183867335319519\n",
            "Loss:  0.31791937351226807\n",
            "Loss:  0.3174539804458618\n",
            "Loss:  0.3169904947280884\n",
            "Loss:  0.31652912497520447\n",
            "Loss:  0.3160698115825653\n",
            "Loss:  0.3156124949455261\n",
            "Loss:  0.31515738368034363\n",
            "Loss:  0.3147042393684387\n",
            "Loss:  0.3142533004283905\n",
            "Loss:  0.31380459666252136\n",
            "Loss:  0.3133579194545746\n",
            "Loss:  0.31291332840919495\n",
            "Loss:  0.3124709725379944\n",
            "Loss:  0.3120306730270386\n",
            "Loss:  0.3115924894809723\n",
            "Loss:  0.31115660071372986\n",
            "Loss:  0.31072279810905457\n",
            "Loss:  0.31029099225997925\n",
            "Loss:  0.3098614811897278\n",
            "Loss:  0.3094339370727539\n",
            "Loss:  0.3090084195137024\n",
            "Loss:  0.3085850775241852\n",
            "Loss:  0.30816373229026794\n",
            "Loss:  0.30774447321891785\n",
            "Loss:  0.30732718110084534\n",
            "Loss:  0.30691197514533997\n",
            "Loss:  0.306498646736145\n",
            "Loss:  0.30608731508255005\n",
            "Loss:  0.3056778907775879\n",
            "Loss:  0.30527031421661377\n",
            "Loss:  0.30486470460891724\n",
            "Loss:  0.30446097254753113\n",
            "Loss:  0.3040589392185211\n",
            "Loss:  0.3036588430404663\n",
            "Loss:  0.30326047539711\n",
            "Loss:  0.3028638958930969\n",
            "Loss:  0.3024689555168152\n",
            "Loss:  0.30207574367523193\n",
            "Loss:  0.3016842305660248\n",
            "Loss:  0.30129438638687134\n",
            "Loss:  0.30090609192848206\n",
            "Loss:  0.3005194664001465\n",
            "Loss:  0.3001343905925751\n",
            "Loss:  0.29975083470344543\n",
            "Loss:  0.29936879873275757\n",
            "Loss:  0.2989882528781891\n",
            "Loss:  0.2986091673374176\n",
            "Loss:  0.2982315719127655\n",
            "Loss:  0.29785531759262085\n",
            "Loss:  0.2974804937839508\n",
            "Loss:  0.297107070684433\n",
            "Loss:  0.29673489928245544\n",
            "Loss:  0.2963641881942749\n",
            "Loss:  0.2959945797920227\n",
            "Loss:  0.29562637209892273\n",
            "Loss:  0.2952594459056854\n",
            "Loss:  0.29489362239837646\n",
            "Loss:  0.29452916979789734\n",
            "Loss:  0.29416579008102417\n",
            "Loss:  0.2938036024570465\n",
            "Loss:  0.2934427559375763\n",
            "Loss:  0.2930828332901001\n",
            "Loss:  0.2927241027355194\n",
            "Loss:  0.2923663556575775\n",
            "Loss:  0.29200980067253113\n",
            "Loss:  0.2916542887687683\n",
            "Loss:  0.29129987955093384\n",
            "Loss:  0.2909463942050934\n",
            "Loss:  0.2905939817428589\n",
            "Loss:  0.29024258255958557\n",
            "Loss:  0.2898920774459839\n",
            "Loss:  0.28954264521598816\n",
            "Loss:  0.2891941964626312\n",
            "Loss:  0.28884658217430115\n",
            "Loss:  0.28850001096725464\n",
            "Loss:  0.2881542444229126\n",
            "Loss:  0.28780946135520935\n",
            "Loss:  0.28746551275253296\n",
            "Loss:  0.2871224284172058\n",
            "Loss:  0.2867802679538727\n",
            "Loss:  0.2864389717578888\n",
            "Loss:  0.286098450422287\n",
            "Loss:  0.2857588529586792\n",
            "Loss:  0.28542008996009827\n",
            "Loss:  0.28508198261260986\n",
            "Loss:  0.28474482893943787\n",
            "Loss:  0.28440842032432556\n",
            "Loss:  0.2840728461742401\n",
            "Loss:  0.2837379574775696\n",
            "Loss:  0.2834039330482483\n",
            "Loss:  0.2830705940723419\n",
            "Loss:  0.2827381193637848\n",
            "Loss:  0.2824062705039978\n",
            "Loss:  0.2820751667022705\n",
            "Loss:  0.28174489736557007\n",
            "Loss:  0.2814151644706726\n",
            "Loss:  0.28108635544776917\n",
            "Loss:  0.2807581424713135\n",
            "Loss:  0.2804305851459503\n",
            "Loss:  0.2801036834716797\n",
            "Loss:  0.2797776758670807\n",
            "Loss:  0.27945220470428467\n",
            "Loss:  0.2791273593902588\n",
            "Loss:  0.2788032591342926\n",
            "Loss:  0.2784798741340637\n",
            "Loss:  0.2781570255756378\n",
            "Loss:  0.27783480286598206\n",
            "Loss:  0.2775132656097412\n",
            "Loss:  0.2771924138069153\n",
            "Loss:  0.2768721878528595\n",
            "Loss:  0.27655264735221863\n",
            "Loss:  0.27623361349105835\n",
            "Loss:  0.27591511607170105\n",
            "Loss:  0.2755974531173706\n",
            "Loss:  0.275280237197876\n",
            "Loss:  0.2749636769294739\n",
            "Loss:  0.27464768290519714\n",
            "Loss:  0.27433228492736816\n",
            "Loss:  0.2740175127983093\n",
            "Loss:  0.27370327711105347\n",
            "Loss:  0.27338966727256775\n",
            "Loss:  0.273076593875885\n",
            "Loss:  0.2727639973163605\n",
            "Loss:  0.27245208621025085\n",
            "Loss:  0.2721407413482666\n",
            "Loss:  0.27182987332344055\n",
            "Loss:  0.2715196907520294\n",
            "Loss:  0.2712099552154541\n",
            "Loss:  0.27090081572532654\n",
            "Loss:  0.2705921232700348\n",
            "Loss:  0.2702839970588684\n",
            "Loss:  0.2699764370918274\n",
            "Loss:  0.26966938376426697\n",
            "Loss:  0.2693628966808319\n",
            "Loss:  0.26905694603919983\n",
            "Loss:  0.26875150203704834\n",
            "Loss:  0.2684464156627655\n",
            "Loss:  0.2681419849395752\n",
            "Loss:  0.26783818006515503\n",
            "Loss:  0.26753461360931396\n",
            "Loss:  0.2672317326068878\n",
            "Loss:  0.2669293284416199\n",
            "Loss:  0.26662734150886536\n",
            "Loss:  0.26632580161094666\n",
            "Loss:  0.26602500677108765\n",
            "Loss:  0.2657244801521301\n",
            "Loss:  0.2654244601726532\n",
            "Loss:  0.26512500643730164\n",
            "Loss:  0.2648259401321411\n",
            "Loss:  0.26452744007110596\n",
            "Loss:  0.26422935724258423\n",
            "Loss:  0.2639317214488983\n",
            "Loss:  0.263634592294693\n",
            "Loss:  0.2633378803730011\n",
            "Loss:  0.2630417048931122\n",
            "Loss:  0.2627459168434143\n",
            "Loss:  0.2624507248401642\n",
            "Loss:  0.26215583086013794\n",
            "Loss:  0.2618614137172699\n",
            "Loss:  0.26156747341156006\n",
            "Loss:  0.2612740099430084\n",
            "Loss:  0.26098090410232544\n",
            "Loss:  0.26068833470344543\n",
            "Loss:  0.260396271944046\n",
            "Loss:  0.2601045072078705\n",
            "Loss:  0.25981318950653076\n",
            "Loss:  0.25952234864234924\n",
            "Loss:  0.25923192501068115\n",
            "Loss:  0.2589419484138489\n",
            "Loss:  0.2586524486541748\n",
            "Loss:  0.2583633065223694\n",
            "Loss:  0.25807464122772217\n",
            "Loss:  0.2577863931655884\n",
            "Loss:  0.25749847292900085\n",
            "Loss:  0.2572110891342163\n",
            "Loss:  0.256924033164978\n",
            "Loss:  0.2566373944282532\n",
            "Loss:  0.25635120272636414\n",
            "Loss:  0.2560654580593109\n",
            "Loss:  0.25578001141548157\n",
            "Loss:  0.2554951012134552\n",
            "Loss:  0.2552105188369751\n",
            "Loss:  0.2549264430999756\n",
            "Loss:  0.25464266538619995\n",
            "Loss:  0.25435933470726013\n",
            "Loss:  0.25407639145851135\n",
            "Loss:  0.2537938356399536\n",
            "Loss:  0.2535117566585541\n",
            "Loss:  0.2532300651073456\n",
            "Loss:  0.25294867157936096\n",
            "Loss:  0.2526676654815674\n",
            "Loss:  0.25238707661628723\n",
            "Loss:  0.2521069049835205\n",
            "Loss:  0.25182706117630005\n",
            "Loss:  0.2515476942062378\n",
            "Loss:  0.2512686252593994\n",
            "Loss:  0.25099003314971924\n",
            "Loss:  0.2507117986679077\n",
            "Loss:  0.25043386220932007\n",
            "Loss:  0.25015637278556824\n",
            "Loss:  0.24987927079200745\n",
            "Loss:  0.24960246682167053\n",
            "Loss:  0.24932609498500824\n",
            "Loss:  0.24905019998550415\n",
            "Loss:  0.24877458810806274\n",
            "Loss:  0.2484993040561676\n",
            "Loss:  0.24822452664375305\n",
            "Loss:  0.2479499876499176\n",
            "Loss:  0.24767577648162842\n",
            "Loss:  0.24740207195281982\n",
            "Loss:  0.2471286654472351\n",
            "Loss:  0.24685560166835785\n",
            "Loss:  0.24658294022083282\n",
            "Loss:  0.24631057679653168\n",
            "Loss:  0.24603873491287231\n",
            "Loss:  0.24576705694198608\n",
            "Loss:  0.24549593031406403\n",
            "Loss:  0.24522504210472107\n",
            "Loss:  0.24495457112789154\n",
            "Loss:  0.2446843683719635\n",
            "Loss:  0.24441459774971008\n",
            "Loss:  0.24414514005184174\n",
            "Loss:  0.24387606978416443\n",
            "Loss:  0.24360734224319458\n",
            "Loss:  0.24333900213241577\n",
            "Loss:  0.24307093024253845\n",
            "Loss:  0.24280327558517456\n",
            "Loss:  0.24253593385219574\n",
            "Loss:  0.24226894974708557\n",
            "Loss:  0.24200233817100525\n",
            "Loss:  0.24173606932163239\n",
            "Loss:  0.2414701133966446\n",
            "Loss:  0.24120451509952545\n",
            "Loss:  0.24093928933143616\n",
            "Loss:  0.24067436158657074\n",
            "Loss:  0.24040985107421875\n",
            "Loss:  0.24014562368392944\n",
            "Loss:  0.2398817390203476\n",
            "Loss:  0.23961810767650604\n",
            "Loss:  0.23935499787330627\n",
            "Loss:  0.23909209668636322\n",
            "Loss:  0.23882953822612762\n",
            "Loss:  0.23856733739376068\n",
            "Loss:  0.2383054941892624\n",
            "Loss:  0.23804397881031036\n",
            "Loss:  0.23778286576271057\n",
            "Loss:  0.23752200603485107\n",
            "Loss:  0.23726147413253784\n",
            "Loss:  0.2370012253522873\n",
            "Loss:  0.23674136400222778\n",
            "Loss:  0.23648186028003693\n",
            "Loss:  0.23622260987758636\n",
            "Loss:  0.2359638214111328\n",
            "Loss:  0.23570528626441956\n",
            "Loss:  0.23544704914093018\n",
            "Loss:  0.23518915474414825\n",
            "Loss:  0.2349316030740738\n",
            "Loss:  0.2346743941307068\n",
            "Loss:  0.23441749811172485\n",
            "Loss:  0.234160915017128\n",
            "Loss:  0.23390458524227142\n",
            "Loss:  0.23364870250225067\n",
            "Loss:  0.23339307308197021\n",
            "Loss:  0.23313774168491364\n",
            "Loss:  0.2328827977180481\n",
            "Loss:  0.23262807726860046\n",
            "Loss:  0.23237375915050507\n",
            "Loss:  0.23211978375911713\n",
            "Loss:  0.23186610639095306\n",
            "Loss:  0.2316126972436905\n",
            "Loss:  0.23135966062545776\n",
            "Loss:  0.2311069518327713\n",
            "Loss:  0.23085446655750275\n",
            "Loss:  0.23060232400894165\n",
            "Loss:  0.2303505688905716\n",
            "Loss:  0.23009900748729706\n",
            "Loss:  0.22984784841537476\n",
            "Loss:  0.2295970469713211\n",
            "Loss:  0.22934645414352417\n",
            "Loss:  0.22909623384475708\n",
            "Loss:  0.22884637117385864\n",
            "Loss:  0.22859664261341095\n",
            "Loss:  0.22834739089012146\n",
            "Loss:  0.22809834778308868\n",
            "Loss:  0.22784973680973053\n",
            "Loss:  0.22760125994682312\n",
            "Loss:  0.2273533046245575\n",
            "Loss:  0.22710546851158142\n",
            "Loss:  0.2268580049276352\n",
            "Loss:  0.22661083936691284\n",
            "Loss:  0.22636406123638153\n",
            "Loss:  0.22611746191978455\n",
            "Loss:  0.22587117552757263\n",
            "Loss:  0.22562533617019653\n",
            "Loss:  0.2253796011209488\n",
            "Loss:  0.22513431310653687\n",
            "Loss:  0.22488926351070404\n",
            "Loss:  0.2246445268392563\n",
            "Loss:  0.2244000881910324\n",
            "Loss:  0.22415602207183838\n",
            "Loss:  0.22391211986541748\n",
            "Loss:  0.22366857528686523\n",
            "Loss:  0.22342534363269806\n",
            "Loss:  0.22318242490291595\n",
            "Loss:  0.22293977439403534\n",
            "Loss:  0.2226973921060562\n",
            "Loss:  0.22245536744594574\n",
            "Loss:  0.22221367061138153\n",
            "Loss:  0.22197219729423523\n",
            "Loss:  0.22173099219799042\n",
            "Loss:  0.22149015963077545\n",
            "Loss:  0.221249520778656\n",
            "Loss:  0.2210092544555664\n",
            "Loss:  0.22076931595802307\n",
            "Loss:  0.22052960097789764\n",
            "Loss:  0.22029024362564087\n",
            "Loss:  0.22005115449428558\n",
            "Loss:  0.2198123186826706\n",
            "Loss:  0.2195737510919571\n",
            "Loss:  0.21933551132678986\n",
            "Loss:  0.21909761428833008\n",
            "Loss:  0.2188599705696106\n",
            "Loss:  0.21862255036830902\n",
            "Loss:  0.2183855026960373\n",
            "Loss:  0.21814869344234467\n",
            "Loss:  0.21791218221187592\n",
            "Loss:  0.21767593920230865\n",
            "Loss:  0.21743999421596527\n",
            "Loss:  0.21720436215400696\n",
            "Loss:  0.21696899831295013\n",
            "Loss:  0.216733917593956\n",
            "Loss:  0.2164991796016693\n",
            "Loss:  0.21626465022563934\n",
            "Loss:  0.21603040397167206\n",
            "Loss:  0.21579648554325104\n",
            "Loss:  0.21556274592876434\n",
            "Loss:  0.2153293937444687\n",
            "Loss:  0.2150963395833969\n",
            "Loss:  0.21486344933509827\n",
            "Loss:  0.21463099122047424\n",
            "Loss:  0.21439871191978455\n",
            "Loss:  0.2141667604446411\n",
            "Loss:  0.21393507719039917\n",
            "Loss:  0.21370354294776917\n",
            "Loss:  0.2134726196527481\n",
            "Loss:  0.21324165165424347\n",
            "Loss:  0.21301105618476868\n",
            "Loss:  0.21278072893619537\n",
            "Loss:  0.21255065500736237\n",
            "Loss:  0.21232086420059204\n",
            "Loss:  0.21209144592285156\n",
            "Loss:  0.21186229586601257\n",
            "Loss:  0.2116333395242691\n",
            "Loss:  0.21140462160110474\n",
            "Loss:  0.21117635071277618\n",
            "Loss:  0.210948184132576\n",
            "Loss:  0.21072040498256683\n",
            "Loss:  0.2104927897453308\n",
            "Loss:  0.21026554703712463\n",
            "Loss:  0.21003852784633636\n",
            "Loss:  0.209811732172966\n",
            "Loss:  0.2095853090286255\n",
            "Loss:  0.20935915410518646\n",
            "Loss:  0.20913320779800415\n",
            "Loss:  0.20890755951404572\n",
            "Loss:  0.2086821347475052\n",
            "Loss:  0.2084570676088333\n",
            "Loss:  0.20823220908641815\n",
            "Loss:  0.20800761878490448\n",
            "Loss:  0.2077832818031311\n",
            "Loss:  0.2075592577457428\n",
            "Loss:  0.2073354721069336\n",
            "Loss:  0.2071119248867035\n",
            "Loss:  0.20688873529434204\n",
            "Loss:  0.20666569471359253\n",
            "Loss:  0.20644298195838928\n",
            "Loss:  0.20622049272060394\n",
            "Loss:  0.2059982717037201\n",
            "Loss:  0.20577648282051086\n",
            "Loss:  0.2055547535419464\n",
            "Loss:  0.2053333818912506\n",
            "Loss:  0.2051122635602951\n",
            "Loss:  0.2048913836479187\n",
            "Loss:  0.20467078685760498\n",
            "Loss:  0.20445041358470917\n",
            "Loss:  0.20423032343387604\n",
            "Loss:  0.20401053130626678\n",
            "Loss:  0.20379094779491425\n",
            "Loss:  0.203571617603302\n",
            "Loss:  0.20335254073143005\n",
            "Loss:  0.20313383638858795\n",
            "Loss:  0.20291532576084137\n",
            "Loss:  0.20269706845283508\n",
            "Loss:  0.2024790644645691\n",
            "Loss:  0.2022613286972046\n",
            "Loss:  0.2020438313484192\n",
            "Loss:  0.2018265575170517\n",
            "Loss:  0.20160962641239166\n",
            "Loss:  0.20139290392398834\n",
            "Loss:  0.20117643475532532\n",
            "Loss:  0.2009602040052414\n",
            "Loss:  0.20074430108070374\n",
            "Loss:  0.2005286067724228\n",
            "Loss:  0.20031315088272095\n",
            "Loss:  0.20009799301624298\n",
            "Loss:  0.19988305866718292\n",
            "Loss:  0.19966834783554077\n",
            "Loss:  0.1994539499282837\n",
            "Loss:  0.19923973083496094\n",
            "Loss:  0.19902580976486206\n",
            "Loss:  0.19881218671798706\n",
            "Loss:  0.19859880208969116\n",
            "Loss:  0.1983855664730072\n",
            "Loss:  0.1981726884841919\n",
            "Loss:  0.1979600340127945\n",
            "Loss:  0.1977475881576538\n",
            "Loss:  0.1975354552268982\n",
            "Loss:  0.1973235160112381\n",
            "Loss:  0.19711188971996307\n",
            "Loss:  0.19690048694610596\n",
            "Loss:  0.1966894119977951\n",
            "Loss:  0.19647838175296783\n",
            "Loss:  0.19626778364181519\n",
            "Loss:  0.19605736434459686\n",
            "Loss:  0.19584716856479645\n",
            "Loss:  0.19563721120357513\n",
            "Loss:  0.19542747735977173\n",
            "Loss:  0.19521816074848175\n",
            "Loss:  0.19500893354415894\n",
            "Loss:  0.19479994475841522\n",
            "Loss:  0.19459125399589539\n",
            "Loss:  0.19438286125659943\n",
            "Loss:  0.1941746324300766\n",
            "Loss:  0.19396668672561646\n",
            "Loss:  0.19375894963741302\n",
            "Loss:  0.1935514509677887\n",
            "Loss:  0.19334425032138824\n",
            "Loss:  0.1931372433900833\n",
            "Loss:  0.19293048977851868\n",
            "Loss:  0.19272397458553314\n",
            "Loss:  0.1925177425146103\n",
            "Loss:  0.19231167435646057\n",
            "Loss:  0.1921059489250183\n",
            "Loss:  0.19190038740634918\n",
            "Loss:  0.19169513881206512\n",
            "Loss:  0.191490039229393\n",
            "Loss:  0.19128520786762238\n",
            "Loss:  0.19108065962791443\n",
            "Loss:  0.19087627530097961\n",
            "Loss:  0.19067220389842987\n",
            "Loss:  0.19046831130981445\n",
            "Loss:  0.19026470184326172\n",
            "Loss:  0.19006134569644928\n",
            "Loss:  0.18985815346240997\n",
            "Loss:  0.18965525925159454\n",
            "Loss:  0.18945258855819702\n",
            "Loss:  0.1892501711845398\n",
            "Loss:  0.18904796242713928\n",
            "Loss:  0.18884597718715668\n",
            "Loss:  0.18864421546459198\n",
            "Loss:  0.18844275176525116\n",
            "Loss:  0.18824148178100586\n",
            "Loss:  0.18804043531417847\n",
            "Loss:  0.18783976137638092\n",
            "Loss:  0.18763908743858337\n",
            "Loss:  0.18743881583213806\n",
            "Loss:  0.18723875284194946\n",
            "Loss:  0.187038853764534\n",
            "Loss:  0.1868392527103424\n",
            "Loss:  0.18663986027240753\n",
            "Loss:  0.18644066154956818\n",
            "Loss:  0.18624171614646912\n",
            "Loss:  0.18604308366775513\n",
            "Loss:  0.1858445405960083\n",
            "Loss:  0.18564635515213013\n",
            "Loss:  0.18544834852218628\n",
            "Loss:  0.18525056540966034\n",
            "Loss:  0.1850530207157135\n",
            "Loss:  0.18485571444034576\n",
            "Loss:  0.18465858697891235\n",
            "Loss:  0.184461772441864\n",
            "Loss:  0.18426507711410522\n",
            "Loss:  0.1840687394142151\n",
            "Loss:  0.18387256562709808\n",
            "Loss:  0.1836766004562378\n",
            "Loss:  0.1834808886051178\n",
            "Loss:  0.1832854300737381\n",
            "Loss:  0.18309012055397034\n",
            "Loss:  0.18289506435394287\n",
            "Loss:  0.1827002763748169\n",
            "Loss:  0.18250568211078644\n",
            "Loss:  0.18231134116649628\n",
            "Loss:  0.18211713433265686\n",
            "Loss:  0.1819233000278473\n",
            "Loss:  0.18172957003116608\n",
            "Loss:  0.18153607845306396\n",
            "Loss:  0.18134286999702454\n",
            "Loss:  0.18114987015724182\n",
            "Loss:  0.18095706403255463\n",
            "Loss:  0.18076452612876892\n",
            "Loss:  0.18057221174240112\n",
            "Loss:  0.18038006126880646\n",
            "Loss:  0.1801881194114685\n",
            "Loss:  0.17999644577503204\n",
            "Loss:  0.17980505526065826\n",
            "Loss:  0.17961379885673523\n",
            "Loss:  0.1794227808713913\n",
            "Loss:  0.1792319267988205\n",
            "Loss:  0.17904144525527954\n",
            "Loss:  0.17885100841522217\n",
            "Loss:  0.17866088449954987\n",
            "Loss:  0.17847096920013428\n",
            "Loss:  0.17828132212162018\n",
            "Loss:  0.17809180915355682\n",
            "Loss:  0.17790254950523376\n",
            "Loss:  0.1777135282754898\n",
            "Loss:  0.17752474546432495\n",
            "Loss:  0.17733608186244965\n",
            "Loss:  0.1771477311849594\n",
            "Loss:  0.17695952951908112\n",
            "Loss:  0.1767715960741043\n",
            "Loss:  0.1765839010477066\n",
            "Loss:  0.17639635503292084\n",
            "Loss:  0.17620904743671417\n",
            "Loss:  0.1760219931602478\n",
            "Loss:  0.17583514750003815\n",
            "Loss:  0.17564842104911804\n",
            "Loss:  0.1754620522260666\n",
            "Loss:  0.17527581751346588\n",
            "Loss:  0.17508982121944427\n",
            "Loss:  0.1749040186405182\n",
            "Loss:  0.17471837997436523\n",
            "Loss:  0.17453309893608093\n",
            "Loss:  0.174347922205925\n",
            "Loss:  0.17416301369667053\n",
            "Loss:  0.1739782691001892\n",
            "Loss:  0.1737937182188034\n",
            "Loss:  0.1736093908548355\n",
            "Loss:  0.1734253466129303\n",
            "Loss:  0.17324143648147583\n",
            "Loss:  0.17305780947208405\n",
            "Loss:  0.1728743314743042\n",
            "Loss:  0.17269109189510345\n",
            "Loss:  0.1725080907344818\n",
            "Loss:  0.1723252236843109\n",
            "Loss:  0.1721426397562027\n",
            "Loss:  0.17196020483970642\n",
            "Loss:  0.17177799344062805\n",
            "Loss:  0.17159605026245117\n",
            "Loss:  0.17141425609588623\n",
            "Loss:  0.1712326854467392\n",
            "Loss:  0.17105136811733246\n",
            "Loss:  0.17087024450302124\n",
            "Loss:  0.17068925499916077\n",
            "Loss:  0.17050856351852417\n",
            "Loss:  0.1703280210494995\n",
            "Loss:  0.17014770209789276\n",
            "Loss:  0.16996756196022034\n",
            "Loss:  0.1697876900434494\n",
            "Loss:  0.16960792243480682\n",
            "Loss:  0.16942846775054932\n",
            "Loss:  0.16924914717674255\n",
            "Loss:  0.1690700650215149\n",
            "Loss:  0.16889114677906036\n",
            "Loss:  0.1687125712633133\n",
            "Loss:  0.168534055352211\n",
            "Loss:  0.1683558225631714\n",
            "Loss:  0.1681777536869049\n",
            "Loss:  0.16799987852573395\n",
            "Loss:  0.1678222268819809\n",
            "Loss:  0.16764475405216217\n",
            "Loss:  0.16746753454208374\n",
            "Loss:  0.16729046404361725\n",
            "Loss:  0.16711364686489105\n",
            "Loss:  0.1669369637966156\n",
            "Loss:  0.16676057875156403\n",
            "Loss:  0.16658437252044678\n",
            "Loss:  0.16640831530094147\n",
            "Loss:  0.16623246669769287\n",
            "Loss:  0.1660567820072174\n",
            "Loss:  0.16588141024112701\n",
            "Loss:  0.16570615768432617\n",
            "Loss:  0.16553114354610443\n",
            "Loss:  0.16535629332065582\n",
            "Loss:  0.16518168151378632\n",
            "Loss:  0.16500729322433472\n",
            "Loss:  0.16483299434185028\n",
            "Loss:  0.1646590232849121\n",
            "Loss:  0.1644851565361023\n",
            "Loss:  0.1643114984035492\n",
            "Loss:  0.1641380935907364\n",
            "Loss:  0.16396483778953552\n",
            "Loss:  0.16379183530807495\n",
            "Loss:  0.16361896693706512\n",
            "Loss:  0.1634463369846344\n",
            "Loss:  0.1632738709449768\n",
            "Loss:  0.1631016582250595\n",
            "Loss:  0.16292957961559296\n",
            "Loss:  0.1627577394247055\n",
            "Loss:  0.1625860631465912\n",
            "Loss:  0.1624145358800888\n",
            "Loss:  0.1622433066368103\n",
            "Loss:  0.16207219660282135\n",
            "Loss:  0.1619013398885727\n",
            "Loss:  0.16173063218593597\n",
            "Loss:  0.16156017780303955\n",
            "Loss:  0.16138988733291626\n",
            "Loss:  0.1612197905778885\n",
            "Loss:  0.16104990243911743\n",
            "Loss:  0.1608801633119583\n",
            "Loss:  0.16071061789989471\n",
            "Loss:  0.16054126620292664\n",
            "Loss:  0.16037212312221527\n",
            "Loss:  0.16020320355892181\n",
            "Loss:  0.16003447771072388\n",
            "Loss:  0.15986590087413788\n",
            "Loss:  0.1596975475549698\n",
            "Loss:  0.15952929854393005\n",
            "Loss:  0.1593613475561142\n",
            "Loss:  0.15919354557991028\n",
            "Loss:  0.15902592241764069\n",
            "Loss:  0.1588585078716278\n",
            "Loss:  0.15869127213954926\n",
            "Loss:  0.15852424502372742\n",
            "Loss:  0.1583574265241623\n",
            "Loss:  0.1581907868385315\n",
            "Loss:  0.15802428126335144\n",
            "Loss:  0.1578580141067505\n",
            "Loss:  0.15769188106060028\n",
            "Loss:  0.15752603113651276\n",
            "Loss:  0.15736033022403717\n",
            "Loss:  0.15719476342201233\n",
            "Loss:  0.1570294350385666\n",
            "Loss:  0.15686434507369995\n",
            "Loss:  0.15669935941696167\n",
            "Loss:  0.15653462707996368\n",
            "Loss:  0.15637001395225525\n",
            "Loss:  0.15620559453964233\n",
            "Loss:  0.1560414433479309\n",
            "Loss:  0.15587735176086426\n",
            "Loss:  0.1557135432958603\n",
            "Loss:  0.15554991364479065\n",
            "Loss:  0.15538643300533295\n",
            "Loss:  0.15522319078445435\n",
            "Loss:  0.15506000816822052\n",
            "Loss:  0.15489709377288818\n",
            "Loss:  0.15473443269729614\n",
            "Loss:  0.15457183122634888\n",
            "Loss:  0.1544094830751419\n",
            "Loss:  0.15424732863903046\n",
            "Loss:  0.15408533811569214\n",
            "Loss:  0.15392352640628815\n",
            "Loss:  0.15376195311546326\n",
            "Loss:  0.15360043942928314\n",
            "Loss:  0.1534392386674881\n",
            "Loss:  0.1532781720161438\n",
            "Loss:  0.15311722457408905\n",
            "Loss:  0.1529565453529358\n",
            "Loss:  0.15279600024223328\n",
            "Loss:  0.15263566374778748\n",
            "Loss:  0.152475506067276\n",
            "Loss:  0.15231552720069885\n",
            "Loss:  0.15215574204921722\n",
            "Loss:  0.15199607610702515\n",
            "Loss:  0.15183666348457336\n",
            "Loss:  0.15167738497257233\n",
            "Loss:  0.15151828527450562\n",
            "Loss:  0.15135939419269562\n",
            "Loss:  0.15120071172714233\n",
            "Loss:  0.1510421335697174\n",
            "Loss:  0.15088379383087158\n",
            "Loss:  0.15072563290596008\n",
            "Loss:  0.15056757628917694\n",
            "Loss:  0.15040981769561768\n",
            "Loss:  0.15025216341018677\n",
            "Loss:  0.1500946581363678\n",
            "Loss:  0.14993739128112793\n",
            "Loss:  0.14978033304214478\n",
            "Loss:  0.1496233493089676\n",
            "Loss:  0.1494665890932083\n",
            "Loss:  0.14930997788906097\n",
            "Loss:  0.14915359020233154\n",
            "Loss:  0.14899732172489166\n",
            "Loss:  0.14884129166603088\n",
            "Loss:  0.14868536591529846\n",
            "Loss:  0.14852967858314514\n",
            "Loss:  0.14837417006492615\n",
            "Loss:  0.14821882545948029\n",
            "Loss:  0.14806364476680756\n",
            "Loss:  0.14790864288806915\n",
            "Loss:  0.14775383472442627\n",
            "Loss:  0.14759916067123413\n",
            "Loss:  0.14744475483894348\n",
            "Loss:  0.14729037880897522\n",
            "Loss:  0.14713628590106964\n",
            "Loss:  0.1469823271036148\n",
            "Loss:  0.1468285769224167\n",
            "Loss:  0.1466749757528305\n",
            "Loss:  0.14652152359485626\n",
            "Loss:  0.14636825025081635\n",
            "Loss:  0.14621521532535553\n",
            "Loss:  0.14606228470802307\n",
            "Loss:  0.14590956270694733\n",
            "Loss:  0.14575691521167755\n",
            "Loss:  0.14560455083847046\n",
            "Loss:  0.1454523503780365\n",
            "Loss:  0.1453002393245697\n",
            "Loss:  0.14514842629432678\n",
            "Loss:  0.14499665796756744\n",
            "Loss:  0.1448451280593872\n",
            "Loss:  0.1446937620639801\n",
            "Loss:  0.14454254508018494\n",
            "Loss:  0.1443915218114853\n",
            "Loss:  0.14424066245555878\n",
            "Loss:  0.14408999681472778\n",
            "Loss:  0.14393940567970276\n",
            "Loss:  0.1437891125679016\n",
            "Loss:  0.14363889396190643\n",
            "Loss:  0.14348894357681274\n",
            "Loss:  0.14333902299404144\n",
            "Loss:  0.14318937063217163\n",
            "Loss:  0.14303986728191376\n",
            "Loss:  0.1428905427455902\n",
            "Loss:  0.14274132251739502\n",
            "Loss:  0.1425923854112625\n",
            "Loss:  0.14244353771209717\n",
            "Loss:  0.14229480922222137\n",
            "Loss:  0.14214633405208588\n",
            "Loss:  0.14199800789356232\n",
            "Loss:  0.1418498456478119\n",
            "Loss:  0.1417018175125122\n",
            "Loss:  0.14155398309230804\n",
            "Loss:  0.1414063572883606\n",
            "Loss:  0.14125880599021912\n",
            "Loss:  0.14111149311065674\n",
            "Loss:  0.14096428453922272\n",
            "Loss:  0.1408172994852066\n",
            "Loss:  0.14067046344280243\n",
            "Loss:  0.1405237317085266\n",
            "Loss:  0.14037729799747467\n",
            "Loss:  0.1402309089899063\n",
            "Loss:  0.14008472859859467\n",
            "Loss:  0.13993871212005615\n",
            "Loss:  0.1397928148508072\n",
            "Loss:  0.13964718580245972\n",
            "Loss:  0.13950161635875702\n",
            "Loss:  0.13935625553131104\n",
            "Loss:  0.1392110288143158\n",
            "Loss:  0.13906602561473846\n",
            "Loss:  0.13892114162445068\n",
            "Loss:  0.13877639174461365\n",
            "Loss:  0.13863182067871094\n",
            "Loss:  0.13848745822906494\n",
            "Loss:  0.1383432149887085\n",
            "Loss:  0.13819913566112518\n",
            "Loss:  0.1380552500486374\n",
            "Loss:  0.13791145384311676\n",
            "Loss:  0.13776791095733643\n",
            "Loss:  0.13762453198432922\n",
            "Loss:  0.137481227517128\n",
            "Loss:  0.13733810186386108\n",
            "Loss:  0.13719519972801208\n",
            "Loss:  0.13705241680145264\n",
            "Loss:  0.13690975308418274\n",
            "Loss:  0.13676731288433075\n",
            "Loss:  0.1366250067949295\n",
            "Loss:  0.1364828646183014\n",
            "Loss:  0.13634085655212402\n",
            "Loss:  0.13619907200336456\n",
            "Loss:  0.13605739176273346\n",
            "Loss:  0.1359158456325531\n",
            "Loss:  0.13577458262443542\n",
            "Loss:  0.13563327491283417\n",
            "Loss:  0.13549228012561798\n",
            "Loss:  0.13535138964653015\n",
            "Loss:  0.13521073758602142\n",
            "Loss:  0.1350701004266739\n",
            "Loss:  0.13492970168590546\n",
            "Loss:  0.13478949666023254\n",
            "Loss:  0.1346493661403656\n",
            "Loss:  0.13450947403907776\n",
            "Loss:  0.13436970114707947\n",
            "Loss:  0.13423007726669312\n",
            "Loss:  0.1340906023979187\n",
            "Loss:  0.13395129144191742\n",
            "Loss:  0.13381211459636688\n",
            "Loss:  0.13367314636707306\n",
            "Loss:  0.133534237742424\n",
            "Loss:  0.13339558243751526\n",
            "Loss:  0.13325710594654083\n",
            "Loss:  0.13311867415905\n",
            "Loss:  0.13298043608665466\n",
            "Loss:  0.13284239172935486\n",
            "Loss:  0.1327044516801834\n",
            "Loss:  0.1325666904449463\n",
            "Loss:  0.1324291080236435\n",
            "Loss:  0.13229165971279144\n",
            "Loss:  0.13215437531471252\n",
            "Loss:  0.13201719522476196\n",
            "Loss:  0.13188019394874573\n",
            "Loss:  0.131743386387825\n",
            "Loss:  0.13160666823387146\n",
            "Loss:  0.13147011399269104\n",
            "Loss:  0.13133373856544495\n",
            "Loss:  0.13119752705097198\n",
            "Loss:  0.13106143474578857\n",
            "Loss:  0.13092553615570068\n",
            "Loss:  0.13078975677490234\n",
            "Loss:  0.13065411150455475\n",
            "Loss:  0.13051864504814148\n",
            "Loss:  0.13038331270217896\n",
            "Loss:  0.13024812936782837\n",
            "Loss:  0.13011310994625092\n",
            "Loss:  0.1299782246351242\n",
            "Loss:  0.12984351813793182\n",
            "Loss:  0.12970894575119019\n",
            "Loss:  0.12957452237606049\n",
            "Loss:  0.1294402927160263\n",
            "Loss:  0.12930609285831451\n",
            "Loss:  0.1291721612215042\n",
            "Loss:  0.12903828918933868\n",
            "Loss:  0.12890462577342987\n",
            "Loss:  0.1287711262702942\n",
            "Loss:  0.12863780558109283\n",
            "Loss:  0.12850449979305267\n",
            "Loss:  0.1283714324235916\n",
            "Loss:  0.1282384991645813\n",
            "Loss:  0.12810571491718292\n",
            "Loss:  0.12797309458255768\n",
            "Loss:  0.12784059345722198\n",
            "Loss:  0.12770821154117584\n",
            "Loss:  0.1275760382413864\n",
            "Loss:  0.12744399905204773\n",
            "Loss:  0.12731212377548218\n",
            "Loss:  0.12718036770820618\n",
            "Loss:  0.12704874575138092\n",
            "Loss:  0.12691724300384521\n",
            "Loss:  0.1267859786748886\n",
            "Loss:  0.12665480375289917\n",
            "Loss:  0.12652379274368286\n",
            "Loss:  0.1263929009437561\n",
            "Loss:  0.1262621134519577\n",
            "Loss:  0.1261315792798996\n",
            "Loss:  0.12600114941596985\n",
            "Loss:  0.12587082386016846\n",
            "Loss:  0.1257406622171402\n",
            "Loss:  0.12561069428920746\n",
            "Loss:  0.12548084557056427\n",
            "Loss:  0.12535108625888824\n",
            "Loss:  0.12522153556346893\n",
            "Loss:  0.12509208917617798\n",
            "Loss:  0.12496282905340195\n",
            "Loss:  0.12483365088701248\n",
            "Loss:  0.12470462918281555\n",
            "Loss:  0.12457580864429474\n",
            "Loss:  0.12444712966680527\n",
            "Loss:  0.12431852519512177\n",
            "Loss:  0.12419005483388901\n",
            "Loss:  0.12406182289123535\n",
            "Loss:  0.12393365800380707\n",
            "Loss:  0.12380566447973251\n",
            "Loss:  0.12367784231901169\n",
            "Loss:  0.12355010956525803\n",
            "Loss:  0.1234225481748581\n",
            "Loss:  0.12329511344432831\n",
            "Loss:  0.12316777557134628\n",
            "Loss:  0.12304061651229858\n",
            "Loss:  0.122913658618927\n",
            "Loss:  0.12278679758310318\n",
            "Loss:  0.1226600855588913\n",
            "Loss:  0.12253343313932419\n",
            "Loss:  0.122406966984272\n",
            "Loss:  0.12228072434663773\n",
            "Loss:  0.12215454131364822\n",
            "Loss:  0.12202849984169006\n",
            "Loss:  0.12190259993076324\n",
            "Loss:  0.12177686393260956\n",
            "Loss:  0.12165126204490662\n",
            "Loss:  0.12152580916881561\n",
            "Loss:  0.12140044569969177\n",
            "Loss:  0.12127522379159927\n",
            "Loss:  0.12115022540092468\n",
            "Loss:  0.12102524936199188\n",
            "Loss:  0.12090053409337997\n",
            "Loss:  0.12077584862709045\n",
            "Loss:  0.12065140902996063\n",
            "Loss:  0.1205269992351532\n",
            "Loss:  0.12040279060602188\n",
            "Loss:  0.12027870118618011\n",
            "Loss:  0.1201547160744667\n",
            "Loss:  0.12003092467784882\n",
            "Loss:  0.11990723758935928\n",
            "Loss:  0.11978369951248169\n",
            "Loss:  0.11966033279895782\n",
            "Loss:  0.11953702569007874\n",
            "Loss:  0.11941394209861755\n",
            "Loss:  0.11929088830947876\n",
            "Loss:  0.11916806548833847\n",
            "Loss:  0.11904533207416534\n",
            "Loss:  0.11892274022102356\n",
            "Loss:  0.11880031228065491\n",
            "Loss:  0.11867796629667282\n",
            "Loss:  0.11855580657720566\n",
            "Loss:  0.11843374371528625\n",
            "Loss:  0.11831184476613998\n",
            "Loss:  0.11819008737802505\n",
            "Loss:  0.11806842684745789\n",
            "Loss:  0.11794687062501907\n",
            "Loss:  0.11782555282115936\n",
            "Loss:  0.1177043542265892\n",
            "Loss:  0.11758321523666382\n",
            "Loss:  0.11746224761009216\n",
            "Loss:  0.11734138429164886\n",
            "Loss:  0.11722066253423691\n",
            "Loss:  0.11710008978843689\n",
            "Loss:  0.11697971075773239\n",
            "Loss:  0.11685937643051147\n",
            "Loss:  0.11673921346664429\n",
            "Loss:  0.11661915481090546\n",
            "Loss:  0.11649928987026215\n",
            "Loss:  0.11637946218252182\n",
            "Loss:  0.1162598580121994\n",
            "Loss:  0.11614034324884415\n",
            "Loss:  0.11602098494768143\n",
            "Loss:  0.11590176820755005\n",
            "Loss:  0.11578260362148285\n",
            "Loss:  0.11566361784934998\n",
            "Loss:  0.11554477363824844\n",
            "Loss:  0.11542605608701706\n",
            "Loss:  0.11530740559101105\n",
            "Loss:  0.11518898606300354\n",
            "Loss:  0.11507067829370499\n",
            "Loss:  0.11495249718427658\n",
            "Loss:  0.11483441293239594\n",
            "Loss:  0.11471647024154663\n",
            "Loss:  0.11459861695766449\n",
            "Loss:  0.11448097974061966\n",
            "Loss:  0.11436346173286438\n",
            "Loss:  0.11424600332975388\n",
            "Loss:  0.11412873864173889\n",
            "Loss:  0.11401159316301346\n",
            "Loss:  0.11389455944299698\n",
            "Loss:  0.11377760022878647\n",
            "Loss:  0.11366088688373566\n",
            "Loss:  0.11354420334100723\n",
            "Loss:  0.11342769861221313\n",
            "Loss:  0.11331132054328918\n",
            "Loss:  0.11319508403539658\n",
            "Loss:  0.11307893693447113\n",
            "Loss:  0.11296291649341583\n",
            "Loss:  0.11284704506397247\n",
            "Loss:  0.11273128539323807\n",
            "Loss:  0.11261571198701859\n",
            "Loss:  0.11250018328428268\n",
            "Loss:  0.11238481849431992\n",
            "Loss:  0.1122695803642273\n",
            "Loss:  0.11215444654226303\n",
            "Loss:  0.1120394691824913\n",
            "Loss:  0.11192462593317032\n",
            "Loss:  0.1118098646402359\n",
            "Loss:  0.11169526726007462\n",
            "Loss:  0.11158078908920288\n",
            "Loss:  0.11146646738052368\n",
            "Loss:  0.11135222762823105\n",
            "Loss:  0.11123811453580856\n",
            "Loss:  0.11112412810325623\n",
            "Loss:  0.11101032048463821\n",
            "Loss:  0.11089656502008438\n",
            "Loss:  0.11078301072120667\n",
            "Loss:  0.11066950857639313\n",
            "Loss:  0.11055609583854675\n",
            "Loss:  0.11044290661811829\n",
            "Loss:  0.11032979935407639\n",
            "Loss:  0.11021684855222702\n",
            "Loss:  0.11010397970676422\n",
            "Loss:  0.10999125242233276\n",
            "Loss:  0.10987868160009384\n",
            "Loss:  0.1097661554813385\n",
            "Loss:  0.10965380072593689\n",
            "Loss:  0.10954156517982483\n",
            "Loss:  0.10942944884300232\n",
            "Loss:  0.10931747406721115\n",
            "Loss:  0.10920557379722595\n",
            "Loss:  0.1090938150882721\n",
            "Loss:  0.10898225009441376\n",
            "Loss:  0.10887070000171661\n",
            "Loss:  0.10875935852527618\n",
            "Loss:  0.10864808410406113\n",
            "Loss:  0.10853695869445801\n",
            "Loss:  0.10842594504356384\n",
            "Loss:  0.10831508040428162\n",
            "Loss:  0.10820430517196655\n",
            "Loss:  0.10809366405010223\n",
            "Loss:  0.10798311978578568\n",
            "Loss:  0.10787273198366165\n",
            "Loss:  0.107762411236763\n",
            "Loss:  0.10765229165554047\n",
            "Loss:  0.10754220932722092\n",
            "Loss:  0.1074323058128357\n",
            "Loss:  0.10732252150774002\n",
            "Loss:  0.10721282660961151\n",
            "Loss:  0.10710328817367554\n",
            "Loss:  0.10699381679296494\n",
            "Loss:  0.10688453912734985\n",
            "Loss:  0.10677534341812134\n",
            "Loss:  0.10666628181934357\n",
            "Loss:  0.10655729472637177\n",
            "Loss:  0.10644847899675369\n",
            "Loss:  0.10633975267410278\n",
            "Loss:  0.10623113811016083\n",
            "Loss:  0.10612272471189499\n",
            "Loss:  0.10601431876420975\n",
            "Loss:  0.10590604692697525\n",
            "Loss:  0.10579794645309448\n",
            "Loss:  0.10568992793560028\n",
            "Loss:  0.10558206588029861\n",
            "Loss:  0.10547427833080292\n",
            "Loss:  0.10536664724349976\n",
            "Loss:  0.10525909066200256\n",
            "Loss:  0.10515165328979492\n",
            "Loss:  0.10504436492919922\n",
            "Loss:  0.10493720322847366\n",
            "Loss:  0.10483016073703766\n",
            "Loss:  0.10472318530082703\n",
            "Loss:  0.10461635887622833\n",
            "Loss:  0.104509636759758\n",
            "Loss:  0.1044030487537384\n",
            "Loss:  0.10429660975933075\n",
            "Loss:  0.10419017821550369\n",
            "Loss:  0.10408394038677216\n",
            "Loss:  0.10397782921791077\n",
            "Loss:  0.10387176275253296\n",
            "Loss:  0.10376592725515366\n",
            "Loss:  0.10366012901067734\n",
            "Loss:  0.10355447977781296\n",
            "Loss:  0.10344891250133514\n",
            "Loss:  0.1033434271812439\n",
            "Loss:  0.10323811322450638\n",
            "Loss:  0.1031329333782196\n",
            "Loss:  0.1030278205871582\n",
            "Loss:  0.10292288661003113\n",
            "Loss:  0.10281800478696823\n",
            "Loss:  0.10271327942609787\n",
            "Loss:  0.10260864347219467\n",
            "Loss:  0.10250408202409744\n",
            "Loss:  0.10239971429109573\n",
            "Loss:  0.1022953912615776\n",
            "Loss:  0.1021912470459938\n",
            "Loss:  0.10208722949028015\n",
            "Loss:  0.10198326408863068\n",
            "Loss:  0.10187943279743195\n",
            "Loss:  0.10177569091320038\n",
            "Loss:  0.10167206078767776\n",
            "Loss:  0.10156862437725067\n",
            "Loss:  0.10146520286798477\n",
            "Loss:  0.10136193037033081\n",
            "Loss:  0.1012587919831276\n",
            "Loss:  0.10115572065114975\n",
            "Loss:  0.10105283558368683\n",
            "Loss:  0.10095002502202988\n",
            "Loss:  0.1008472889661789\n",
            "Loss:  0.10074471682310104\n",
            "Loss:  0.10064222663640976\n",
            "Loss:  0.10053981840610504\n",
            "Loss:  0.10043754428625107\n",
            "Loss:  0.10033540427684784\n",
            "Loss:  0.10023340582847595\n",
            "Loss:  0.10013147443532944\n",
            "Loss:  0.10002963989973068\n",
            "Loss:  0.09992793947458267\n",
            "Loss:  0.099826380610466\n",
            "Loss:  0.0997249186038971\n",
            "Loss:  0.09962354600429535\n",
            "Loss:  0.09952224045991898\n",
            "Loss:  0.09942112118005753\n",
            "Loss:  0.09932006895542145\n",
            "Loss:  0.09921915084123611\n",
            "Loss:  0.09911832213401794\n",
            "Loss:  0.09901761263608932\n",
            "Loss:  0.09891704469919205\n",
            "Loss:  0.09881655871868134\n",
            "Loss:  0.09871619939804077\n",
            "Loss:  0.09861587733030319\n",
            "Loss:  0.09851574152708054\n",
            "Loss:  0.09841565787792206\n",
            "Loss:  0.09831572324037552\n",
            "Loss:  0.09821590781211853\n",
            "Loss:  0.09811615198850632\n",
            "Loss:  0.09801656752824783\n",
            "Loss:  0.09791705012321472\n",
            "Loss:  0.09781762212514877\n",
            "Loss:  0.09771835058927536\n",
            "Loss:  0.0976191833615303\n",
            "Loss:  0.09752009063959122\n",
            "Loss:  0.09742110967636108\n",
            "Loss:  0.0973222479224205\n",
            "Loss:  0.09722352027893066\n",
            "Loss:  0.097124844789505\n",
            "Loss:  0.09702634066343307\n",
            "Loss:  0.09692788124084473\n",
            "Loss:  0.09682959318161011\n",
            "Loss:  0.09673133492469788\n",
            "Loss:  0.09663326293230057\n",
            "Loss:  0.09653527289628983\n",
            "Loss:  0.09643736481666565\n",
            "Loss:  0.09633956104516983\n",
            "Loss:  0.09624187648296356\n",
            "Loss:  0.09614432603120804\n",
            "Loss:  0.09604685008525848\n",
            "Loss:  0.09594951570034027\n",
            "Loss:  0.09585226327180862\n",
            "Loss:  0.09575506299734116\n",
            "Loss:  0.09565801918506622\n",
            "Loss:  0.09556106477975845\n",
            "Loss:  0.09546420723199844\n",
            "Loss:  0.09536747634410858\n",
            "Loss:  0.09527091681957245\n",
            "Loss:  0.09517435729503632\n",
            "Loss:  0.09507793188095093\n",
            "Loss:  0.09498164057731628\n",
            "Loss:  0.0948854461312294\n",
            "Loss:  0.09478933364152908\n",
            "Loss:  0.09469334781169891\n",
            "Loss:  0.09459741413593292\n",
            "Loss:  0.09450166672468185\n",
            "Loss:  0.09440598636865616\n",
            "Loss:  0.09431037306785583\n",
            "Loss:  0.09421491622924805\n",
            "Loss:  0.09411952644586563\n",
            "Loss:  0.09402429312467575\n",
            "Loss:  0.09392911195755005\n",
            "Loss:  0.09383406490087509\n",
            "Loss:  0.09373906999826431\n",
            "Loss:  0.09364420920610428\n",
            "Loss:  0.0935494601726532\n",
            "Loss:  0.09345481544733047\n",
            "Loss:  0.09336024522781372\n",
            "Loss:  0.09326579421758652\n",
            "Loss:  0.09317143261432648\n",
            "Loss:  0.09307722002267838\n",
            "Loss:  0.09298308193683624\n",
            "Loss:  0.09288903325796127\n",
            "Loss:  0.09279510378837585\n",
            "Loss:  0.09270129352807999\n",
            "Loss:  0.09260757267475128\n",
            "Loss:  0.09251390397548676\n",
            "Loss:  0.09242036938667297\n",
            "Loss:  0.09232693910598755\n",
            "Loss:  0.09223361313343048\n",
            "Loss:  0.09214040637016296\n",
            "Loss:  0.09204728156328201\n",
            "Loss:  0.09195425361394882\n",
            "Loss:  0.09186135232448578\n",
            "Loss:  0.09176848083734512\n",
            "Loss:  0.0916757583618164\n",
            "Loss:  0.09158312529325485\n",
            "Loss:  0.09149061888456345\n",
            "Loss:  0.0913981944322586\n",
            "Loss:  0.09130587428808212\n",
            "Loss:  0.09121367335319519\n",
            "Loss:  0.09112153947353363\n",
            "Loss:  0.09102953225374222\n",
            "Loss:  0.09093761444091797\n",
            "Loss:  0.09084576368331909\n",
            "Loss:  0.09075403958559036\n",
            "Loss:  0.09066244214773178\n",
            "Loss:  0.09057090431451797\n",
            "Loss:  0.09047947078943253\n",
            "Loss:  0.09038814902305603\n",
            "Loss:  0.09029695391654968\n",
            "Loss:  0.09020578116178513\n",
            "Loss:  0.0901147648692131\n",
            "Loss:  0.09002383053302765\n",
            "Loss:  0.08993298560380936\n",
            "Loss:  0.08984227478504181\n",
            "Loss:  0.08975163102149963\n",
            "Loss:  0.08966108411550522\n",
            "Loss:  0.08957063406705856\n",
            "Loss:  0.08948031067848206\n",
            "Loss:  0.08939004689455032\n",
            "Loss:  0.08929992467164993\n",
            "Loss:  0.08920986205339432\n",
            "Loss:  0.08911991864442825\n",
            "Loss:  0.08903007209300995\n",
            "Loss:  0.08894027769565582\n",
            "Loss:  0.08885067701339722\n",
            "Loss:  0.08876105397939682\n",
            "Loss:  0.08867163956165314\n",
            "Loss:  0.08858224749565125\n",
            "Loss:  0.0884929671883583\n",
            "Loss:  0.08840380609035492\n",
            "Loss:  0.08831470459699631\n",
            "Loss:  0.08822572976350784\n",
            "Loss:  0.08813684433698654\n",
            "Loss:  0.0880480483174324\n",
            "Loss:  0.08795934915542603\n",
            "Loss:  0.08787079155445099\n",
            "Loss:  0.08778224885463715\n",
            "Loss:  0.08769383281469345\n",
            "Loss:  0.0876055657863617\n",
            "Loss:  0.08751729875802994\n",
            "Loss:  0.08742917329072952\n",
            "Loss:  0.08734115958213806\n",
            "Loss:  0.08725322037935257\n",
            "Loss:  0.08716544508934021\n",
            "Loss:  0.08707764744758606\n",
            "Loss:  0.08699002116918564\n",
            "Loss:  0.08690247684717178\n",
            "Loss:  0.0868149921298027\n",
            "Loss:  0.08672764897346497\n",
            "Loss:  0.0866403803229332\n",
            "Loss:  0.08655320852994919\n",
            "Loss:  0.08646608144044876\n",
            "Loss:  0.08637914806604385\n",
            "Loss:  0.08629224449396133\n",
            "Loss:  0.08620542287826538\n",
            "Loss:  0.08611873537302017\n",
            "Loss:  0.08603212237358093\n",
            "Loss:  0.08594562858343124\n",
            "Loss:  0.08585915714502335\n",
            "Loss:  0.08577287942171097\n",
            "Loss:  0.0856865793466568\n",
            "Loss:  0.08560046553611755\n",
            "Loss:  0.08551439642906189\n",
            "Loss:  0.08542843908071518\n",
            "Loss:  0.08534257858991623\n",
            "Loss:  0.08525680750608444\n",
            "Loss:  0.08517111837863922\n",
            "Loss:  0.08508551865816116\n",
            "Loss:  0.08500000834465027\n",
            "Loss:  0.08491461724042892\n",
            "Loss:  0.08482930809259415\n",
            "Loss:  0.08474407345056534\n",
            "Loss:  0.08465895801782608\n",
            "Loss:  0.08457394689321518\n",
            "Loss:  0.08448898047208786\n",
            "Loss:  0.0844041183590889\n",
            "Loss:  0.0843193307518959\n",
            "Loss:  0.08423462510108948\n",
            "Loss:  0.08415008336305618\n",
            "Loss:  0.08406558632850647\n",
            "Loss:  0.08398117870092392\n",
            "Loss:  0.08389687538146973\n",
            "Loss:  0.0838126689195633\n",
            "Loss:  0.08372855931520462\n",
            "Loss:  0.08364450186491013\n",
            "Loss:  0.08356057107448578\n",
            "Loss:  0.083476722240448\n",
            "Loss:  0.08339295536279678\n",
            "Loss:  0.08330924808979034\n",
            "Loss:  0.08322569727897644\n",
            "Loss:  0.08314216881990433\n",
            "Loss:  0.08305875957012177\n",
            "Loss:  0.08297546207904816\n",
            "Loss:  0.08289225399494171\n",
            "Loss:  0.08280912041664124\n",
            "Loss:  0.08272606134414673\n",
            "Loss:  0.08264307677745819\n",
            "Loss:  0.08256024122238159\n",
            "Loss:  0.08247745782136917\n",
            "Loss:  0.08239477127790451\n",
            "Loss:  0.08231214433908463\n",
            "Loss:  0.0822296142578125\n",
            "Loss:  0.08214718848466873\n",
            "Loss:  0.08206488937139511\n",
            "Loss:  0.0819825753569603\n",
            "Loss:  0.0819004476070404\n",
            "Loss:  0.0818183645606041\n",
            "Loss:  0.08173636347055435\n",
            "Loss:  0.08165451139211655\n",
            "Loss:  0.08157269656658173\n",
            "Loss:  0.08149097114801407\n",
            "Loss:  0.08140938729047775\n",
            "Loss:  0.08132780343294144\n",
            "Loss:  0.08124638348817825\n",
            "Loss:  0.08116501569747925\n",
            "Loss:  0.08108367770910263\n",
            "Loss:  0.08100251853466034\n",
            "Loss:  0.08092141896486282\n",
            "Loss:  0.08084039390087128\n",
            "Loss:  0.0807594284415245\n",
            "Loss:  0.08067859709262848\n",
            "Loss:  0.08059784024953842\n",
            "Loss:  0.08051719516515732\n",
            "Loss:  0.08043656498193741\n",
            "Loss:  0.08035609126091003\n",
            "Loss:  0.08027566969394684\n",
            "Loss:  0.0801953375339508\n",
            "Loss:  0.08011511713266373\n",
            "Loss:  0.08003493398427963\n",
            "Loss:  0.07995487004518509\n",
            "Loss:  0.0798749327659607\n",
            "Loss:  0.07979495823383331\n",
            "Loss:  0.07971520721912384\n",
            "Loss:  0.07963547855615616\n",
            "Loss:  0.07955581694841385\n",
            "Loss:  0.07947628945112228\n",
            "Loss:  0.0793968066573143\n",
            "Loss:  0.07931742072105408\n",
            "Loss:  0.0792381539940834\n",
            "Loss:  0.07915889471769333\n",
            "Loss:  0.079079769551754\n",
            "Loss:  0.07900073379278183\n",
            "Loss:  0.07892178744077682\n",
            "Loss:  0.07884291559457779\n",
            "Loss:  0.07876411825418472\n",
            "Loss:  0.07868539541959763\n",
            "Loss:  0.07860679924488068\n",
            "Loss:  0.0785282552242279\n",
            "Loss:  0.0784497857093811\n",
            "Loss:  0.07837143540382385\n",
            "Loss:  0.07829315215349197\n",
            "Loss:  0.07821497321128845\n",
            "Loss:  0.07813683897256851\n",
            "Loss:  0.07805879414081573\n",
            "Loss:  0.07798086106777191\n",
            "Loss:  0.07790298014879227\n",
            "Loss:  0.07782521843910217\n",
            "Loss:  0.07774747908115387\n",
            "Loss:  0.07766987383365631\n",
            "Loss:  0.07759235054254532\n",
            "Loss:  0.0775148794054985\n",
            "Loss:  0.07743752002716064\n",
            "Loss:  0.07736026495695114\n",
            "Loss:  0.07728305459022522\n",
            "Loss:  0.07720596343278885\n",
            "Loss:  0.07712891697883606\n",
            "Loss:  0.07705192267894745\n",
            "Loss:  0.0769750326871872\n",
            "Loss:  0.07689829170703888\n",
            "Loss:  0.07682155072689056\n",
            "Loss:  0.07674497365951538\n",
            "Loss:  0.07666835188865662\n",
            "Loss:  0.07659193873405457\n",
            "Loss:  0.07651554048061371\n",
            "Loss:  0.07643929123878479\n",
            "Loss:  0.0763629823923111\n",
            "Loss:  0.07628690451383591\n",
            "Loss:  0.07621084153652191\n",
            "Loss:  0.07613486051559448\n",
            "Loss:  0.07605894654989243\n",
            "Loss:  0.07598315179347992\n",
            "Loss:  0.07590746134519577\n",
            "Loss:  0.07583177834749222\n",
            "Loss:  0.07575623691082001\n",
            "Loss:  0.07568071782588959\n",
            "Loss:  0.0756053552031517\n",
            "Loss:  0.07552998512983322\n",
            "Loss:  0.07545475661754608\n",
            "Loss:  0.0753796175122261\n",
            "Loss:  0.07530451565980911\n",
            "Loss:  0.07522949576377869\n",
            "Loss:  0.07515459507703781\n",
            "Loss:  0.07507973909378052\n",
            "Loss:  0.07500496506690979\n",
            "Loss:  0.07493031024932861\n",
            "Loss:  0.07485567778348923\n",
            "Loss:  0.07478117197751999\n",
            "Loss:  0.07470674812793732\n",
            "Loss:  0.07463237643241882\n",
            "Loss:  0.0745580792427063\n",
            "Loss:  0.07448386400938034\n",
            "Loss:  0.07440975308418274\n",
            "Loss:  0.07433569431304932\n",
            "Loss:  0.07426173985004425\n",
            "Loss:  0.07418781518936157\n",
            "Loss:  0.07411406189203262\n",
            "Loss:  0.07404033094644547\n",
            "Loss:  0.07396665215492249\n",
            "Loss:  0.07389307022094727\n",
            "Loss:  0.07381957769393921\n",
            "Loss:  0.07374615967273712\n",
            "Loss:  0.0736728310585022\n",
            "Loss:  0.07359956949949265\n",
            "Loss:  0.07352636754512787\n",
            "Loss:  0.07345326244831085\n",
            "Loss:  0.07338022440671921\n",
            "Loss:  0.07330727577209473\n",
            "Loss:  0.07323437929153442\n",
            "Loss:  0.07316160202026367\n",
            "Loss:  0.0730888843536377\n",
            "Loss:  0.0730162039399147\n",
            "Loss:  0.07294369488954544\n",
            "Loss:  0.07287118583917618\n",
            "Loss:  0.07279877364635468\n",
            "Loss:  0.07272645831108093\n",
            "Loss:  0.07265423983335495\n",
            "Loss:  0.07258202880620956\n",
            "Loss:  0.07250992953777313\n",
            "Loss:  0.07243788987398148\n",
            "Loss:  0.07236600667238235\n",
            "Loss:  0.07229408621788025\n",
            "Loss:  0.07222229242324829\n",
            "Loss:  0.07215059548616409\n",
            "Loss:  0.07207892090082169\n",
            "Loss:  0.07200739532709122\n",
            "Loss:  0.07193589210510254\n",
            "Loss:  0.07186446338891983\n",
            "Loss:  0.07179315388202667\n",
            "Loss:  0.07172191888093948\n",
            "Loss:  0.07165071368217468\n",
            "Loss:  0.07157958298921585\n",
            "Loss:  0.07150860130786896\n",
            "Loss:  0.07143764197826385\n",
            "Loss:  0.07136672735214233\n",
            "Loss:  0.07129590958356857\n",
            "Loss:  0.07122519612312317\n",
            "Loss:  0.07115453481674194\n",
            "Loss:  0.07108394056558609\n",
            "Loss:  0.07101345807313919\n",
            "Loss:  0.07094303518533707\n",
            "Loss:  0.07087264209985733\n",
            "Loss:  0.07080239802598953\n",
            "Loss:  0.07073217630386353\n",
            "Loss:  0.07066205143928528\n",
            "Loss:  0.07059202343225479\n",
            "Loss:  0.0705219954252243\n",
            "Loss:  0.07045207917690277\n",
            "Loss:  0.0703822672367096\n",
            "Loss:  0.0703125\n",
            "Loss:  0.07024285942316055\n",
            "Loss:  0.07017319649457932\n",
            "Loss:  0.07010367512702942\n",
            "Loss:  0.07003423571586609\n",
            "Loss:  0.06996488571166992\n",
            "Loss:  0.06989553570747375\n",
            "Loss:  0.06982629746198654\n",
            "Loss:  0.0697571337223053\n",
            "Loss:  0.06968802213668823\n",
            "Loss:  0.06961898505687714\n",
            "Loss:  0.0695500448346138\n",
            "Loss:  0.06948119401931763\n",
            "Loss:  0.06941239535808563\n",
            "Loss:  0.06934365630149841\n",
            "Loss:  0.06927500665187836\n",
            "Loss:  0.06920645385980606\n",
            "Loss:  0.06913793832063675\n",
            "Loss:  0.069069504737854\n",
            "Loss:  0.06900111585855484\n",
            "Loss:  0.06893283128738403\n",
            "Loss:  0.0688646137714386\n",
            "Loss:  0.06879646331071854\n",
            "Loss:  0.06872837990522385\n",
            "Loss:  0.06866039335727692\n",
            "Loss:  0.06859244406223297\n",
            "Loss:  0.068524569272995\n",
            "Loss:  0.06845679134130478\n",
            "Loss:  0.06838909536600113\n",
            "Loss:  0.06832145154476166\n",
            "Loss:  0.06825389713048935\n",
            "Loss:  0.06818639487028122\n",
            "Loss:  0.06811895966529846\n",
            "Loss:  0.06805160641670227\n",
            "Loss:  0.06798435747623444\n",
            "Loss:  0.0679171234369278\n",
            "Loss:  0.06784995645284653\n",
            "Loss:  0.0677829310297966\n",
            "Loss:  0.06771589070558548\n",
            "Loss:  0.0676489770412445\n",
            "Loss:  0.06758210808038712\n",
            "Loss:  0.06751533597707748\n",
            "Loss:  0.06744860857725143\n",
            "Loss:  0.06738195568323135\n",
            "Loss:  0.06731539964675903\n",
            "Loss:  0.06724891066551208\n",
            "Loss:  0.06718245148658752\n",
            "Loss:  0.06711611896753311\n",
            "Loss:  0.06704983115196228\n",
            "Loss:  0.06698360294103622\n",
            "Loss:  0.06691745668649673\n",
            "Loss:  0.06685134768486023\n",
            "Loss:  0.06678534299135208\n",
            "Loss:  0.06671939045190811\n",
            "Loss:  0.0666535273194313\n",
            "Loss:  0.06658770889043808\n",
            "Loss:  0.06652204692363739\n",
            "Loss:  0.06645634025335312\n",
            "Loss:  0.06639076769351959\n",
            "Loss:  0.06632523238658905\n",
            "Loss:  0.06625976413488388\n",
            "Loss:  0.06619441509246826\n",
            "Loss:  0.06612910330295563\n",
            "Loss:  0.06606385111808777\n",
            "Loss:  0.06599867343902588\n",
            "Loss:  0.06593354046344757\n",
            "Loss:  0.06586853414773941\n",
            "Loss:  0.06580351293087006\n",
            "Loss:  0.06573863327503204\n",
            "Loss:  0.0656738206744194\n",
            "Loss:  0.06560906022787094\n",
            "Loss:  0.06554435193538666\n",
            "Loss:  0.06547971069812775\n",
            "Loss:  0.0654151663184166\n",
            "Loss:  0.06535065919160843\n",
            "Loss:  0.06528628617525101\n",
            "Loss:  0.0652218908071518\n",
            "Loss:  0.06515762209892273\n",
            "Loss:  0.06509339064359665\n",
            "Loss:  0.06502923369407654\n",
            "Loss:  0.06496517360210419\n",
            "Loss:  0.06490111351013184\n",
            "Loss:  0.06483719497919083\n",
            "Loss:  0.0647733062505722\n",
            "Loss:  0.06470949947834015\n",
            "Loss:  0.06464577466249466\n",
            "Loss:  0.06458208709955215\n",
            "Loss:  0.06451845914125443\n",
            "Loss:  0.06445492058992386\n",
            "Loss:  0.06439147144556046\n",
            "Loss:  0.06432806700468063\n",
            "Loss:  0.0642646849155426\n",
            "Loss:  0.06420141458511353\n",
            "Loss:  0.0641382485628128\n",
            "Loss:  0.06407506763935089\n",
            "Loss:  0.06401199102401733\n",
            "Loss:  0.06394895911216736\n",
            "Loss:  0.06388608366250992\n",
            "Loss:  0.06382317841053009\n",
            "Loss:  0.06376039236783981\n",
            "Loss:  0.06369763612747192\n",
            "Loss:  0.0636349469423294\n",
            "Loss:  0.06357234716415405\n",
            "Loss:  0.06350977718830109\n",
            "Loss:  0.06344734132289886\n",
            "Loss:  0.06338489800691605\n",
            "Loss:  0.0633225291967392\n",
            "Loss:  0.0632602870464325\n",
            "Loss:  0.06319806724786758\n",
            "Loss:  0.06313593685626984\n",
            "Loss:  0.06307382881641388\n",
            "Loss:  0.0630117803812027\n",
            "Loss:  0.06294983625411987\n",
            "Loss:  0.0628879964351654\n",
            "Loss:  0.06282615661621094\n",
            "Loss:  0.06276439875364304\n",
            "Loss:  0.0627027153968811\n",
            "Loss:  0.06264106929302216\n",
            "Loss:  0.06257949769496918\n",
            "Loss:  0.06251801550388336\n",
            "Loss:  0.062456563115119934\n",
            "Loss:  0.06239522993564606\n",
            "Loss:  0.06233393773436546\n",
            "Loss:  0.06227269023656845\n",
            "Loss:  0.06221149116754532\n",
            "Loss:  0.06215040385723114\n",
            "Loss:  0.062089353799819946\n",
            "Loss:  0.06202835962176323\n",
            "Loss:  0.061967428773641586\n",
            "Loss:  0.0619066096842289\n",
            "Loss:  0.06184579059481621\n",
            "Loss:  0.061785101890563965\n",
            "Loss:  0.06172442436218262\n",
            "Loss:  0.06166383624076843\n",
            "Loss:  0.061603277921676636\n",
            "Loss:  0.061542823910713196\n",
            "Loss:  0.06148241087794304\n",
            "Loss:  0.06142209842801094\n",
            "Loss:  0.06136181950569153\n",
            "Loss:  0.06130160763859749\n",
            "Loss:  0.06124145910143852\n",
            "Loss:  0.061181362718343735\n",
            "Loss:  0.061121340841054916\n",
            "Loss:  0.06106136739253998\n",
            "Loss:  0.0610014908015728\n",
            "Loss:  0.0609416663646698\n",
            "Loss:  0.06088186427950859\n",
            "Loss:  0.06082214415073395\n",
            "Loss:  0.06076250225305557\n",
            "Loss:  0.060702934861183167\n",
            "Loss:  0.06064338609576225\n",
            "Loss:  0.06058395281434059\n",
            "Loss:  0.06052456423640251\n",
            "Loss:  0.060465261340141296\n",
            "Loss:  0.06040596961975098\n",
            "Loss:  0.06034674495458603\n",
            "Loss:  0.06028759852051735\n",
            "Loss:  0.06022851914167404\n",
            "Loss:  0.0601695254445076\n",
            "Loss:  0.06011056900024414\n",
            "Loss:  0.060051675885915756\n",
            "Loss:  0.05999280884861946\n",
            "Loss:  0.05993403494358063\n",
            "Loss:  0.059875357896089554\n",
            "Loss:  0.05981668457388878\n",
            "Loss:  0.05975810065865517\n",
            "Loss:  0.059699565172195435\n",
            "Loss:  0.059641122817993164\n",
            "Loss:  0.05958271026611328\n",
            "Loss:  0.05952436476945877\n",
            "Loss:  0.05946608632802963\n",
            "Loss:  0.059407878667116165\n",
            "Loss:  0.059349700808525085\n",
            "Loss:  0.05929159000515938\n",
            "Loss:  0.05923360958695412\n",
            "Loss:  0.05917559191584587\n",
            "Loss:  0.05911768972873688\n",
            "Loss:  0.059059835970401764\n",
            "Loss:  0.059002019464969635\n",
            "Loss:  0.05894431471824646\n",
            "Loss:  0.05888664349913597\n",
            "Loss:  0.058829035609960556\n",
            "Loss:  0.05877145752310753\n",
            "Loss:  0.05871402844786644\n",
            "Loss:  0.05865652486681938\n",
            "Loss:  0.058599166572093964\n",
            "Loss:  0.058541879057884216\n",
            "Loss:  0.05848461762070656\n",
            "Loss:  0.058427467942237854\n",
            "Loss:  0.05837032198905945\n",
            "Loss:  0.05831323564052582\n",
            "Loss:  0.058256253600120544\n",
            "Loss:  0.05819930508732796\n",
            "Loss:  0.05814240127801895\n",
            "Loss:  0.058085594326257706\n",
            "Loss:  0.058028798550367355\n",
            "Loss:  0.05797210708260536\n",
            "Loss:  0.05791547894477844\n",
            "Loss:  0.05785883590579033\n",
            "Loss:  0.057802289724349976\n",
            "Loss:  0.057745881378650665\n",
            "Loss:  0.057689446955919266\n",
            "Loss:  0.05763307586312294\n",
            "Loss:  0.05757680535316467\n",
            "Loss:  0.05752057954668999\n",
            "Loss:  0.0574643649160862\n",
            "Loss:  0.05740826949477196\n",
            "Loss:  0.057352207601070404\n",
            "Loss:  0.05729619041085243\n",
            "Loss:  0.057240232825279236\n",
            "Loss:  0.057184360921382904\n",
            "Loss:  0.057128552347421646\n",
            "Loss:  0.05707278102636337\n",
            "Loss:  0.05701708793640137\n",
            "Loss:  0.05696142837405205\n",
            "Loss:  0.05690580978989601\n",
            "Loss:  0.05685028061270714\n",
            "Loss:  0.05679486691951752\n",
            "Loss:  0.05673939734697342\n",
            "Loss:  0.05668407306075096\n",
            "Loss:  0.0566287487745285\n",
            "Loss:  0.0565735287964344\n",
            "Loss:  0.056518327444791794\n",
            "Loss:  0.05646319314837456\n",
            "Loss:  0.05640818178653717\n",
            "Loss:  0.0563531219959259\n",
            "Loss:  0.05629818141460419\n",
            "Loss:  0.056243252009153366\n",
            "Loss:  0.0561884269118309\n",
            "Loss:  0.05613364279270172\n",
            "Loss:  0.05607892945408821\n",
            "Loss:  0.05602426454424858\n",
            "Loss:  0.05596964806318283\n",
            "Loss:  0.05591508373618126\n",
            "Loss:  0.05586061626672745\n",
            "Loss:  0.05580615624785423\n",
            "Loss:  0.055751752108335495\n",
            "Loss:  0.05569743737578392\n",
            "Loss:  0.05564315244555473\n",
            "Loss:  0.0555889718234539\n",
            "Loss:  0.055534809827804565\n",
            "Loss:  0.055480703711509705\n",
            "Loss:  0.05542668327689171\n",
            "Loss:  0.0553727000951767\n",
            "Loss:  0.05531877651810646\n",
            "Loss:  0.055264879018068314\n",
            "Loss:  0.05521107092499733\n",
            "Loss:  0.05515731871128082\n",
            "Loss:  0.0551036112010479\n",
            "Loss:  0.05504998564720154\n",
            "Loss:  0.054996393620967865\n",
            "Loss:  0.05494284629821777\n",
            "Loss:  0.054889366030693054\n",
            "Loss:  0.054835960268974304\n",
            "Loss:  0.05478259176015854\n",
            "Loss:  0.05472926050424576\n",
            "Loss:  0.05467600375413895\n",
            "Loss:  0.054622799158096313\n",
            "Loss:  0.05456967279314995\n",
            "Loss:  0.05451658368110657\n",
            "Loss:  0.05446352809667587\n",
            "Loss:  0.05441057309508324\n",
            "Loss:  0.054357632994651794\n",
            "Loss:  0.054304782301187515\n",
            "Loss:  0.05425199121236801\n",
            "Loss:  0.054199203848838806\n",
            "Loss:  0.054146505892276764\n",
            "Loss:  0.05409383773803711\n",
            "Loss:  0.05404128134250641\n",
            "Loss:  0.05398871749639511\n",
            "Loss:  0.05393624305725098\n",
            "Loss:  0.05388383939862251\n",
            "Loss:  0.053831443190574646\n",
            "Loss:  0.053779128938913345\n",
            "Loss:  0.053726863116025925\n",
            "Loss:  0.053674641996622086\n",
            "Loss:  0.053622495383024216\n",
            "Loss:  0.05357037112116814\n",
            "Loss:  0.05351833254098892\n",
            "Loss:  0.05346635729074478\n",
            "Loss:  0.05341440066695213\n",
            "Loss:  0.05336253345012665\n",
            "Loss:  0.05331071466207504\n",
            "Loss:  0.05325893312692642\n",
            "Loss:  0.0532071590423584\n",
            "Loss:  0.053155556321144104\n",
            "Loss:  0.05310393497347832\n",
            "Loss:  0.0530523881316185\n",
            "Loss:  0.05300082638859749\n",
            "Loss:  0.05294940993189812\n",
            "Loss:  0.05289802700281143\n",
            "Loss:  0.05284665897488594\n",
            "Loss:  0.052795398980379105\n",
            "Loss:  0.05274413153529167\n",
            "Loss:  0.0526929646730423\n",
            "Loss:  0.0526418462395668\n",
            "Loss:  0.052590757608413696\n",
            "Loss:  0.05253971740603447\n",
            "Loss:  0.05248875543475151\n",
            "Loss:  0.052437830716371536\n",
            "Loss:  0.05238696560263634\n",
            "Loss:  0.052336160093545914\n",
            "Loss:  0.052285388112068176\n",
            "Loss:  0.05223468691110611\n",
            "Loss:  0.05218403413891792\n",
            "Loss:  0.05213343724608421\n",
            "Loss:  0.05208289250731468\n",
            "Loss:  0.052032388746738434\n",
            "Loss:  0.051981955766677856\n",
            "Loss:  0.05193157121539116\n",
            "Loss:  0.05188124254345894\n",
            "Loss:  0.05183095484972\n",
            "Loss:  0.05178070068359375\n",
            "Loss:  0.05173055827617645\n",
            "Loss:  0.05168037489056587\n",
            "Loss:  0.051630303263664246\n",
            "Loss:  0.05158030614256859\n",
            "Loss:  0.05153031647205353\n",
            "Loss:  0.05148040130734444\n",
            "Loss:  0.05143051594495773\n",
            "Loss:  0.05138073116540909\n",
            "Loss:  0.05133094638586044\n",
            "Loss:  0.051281269639730453\n",
            "Loss:  0.05123160034418106\n",
            "Loss:  0.05118197947740555\n",
            "Loss:  0.051132407039403915\n",
            "Loss:  0.05108291283249855\n",
            "Loss:  0.05103343352675438\n",
            "Loss:  0.05098404362797737\n",
            "Loss:  0.05093468353152275\n",
            "Loss:  0.0508854016661644\n",
            "Loss:  0.05083614960312843\n",
            "Loss:  0.050786953419446945\n",
            "Loss:  0.05073780566453934\n",
            "Loss:  0.050688691437244415\n",
            "Loss:  0.050639696419239044\n",
            "Loss:  0.050590649247169495\n",
            "Loss:  0.050541721284389496\n",
            "Loss:  0.050492819398641586\n",
            "Loss:  0.05044398829340935\n",
            "Loss:  0.05039522051811218\n",
            "Loss:  0.05034647881984711\n",
            "Loss:  0.050297804176807404\n",
            "Loss:  0.05024915188550949\n",
            "Loss:  0.050200559198856354\n",
            "Loss:  0.0501520149409771\n",
            "Loss:  0.05010354518890381\n",
            "Loss:  0.05005508288741112\n",
            "Loss:  0.0500066801905632\n",
            "Loss:  0.04995834454894066\n",
            "Loss:  0.049910079687833786\n",
            "Loss:  0.04986181855201721\n",
            "Loss:  0.0498136505484581\n",
            "Loss:  0.049765512347221375\n",
            "Loss:  0.049717411398887634\n",
            "Loss:  0.04966939985752106\n",
            "Loss:  0.04962138831615448\n",
            "Loss:  0.04957347735762596\n",
            "Loss:  0.049525585025548935\n",
            "Loss:  0.04947775974869728\n",
            "Loss:  0.04942996799945831\n",
            "Loss:  0.04938222095370293\n",
            "Loss:  0.049334555864334106\n",
            "Loss:  0.04928690567612648\n",
            "Loss:  0.049239322543144226\n",
            "Loss:  0.04919179528951645\n",
            "Loss:  0.04914430156350136\n",
            "Loss:  0.04909684509038925\n",
            "Loss:  0.04904947429895401\n",
            "Loss:  0.04900212213397026\n",
            "Loss:  0.048954807221889496\n",
            "Loss:  0.048907581716775894\n",
            "Loss:  0.04886038228869438\n",
            "Loss:  0.04881322383880615\n",
            "Loss:  0.0487661138176918\n",
            "Loss:  0.04871905967593193\n",
            "Loss:  0.048672083765268326\n",
            "Loss:  0.048625096678733826\n",
            "Loss:  0.048578206449747086\n",
            "Loss:  0.048531368374824524\n",
            "Loss:  0.048484526574611664\n",
            "Loss:  0.04843779653310776\n",
            "Loss:  0.04839104041457176\n",
            "Loss:  0.04834441840648651\n",
            "Loss:  0.048297811299562454\n",
            "Loss:  0.04825124144554138\n",
            "Loss:  0.04820474609732628\n",
            "Loss:  0.04815826565027237\n",
            "Loss:  0.04811185970902443\n",
            "Loss:  0.048065461218357086\n",
            "Loss:  0.04801918938755989\n",
            "Loss:  0.04797288775444031\n",
            "Loss:  0.047926679253578186\n",
            "Loss:  0.04788048192858696\n",
            "Loss:  0.0478343591094017\n",
            "Loss:  0.047788284718990326\n",
            "Loss:  0.04774225130677223\n",
            "Loss:  0.04769626259803772\n",
            "Loss:  0.04765031859278679\n",
            "Loss:  0.04760439693927765\n",
            "Loss:  0.04755859076976776\n",
            "Loss:  0.04751279950141907\n",
            "Loss:  0.04746703803539276\n",
            "Loss:  0.04742133989930153\n",
            "Loss:  0.04737568274140358\n",
            "Loss:  0.047330085188150406\n",
            "Loss:  0.04728451371192932\n",
            "Loss:  0.04723898321390152\n",
            "Loss:  0.04719354584813118\n",
            "Loss:  0.04714811220765114\n",
            "Loss:  0.04710271954536438\n",
            "Loss:  0.04705740138888359\n",
            "Loss:  0.047012120485305786\n",
            "Loss:  0.04696691408753395\n",
            "Loss:  0.046921711415052414\n",
            "Loss:  0.04687657952308655\n",
            "Loss:  0.04683146998286247\n",
            "Loss:  0.04678643122315407\n",
            "Loss:  0.04674142971634865\n",
            "Loss:  0.046696487814188004\n",
            "Loss:  0.046651557087898254\n",
            "Loss:  0.04660670831799507\n",
            "Loss:  0.04656190425157547\n",
            "Loss:  0.04651710018515587\n",
            "Loss:  0.046472400426864624\n",
            "Loss:  0.04642772302031517\n",
            "Loss:  0.046383097767829895\n",
            "Loss:  0.04633850231766701\n",
            "Loss:  0.04629400372505188\n",
            "Loss:  0.046249475330114365\n",
            "Loss:  0.046205054968595505\n",
            "Loss:  0.04616064205765724\n",
            "Loss:  0.04611629992723465\n",
            "Loss:  0.04607197269797325\n",
            "Loss:  0.04602772369980812\n",
            "Loss:  0.04598350450396538\n",
            "Loss:  0.04593932256102562\n",
            "Loss:  0.045895181596279144\n",
            "Loss:  0.04585110768675804\n",
            "Loss:  0.045807093381881714\n",
            "Loss:  0.045763082802295685\n",
            "Loss:  0.04571915045380592\n",
            "Loss:  0.04567524045705795\n",
            "Loss:  0.045631419867277145\n",
            "Loss:  0.04558758810162544\n",
            "Loss:  0.04554382339119911\n",
            "Loss:  0.045500125735998154\n",
            "Loss:  0.045456454157829285\n",
            "Loss:  0.045412827283144\n",
            "Loss:  0.04536925628781319\n",
            "Loss:  0.04532570764422417\n",
            "Loss:  0.04528222605586052\n",
            "Loss:  0.04523878172039986\n",
            "Loss:  0.04519536346197128\n",
            "Loss:  0.04515203833580017\n",
            "Loss:  0.04510870948433876\n",
            "Loss:  0.04506545886397362\n",
            "Loss:  0.04502223804593086\n",
            "Loss:  0.04497907683253288\n",
            "Loss:  0.044935911893844604\n",
            "Loss:  0.04489285126328468\n",
            "Loss:  0.04484979063272476\n",
            "Loss:  0.04480680450797081\n",
            "Loss:  0.044763870537281036\n",
            "Loss:  0.04472094401717186\n",
            "Loss:  0.044678036123514175\n",
            "Loss:  0.04463523253798485\n",
            "Loss:  0.0445924811065197\n",
            "Loss:  0.044549766927957535\n",
            "Loss:  0.044507090002298355\n",
            "Loss:  0.044464413076639175\n",
            "Loss:  0.04442179575562477\n",
            "Loss:  0.044379234313964844\n",
            "Loss:  0.044336721301078796\n",
            "Loss:  0.04429423436522484\n",
            "Loss:  0.04425181820988655\n",
            "Loss:  0.044209446758031845\n",
            "Loss:  0.04416707158088684\n",
            "Loss:  0.044124796986579895\n",
            "Loss:  0.04408253729343414\n",
            "Loss:  0.044040318578481674\n",
            "Loss:  0.04399813339114189\n",
            "Loss:  0.04395602270960808\n",
            "Loss:  0.04391392692923546\n",
            "Loss:  0.04387188330292702\n",
            "Loss:  0.04382989928126335\n",
            "Loss:  0.04378797113895416\n",
            "Loss:  0.04374605789780617\n",
            "Loss:  0.04370419681072235\n",
            "Loss:  0.043662361800670624\n",
            "Loss:  0.04362059757113457\n",
            "Loss:  0.0435788594186306\n",
            "Loss:  0.04353717714548111\n",
            "Loss:  0.0434955470263958\n",
            "Loss:  0.043453920632600784\n",
            "Loss:  0.04341236129403114\n",
            "Loss:  0.043370846658945084\n",
            "Loss:  0.04332934692502022\n",
            "Loss:  0.04328791797161102\n",
            "Loss:  0.04324653744697571\n",
            "Loss:  0.04320518672466278\n",
            "Loss:  0.04316387698054314\n",
            "Loss:  0.043122611939907074\n",
            "Loss:  0.043081387877464294\n",
            "Loss:  0.0430402047932148\n",
            "Loss:  0.042999085038900375\n",
            "Loss:  0.04295800253748894\n",
            "Loss:  0.042916931211948395\n",
            "Loss:  0.04287588968873024\n",
            "Loss:  0.04283493384718895\n",
            "Loss:  0.04279400780797005\n",
            "Loss:  0.04275311902165413\n",
            "Loss:  0.042712267488241196\n",
            "Loss:  0.04267149791121483\n",
            "Loss:  0.04263072460889816\n",
            "Loss:  0.042590055614709854\n",
            "Loss:  0.04254935309290886\n",
            "Loss:  0.042508698999881744\n",
            "Loss:  0.0424681082367897\n",
            "Loss:  0.042427584528923035\n",
            "Loss:  0.04238707572221756\n",
            "Loss:  0.04234660044312477\n",
            "Loss:  0.04230620712041855\n",
            "Loss:  0.04226581007242203\n",
            "Loss:  0.0422254353761673\n",
            "Loss:  0.04218515381217003\n",
            "Loss:  0.04214486479759216\n",
            "Loss:  0.04210464656352997\n",
            "Loss:  0.04206446558237076\n",
            "Loss:  0.042024340480566025\n",
            "Loss:  0.041984234005212784\n",
            "Loss:  0.041944194585084915\n",
            "Loss:  0.04190418869256973\n",
            "Loss:  0.04186422750353813\n",
            "Loss:  0.04182427376508713\n",
            "Loss:  0.04178440198302269\n",
            "Loss:  0.04174455255270004\n",
            "Loss:  0.04170477017760277\n",
            "Loss:  0.041664980351924896\n",
            "Loss:  0.0416252575814724\n",
            "Loss:  0.041585586965084076\n",
            "Loss:  0.04154593497514725\n",
            "Loss:  0.0415063202381134\n",
            "Loss:  0.04146674647927284\n",
            "Loss:  0.041427209973335266\n",
            "Loss:  0.04138777405023575\n",
            "Loss:  0.04134829342365265\n",
            "Loss:  0.04130889102816582\n",
            "Loss:  0.041269537061452866\n",
            "Loss:  0.04123023524880409\n",
            "Loss:  0.04119093716144562\n",
            "Loss:  0.041151728481054306\n",
            "Loss:  0.04111252725124359\n",
            "Loss:  0.04107334092259407\n",
            "Loss:  0.04103425145149231\n",
            "Loss:  0.04099515825510025\n",
            "Loss:  0.04095613583922386\n",
            "Loss:  0.04091709479689598\n",
            "Loss:  0.040878139436244965\n",
            "Loss:  0.040839213877916336\n",
            "Loss:  0.04080033674836159\n",
            "Loss:  0.040761493146419525\n",
            "Loss:  0.04072272777557373\n",
            "Loss:  0.04068394750356674\n",
            "Loss:  0.04064525291323662\n",
            "Loss:  0.0406065508723259\n",
            "Loss:  0.04056793078780174\n",
            "Loss:  0.040529314428567886\n",
            "Loss:  0.040490761399269104\n",
            "Loss:  0.04045221209526062\n",
            "Loss:  0.0404137559235096\n",
            "Loss:  0.040375277400016785\n",
            "Loss:  0.04033692181110382\n",
            "Loss:  0.04029853641986847\n",
            "Loss:  0.04026021435856819\n",
            "Loss:  0.0402219295501709\n",
            "Loss:  0.0401836633682251\n",
            "Loss:  0.04014549404382706\n",
            "Loss:  0.04010731354355812\n",
            "Loss:  0.04006917029619217\n",
            "Loss:  0.04003109037876129\n",
            "Loss:  0.039993006736040115\n",
            "Loss:  0.0399550199508667\n",
            "Loss:  0.039917029440402985\n",
            "Loss:  0.03987909108400345\n",
            "Loss:  0.03984121233224869\n",
            "Loss:  0.039803363382816315\n",
            "Loss:  0.039765533059835434\n",
            "Loss:  0.03972773626446724\n",
            "Loss:  0.03969000279903412\n",
            "Loss:  0.03965231031179428\n",
            "Loss:  0.03961465507745743\n",
            "Loss:  0.03957701846957207\n",
            "Loss:  0.03953944519162178\n",
            "Loss:  0.03950188308954239\n",
            "Loss:  0.03946434706449509\n",
            "Loss:  0.03942691907286644\n",
            "Loss:  0.0393894761800766\n",
            "Loss:  0.03935207054018974\n",
            "Loss:  0.03931470960378647\n",
            "Loss:  0.03927740082144737\n",
            "Loss:  0.039240121841430664\n",
            "Loss:  0.03920286148786545\n",
            "Loss:  0.03916565701365471\n",
            "Loss:  0.03912849724292755\n",
            "Loss:  0.039091359823942184\n",
            "Loss:  0.03905428573489189\n",
            "Loss:  0.039017222821712494\n",
            "Loss:  0.03898020088672638\n",
            "Loss:  0.03894321620464325\n",
            "Loss:  0.0389062836766243\n",
            "Loss:  0.038869380950927734\n",
            "Loss:  0.03883252292871475\n",
            "Loss:  0.038795698434114456\n",
            "Loss:  0.03875890374183655\n",
            "Loss:  0.03872215002775192\n",
            "Loss:  0.03868541866540909\n",
            "Loss:  0.03864874318242073\n",
            "Loss:  0.038612108677625656\n",
            "Loss:  0.03857552632689476\n",
            "Loss:  0.03853897005319595\n",
            "Loss:  0.03850242495536804\n",
            "Loss:  0.03846597298979759\n",
            "Loss:  0.038429468870162964\n",
            "Loss:  0.03839308023452759\n",
            "Loss:  0.038356684148311615\n",
            "Loss:  0.03832034766674042\n",
            "Loss:  0.03828403726220131\n",
            "Loss:  0.03824777528643608\n",
            "Loss:  0.03821153938770294\n",
            "Loss:  0.038175370544195175\n",
            "Loss:  0.03813920170068741\n",
            "Loss:  0.03810306265950203\n",
            "Loss:  0.03806700184941292\n",
            "Loss:  0.0380309522151947\n",
            "Loss:  0.03799492120742798\n",
            "Loss:  0.03795897215604782\n",
            "Loss:  0.037923045456409454\n",
            "Loss:  0.03788713738322258\n",
            "Loss:  0.037851277738809586\n",
            "Loss:  0.03781542927026749\n",
            "Loss:  0.037779636681079865\n",
            "Loss:  0.03774389997124672\n",
            "Loss:  0.037708181887865067\n",
            "Loss:  0.0376724898815155\n",
            "Loss:  0.037636853754520416\n",
            "Loss:  0.03760126605629921\n",
            "Loss:  0.0375656932592392\n",
            "Loss:  0.03753014653921127\n",
            "Loss:  0.03749464824795723\n",
            "Loss:  0.037459198385477066\n",
            "Loss:  0.0374237596988678\n",
            "Loss:  0.03738836199045181\n",
            "Loss:  0.03735301271080971\n",
            "Loss:  0.037317704409360886\n",
            "Loss:  0.03728242963552475\n",
            "Loss:  0.037247177213430405\n",
            "Loss:  0.03721194341778755\n",
            "Loss:  0.03717677295207977\n",
            "Loss:  0.03714164346456528\n",
            "Loss:  0.03710655868053436\n",
            "Loss:  0.037071455270051956\n",
            "Loss:  0.03703644126653671\n",
            "Loss:  0.03700144961476326\n",
            "Loss:  0.0369664803147316\n",
            "Loss:  0.03693155199289322\n",
            "Loss:  0.03689667582511902\n",
            "Loss:  0.036861810833215714\n",
            "Loss:  0.03682699427008629\n",
            "Loss:  0.036792218685150146\n",
            "Loss:  0.036757465451955795\n",
            "Loss:  0.03672276437282562\n",
            "Loss:  0.03668811172246933\n",
            "Loss:  0.03665344417095184\n",
            "Loss:  0.036618854850530624\n",
            "Loss:  0.036584269255399704\n",
            "Loss:  0.03654973581433296\n",
            "Loss:  0.036515213549137115\n",
            "Loss:  0.03648076206445694\n",
            "Loss:  0.03644632175564766\n",
            "Loss:  0.03641194850206375\n",
            "Loss:  0.03637756034731865\n",
            "Loss:  0.03634324297308922\n",
            "Loss:  0.03630897030234337\n",
            "Loss:  0.036274705082178116\n",
            "Loss:  0.036240484565496445\n",
            "Loss:  0.03620630130171776\n",
            "Loss:  0.03617214784026146\n",
            "Loss:  0.036138035356998444\n",
            "Loss:  0.03610394522547722\n",
            "Loss:  0.036069903522729874\n",
            "Loss:  0.03603587672114372\n",
            "Loss:  0.03600193187594414\n",
            "Loss:  0.03596798703074455\n",
            "Loss:  0.035934045910835266\n",
            "Loss:  0.035900186747312546\n",
            "Loss:  0.035866353660821915\n",
            "Loss:  0.03583253175020218\n",
            "Loss:  0.03579873591661453\n",
            "Loss:  0.03576502948999405\n",
            "Loss:  0.03573131933808327\n",
            "Loss:  0.03569765388965607\n",
            "Loss:  0.03566401079297066\n",
            "Loss:  0.03563043102622032\n",
            "Loss:  0.03559685871005058\n",
            "Loss:  0.03556331247091293\n",
            "Loss:  0.035529814660549164\n",
            "Loss:  0.03549635037779808\n",
            "Loss:  0.03546294569969177\n",
            "Loss:  0.03542950749397278\n",
            "Loss:  0.035396166145801544\n",
            "Loss:  0.03536283224821091\n",
            "Loss:  0.03532952815294266\n",
            "Loss:  0.035296276211738586\n",
            "Loss:  0.03526303172111511\n",
            "Loss:  0.03522983193397522\n",
            "Loss:  0.035196684300899506\n",
            "Loss:  0.03516357019543648\n",
            "Loss:  0.035130467265844345\n",
            "Loss:  0.035097405314445496\n",
            "Loss:  0.03506440296769142\n",
            "Loss:  0.03503137454390526\n",
            "Loss:  0.03499843180179596\n",
            "Loss:  0.034965503960847855\n",
            "Loss:  0.034932613372802734\n",
            "Loss:  0.0348997563123703\n",
            "Loss:  0.03486693277955055\n",
            "Loss:  0.0348341166973114\n",
            "Loss:  0.034801382571458817\n",
            "Loss:  0.03476865217089653\n",
            "Loss:  0.03473595529794693\n",
            "Loss:  0.034703291952610016\n",
            "Loss:  0.03467068076133728\n",
            "Loss:  0.03463810309767723\n",
            "Loss:  0.034605517983436584\n",
            "Loss:  0.034572992473840714\n",
            "Loss:  0.03454049676656723\n",
            "Loss:  0.03450804203748703\n",
            "Loss:  0.03447561338543892\n",
            "Loss:  0.034443199634552\n",
            "Loss:  0.03441084548830986\n",
            "Loss:  0.03437851369380951\n",
            "Loss:  0.03434623405337334\n",
            "Loss:  0.03431396186351776\n",
            "Loss:  0.03428170830011368\n",
            "Loss:  0.03424951061606407\n",
            "Loss:  0.034217361360788345\n",
            "Loss:  0.03418520465493202\n",
            "Loss:  0.03415312618017197\n",
            "Loss:  0.034121058881282806\n",
            "Loss:  0.03408900648355484\n",
            "Loss:  0.03405700996518135\n",
            "Loss:  0.03402504324913025\n",
            "Loss:  0.033993080258369446\n",
            "Loss:  0.03396119177341461\n",
            "Loss:  0.033929288387298584\n",
            "Loss:  0.03389744833111763\n",
            "Loss:  0.033865634351968765\n",
            "Loss:  0.033833883702754974\n",
            "Loss:  0.033802129328250885\n",
            "Loss:  0.03377040475606918\n",
            "Loss:  0.033738743513822556\n",
            "Loss:  0.03370709344744682\n",
            "Loss:  0.03367546945810318\n",
            "Loss:  0.033643897622823715\n",
            "Loss:  0.03361232206225395\n",
            "Loss:  0.033580806106328964\n",
            "Loss:  0.03354932367801666\n",
            "Loss:  0.033517852425575256\n",
            "Loss:  0.033486440777778625\n",
            "Loss:  0.033455029129981995\n",
            "Loss:  0.03342367708683014\n",
            "Loss:  0.03339233994483948\n",
            "Loss:  0.033361054956912994\n",
            "Loss:  0.03332977369427681\n",
            "Loss:  0.0332985445857048\n",
            "Loss:  0.03326733410358429\n",
            "Loss:  0.03323616832494736\n",
            "Loss:  0.033205028623342514\n",
            "Loss:  0.03317392244935036\n",
            "Loss:  0.03314284235239029\n",
            "Loss:  0.03311179205775261\n",
            "Loss:  0.03308076784014702\n",
            "Loss:  0.03304979205131531\n",
            "Loss:  0.033018823713064194\n",
            "Loss:  0.032987918704748154\n",
            "Loss:  0.03295702114701271\n",
            "Loss:  0.032926153391599655\n",
            "Loss:  0.03289531171321869\n",
            "Loss:  0.03286454454064369\n",
            "Loss:  0.03283379226922989\n",
            "Loss:  0.03280302509665489\n",
            "Loss:  0.03277232125401497\n",
            "Loss:  0.032741647213697433\n",
            "Loss:  0.03271103650331497\n",
            "Loss:  0.03268039599061012\n",
            "Loss:  0.032649822533130646\n",
            "Loss:  0.03261927515268326\n",
            "Loss:  0.032588742673397064\n",
            "Loss:  0.03255827724933624\n",
            "Loss:  0.03252781555056572\n",
            "Loss:  0.03249737247824669\n",
            "Loss:  0.032467007637023926\n",
            "Loss:  0.03243662416934967\n",
            "Loss:  0.0324062816798687\n",
            "Loss:  0.03237597644329071\n",
            "Loss:  0.03234570100903511\n",
            "Loss:  0.03231547027826309\n",
            "Loss:  0.03228524327278137\n",
            "Loss:  0.03225506842136383\n",
            "Loss:  0.03222492337226868\n",
            "Loss:  0.03219480440020561\n",
            "Loss:  0.03216470405459404\n",
            "Loss:  0.03213464841246605\n",
            "Loss:  0.032104622572660446\n",
            "Loss:  0.032074619084596634\n",
            "Loss:  0.03204464167356491\n",
            "Loss:  0.03201472386717796\n",
            "Loss:  0.03198478743433952\n",
            "Loss:  0.031954921782016754\n",
            "Loss:  0.031925078481435776\n",
            "Loss:  0.03189527243375778\n",
            "Loss:  0.03186545893549919\n",
            "Loss:  0.031835686415433884\n",
            "Loss:  0.031805962324142456\n",
            "Loss:  0.03177628293633461\n",
            "Loss:  0.03174660727381706\n",
            "Loss:  0.0317169725894928\n",
            "Loss:  0.031687334179878235\n",
            "Loss:  0.03165779262781143\n",
            "Loss:  0.03162822499871254\n",
            "Loss:  0.03159874305129051\n",
            "Loss:  0.0315692201256752\n",
            "Loss:  0.03153976425528526\n",
            "Loss:  0.031510356813669205\n",
            "Loss:  0.03148093819618225\n",
            "Loss:  0.031451549381017685\n",
            "Loss:  0.031422194093465805\n",
            "Loss:  0.031392909586429596\n",
            "Loss:  0.03136362507939339\n",
            "Loss:  0.031334370374679565\n",
            "Loss:  0.03130512684583664\n",
            "Loss:  0.031275950372219086\n",
            "Loss:  0.03124677576124668\n",
            "Loss:  0.031217636540532112\n",
            "Loss:  0.031188547611236572\n",
            "Loss:  0.031159458681941032\n",
            "Loss:  0.031130405142903328\n",
            "Loss:  0.03110138699412346\n",
            "Loss:  0.03107241913676262\n",
            "Loss:  0.031043458729982376\n",
            "Loss:  0.03101450949907303\n",
            "Loss:  0.030985619872808456\n",
            "Loss:  0.030956752598285675\n",
            "Loss:  0.030927913263440132\n",
            "Loss:  0.030899085104465485\n",
            "Loss:  0.030870290473103523\n",
            "Loss:  0.030841564759612083\n",
            "Loss:  0.030812831595540047\n",
            "Loss:  0.03078410215675831\n",
            "Loss:  0.030755462124943733\n",
            "Loss:  0.030726805329322815\n",
            "Loss:  0.030698204413056374\n",
            "Loss:  0.03066960722208023\n",
            "Loss:  0.030641060322523117\n",
            "Loss:  0.03061249479651451\n",
            "Loss:  0.030584026128053665\n",
            "Loss:  0.030555568635463715\n",
            "Loss:  0.030527113005518913\n",
            "Loss:  0.03049871139228344\n",
            "Loss:  0.030470311641693115\n",
            "Loss:  0.03044196218252182\n",
            "Loss:  0.030413644388318062\n",
            "Loss:  0.030385339632630348\n",
            "Loss:  0.03035707399249077\n",
            "Loss:  0.030328838154673576\n",
            "Loss:  0.03030060976743698\n",
            "Loss:  0.03027244657278061\n",
            "Loss:  0.030244281515479088\n",
            "Loss:  0.03021613322198391\n",
            "Loss:  0.030188029631972313\n",
            "Loss:  0.030159972608089447\n",
            "Loss:  0.030131936073303223\n",
            "Loss:  0.030103929340839386\n",
            "Loss:  0.030075933784246445\n",
            "Loss:  0.030047988519072533\n",
            "Loss:  0.030020035803318024\n",
            "Loss:  0.02999214455485344\n",
            "Loss:  0.029964257031679153\n",
            "Loss:  0.02993644028902054\n",
            "Loss:  0.02990860491991043\n",
            "Loss:  0.029880814254283905\n",
            "Loss:  0.029853027313947678\n",
            "Loss:  0.029825322329998016\n",
            "Loss:  0.02979762852191925\n",
            "Loss:  0.029769929125905037\n",
            "Loss:  0.02974228374660015\n",
            "Loss:  0.02971465140581131\n",
            "Loss:  0.029687058180570602\n",
            "Loss:  0.029659507796168327\n",
            "Loss:  0.029631946235895157\n",
            "Loss:  0.029604457318782806\n",
            "Loss:  0.02957693487405777\n",
            "Loss:  0.029549501836299896\n",
            "Loss:  0.02952207624912262\n",
            "Loss:  0.02949465997517109\n",
            "Loss:  0.02946726232767105\n",
            "Loss:  0.02943994104862213\n",
            "Loss:  0.029412612318992615\n",
            "Loss:  0.02938532829284668\n",
            "Loss:  0.029358047991991043\n",
            "Loss:  0.029330825433135033\n",
            "Loss:  0.029303601011633873\n",
            "Loss:  0.0292764063924551\n",
            "Loss:  0.029249276965856552\n",
            "Loss:  0.029222141951322556\n",
            "Loss:  0.029195036739110947\n",
            "Loss:  0.029167961329221725\n",
            "Loss:  0.029140915721654892\n",
            "Loss:  0.029113899916410446\n",
            "Loss:  0.029086889699101448\n",
            "Loss:  0.029059939086437225\n",
            "Loss:  0.02903301641345024\n",
            "Loss:  0.02900608628988266\n",
            "Loss:  0.028979191556572914\n",
            "Loss:  0.02895234525203705\n",
            "Loss:  0.028925511986017227\n",
            "Loss:  0.028898704797029495\n",
            "Loss:  0.028871910646557808\n",
            "Loss:  0.02884518913924694\n",
            "Loss:  0.028818445280194283\n",
            "Loss:  0.0287917573004961\n",
            "Loss:  0.028765076771378517\n",
            "Loss:  0.02873843163251877\n",
            "Loss:  0.028711829334497452\n",
            "Loss:  0.028685245662927628\n",
            "Loss:  0.0286586694419384\n",
            "Loss:  0.02863212861120701\n",
            "Loss:  0.0286056250333786\n",
            "Loss:  0.028579123318195343\n",
            "Loss:  0.028552668169140816\n",
            "Loss:  0.028526240959763527\n",
            "Loss:  0.02849981002509594\n",
            "Loss:  0.028473442420363426\n",
            "Loss:  0.0284471083432436\n",
            "Loss:  0.02842077799141407\n",
            "Loss:  0.028394484892487526\n",
            "Loss:  0.02836821973323822\n",
            "Loss:  0.02834194339811802\n",
            "Loss:  0.02831573784351349\n",
            "Loss:  0.028289547190070152\n",
            "Loss:  0.02826337330043316\n",
            "Loss:  0.028237247839570045\n",
            "Loss:  0.028211122378706932\n",
            "Loss:  0.0281850416213274\n",
            "Loss:  0.028158972039818764\n",
            "Loss:  0.028132930397987366\n",
            "Loss:  0.028106942772865295\n",
            "Loss:  0.028080938383936882\n",
            "Loss:  0.02805497497320175\n",
            "Loss:  0.028029046952724457\n",
            "Loss:  0.028003113344311714\n",
            "Loss:  0.027977237477898598\n",
            "Loss:  0.027951369062066078\n",
            "Loss:  0.02792556770145893\n",
            "Loss:  0.02789974771440029\n",
            "Loss:  0.02787395939230919\n",
            "Loss:  0.027848215773701668\n",
            "Loss:  0.02782248519361019\n",
            "Loss:  0.027796786278486252\n",
            "Loss:  0.027771124616265297\n",
            "Loss:  0.027745461091399193\n",
            "Loss:  0.027719823643565178\n",
            "Loss:  0.02769424021244049\n",
            "Loss:  0.027668656781315804\n",
            "Loss:  0.027643119916319847\n",
            "Loss:  0.02761758305132389\n",
            "Loss:  0.027592089027166367\n",
            "Loss:  0.027566611766815186\n",
            "Loss:  0.02754116803407669\n",
            "Loss:  0.02751574106514454\n",
            "Loss:  0.02749035879969597\n",
            "Loss:  0.02746497467160225\n",
            "Loss:  0.027439642697572708\n",
            "Loss:  0.02741432376205921\n",
            "Loss:  0.027389023452997208\n",
            "Loss:  0.027363743633031845\n",
            "Loss:  0.027338510379195213\n",
            "Loss:  0.027313299477100372\n",
            "Loss:  0.027288110926747322\n",
            "Loss:  0.027262920513749123\n",
            "Loss:  0.0272377897053957\n",
            "Loss:  0.027212677523493767\n",
            "Loss:  0.02718760073184967\n",
            "Loss:  0.027162499725818634\n",
            "Loss:  0.02713746950030327\n",
            "Loss:  0.027112441137433052\n",
            "Loss:  0.027087459340691566\n",
            "Loss:  0.027062494307756424\n",
            "Loss:  0.02703755535185337\n",
            "Loss:  0.027012638747692108\n",
            "Loss:  0.026987729594111443\n",
            "Loss:  0.026962868869304657\n",
            "Loss:  0.026938021183013916\n",
            "Loss:  0.02691320888698101\n",
            "Loss:  0.026888420805335045\n",
            "Loss:  0.026863645762205124\n",
            "Loss:  0.026838887482881546\n",
            "Loss:  0.02681417018175125\n",
            "Loss:  0.02678946778178215\n",
            "Loss:  0.026764795184135437\n",
            "Loss:  0.02674015611410141\n",
            "Loss:  0.026715541258454323\n",
            "Loss:  0.026690945029258728\n",
            "Loss:  0.02666633576154709\n",
            "Loss:  0.02664179727435112\n",
            "Loss:  0.026617279276251793\n",
            "Loss:  0.026592785492539406\n",
            "Loss:  0.026568308472633362\n",
            "Loss:  0.026543857529759407\n",
            "Loss:  0.026519425213336945\n",
            "Loss:  0.02649502083659172\n",
            "Loss:  0.026470646262168884\n",
            "Loss:  0.026446277275681496\n",
            "Loss:  0.026421956717967987\n",
            "Loss:  0.02639763616025448\n",
            "Loss:  0.026373371481895447\n",
            "Loss:  0.02634909562766552\n",
            "Loss:  0.026324886828660965\n",
            "Loss:  0.026300659403204918\n",
            "Loss:  0.026276463642716408\n",
            "Loss:  0.02625231258571148\n",
            "Loss:  0.026228198781609535\n",
            "Loss:  0.026204075664281845\n",
            "Loss:  0.026179976761341095\n",
            "Loss:  0.026155924424529076\n",
            "Loss:  0.02613188326358795\n",
            "Loss:  0.02610786259174347\n",
            "Loss:  0.026083875447511673\n",
            "Loss:  0.02605990134179592\n",
            "Loss:  0.02603597566485405\n",
            "Loss:  0.02601204812526703\n",
            "Loss:  0.025988135486841202\n",
            "Loss:  0.02596425451338291\n",
            "Loss:  0.025940416380763054\n",
            "Loss:  0.025916581973433495\n",
            "Loss:  0.02589278854429722\n",
            "Loss:  0.025869010016322136\n",
            "Loss:  0.025845270603895187\n",
            "Loss:  0.02582150511443615\n",
            "Loss:  0.025797812268137932\n",
            "Loss:  0.02577413059771061\n",
            "Loss:  0.025750448927283287\n",
            "Loss:  0.025726821273565292\n",
            "Loss:  0.025703201070427895\n",
            "Loss:  0.02567962557077408\n",
            "Loss:  0.025656038895249367\n",
            "Loss:  0.02563251554965973\n",
            "Loss:  0.02560899220407009\n",
            "Loss:  0.025585494935512543\n",
            "Loss:  0.025562020018696785\n",
            "Loss:  0.025538569316267967\n",
            "Loss:  0.025515137240290642\n",
            "Loss:  0.02549174055457115\n",
            "Loss:  0.02546834386885166\n",
            "Loss:  0.025445008650422096\n",
            "Loss:  0.025421656668186188\n",
            "Loss:  0.025398340076208115\n",
            "Loss:  0.025375066325068474\n",
            "Loss:  0.025351788848638535\n",
            "Loss:  0.02532854862511158\n",
            "Loss:  0.025305332615971565\n",
            "Loss:  0.02528214268386364\n",
            "Loss:  0.025258975103497505\n",
            "Loss:  0.025235822424292564\n",
            "Loss:  0.025212673470377922\n",
            "Loss:  0.025189584121108055\n",
            "Loss:  0.025166518986225128\n",
            "Loss:  0.025143466889858246\n",
            "Loss:  0.02512042224407196\n",
            "Loss:  0.025097422301769257\n",
            "Loss:  0.025074420496821404\n",
            "Loss:  0.025051455944776535\n",
            "Loss:  0.02502851001918316\n",
            "Loss:  0.025005586445331573\n",
            "Loss:  0.02498268522322178\n",
            "Loss:  0.02495981939136982\n",
            "Loss:  0.024936946108937263\n",
            "Loss:  0.024914108216762543\n",
            "Loss:  0.024891316890716553\n",
            "Loss:  0.024868542328476906\n",
            "Loss:  0.0248457882553339\n",
            "Loss:  0.02482304722070694\n",
            "Loss:  0.024800309911370277\n",
            "Loss:  0.024777628481388092\n",
            "Loss:  0.024754958227276802\n",
            "Loss:  0.024732299149036407\n",
            "Loss:  0.02470966801047325\n",
            "Loss:  0.024687083438038826\n",
            "Loss:  0.024664467200636864\n",
            "Loss:  0.024641936644911766\n",
            "Loss:  0.024619383737444878\n",
            "Loss:  0.024596862494945526\n",
            "Loss:  0.024574408307671547\n",
            "Loss:  0.024551905691623688\n",
            "Loss:  0.02452947199344635\n",
            "Loss:  0.024507060647010803\n",
            "Loss:  0.024484632536768913\n",
            "Loss:  0.024462256580591202\n",
            "Loss:  0.02443987876176834\n",
            "Loss:  0.024417536333203316\n",
            "Loss:  0.024395234882831573\n",
            "Loss:  0.024372944608330727\n",
            "Loss:  0.02435067854821682\n",
            "Loss:  0.024328436702489853\n",
            "Loss:  0.024306192994117737\n",
            "Loss:  0.024283990263938904\n",
            "Loss:  0.024261822924017906\n",
            "Loss:  0.024239692836999893\n",
            "Loss:  0.0242175105959177\n",
            "Loss:  0.02419540472328663\n",
            "Loss:  0.024173308163881302\n",
            "Loss:  0.024151241406798363\n",
            "Loss:  0.024129152297973633\n",
            "Loss:  0.02410714700818062\n",
            "Loss:  0.024085141718387604\n",
            "Loss:  0.024063153192400932\n",
            "Loss:  0.02404119446873665\n",
            "Loss:  0.024019256234169006\n",
            "Loss:  0.023997344076633453\n",
            "Loss:  0.023975415155291557\n",
            "Loss:  0.023953575640916824\n",
            "Loss:  0.023931697010993958\n",
            "Loss:  0.02390986680984497\n",
            "Loss:  0.023888040333986282\n",
            "Loss:  0.023866264149546623\n",
            "Loss:  0.023844469338655472\n",
            "Loss:  0.02382274717092514\n",
            "Loss:  0.023801017552614212\n",
            "Loss:  0.023779291659593582\n",
            "Loss:  0.023757610470056534\n",
            "Loss:  0.023735947906970978\n",
            "Loss:  0.02371431142091751\n",
            "Loss:  0.02369268238544464\n",
            "Loss:  0.02367110550403595\n",
            "Loss:  0.023649513721466064\n",
            "Loss:  0.02362797036767006\n",
            "Loss:  0.023606427013874054\n",
            "Loss:  0.02358490228652954\n",
            "Loss:  0.02356341853737831\n",
            "Loss:  0.02354196086525917\n",
            "Loss:  0.02352050691843033\n",
            "Loss:  0.023499060422182083\n",
            "Loss:  0.023477671667933464\n",
            "Loss:  0.023456284776329994\n",
            "Loss:  0.02343490906059742\n",
            "Loss:  0.023413576185703278\n",
            "Loss:  0.02339225448668003\n",
            "Loss:  0.023370932787656784\n",
            "Loss:  0.023349693045020103\n",
            "Loss:  0.023328406736254692\n",
            "Loss:  0.023307180032134056\n",
            "Loss:  0.02328595146536827\n",
            "Loss:  0.02326476387679577\n",
            "Loss:  0.023243583738803864\n",
            "Loss:  0.023222418501973152\n",
            "Loss:  0.02320128120481968\n",
            "Loss:  0.02318018116056919\n",
            "Loss:  0.0231590885668993\n",
            "Loss:  0.02313802018761635\n",
            "Loss:  0.023116955533623695\n",
            "Loss:  0.02309594117105007\n",
            "Loss:  0.023074930533766747\n",
            "Loss:  0.023053953424096107\n",
            "Loss:  0.02303297072649002\n",
            "Loss:  0.023012040182948112\n",
            "Loss:  0.022991109639406204\n",
            "Loss:  0.02297021821141243\n",
            "Loss:  0.022949304431676865\n",
            "Loss:  0.02292846515774727\n",
            "Loss:  0.022907622158527374\n",
            "Loss:  0.02288678102195263\n",
            "Loss:  0.02286599949002266\n",
            "Loss:  0.02284521423280239\n",
            "Loss:  0.022824430838227272\n",
            "Loss:  0.02280368283390999\n",
            "Loss:  0.022782975807785988\n",
            "Loss:  0.022762270644307137\n",
            "Loss:  0.02274160087108612\n",
            "Loss:  0.022720927372574806\n",
            "Loss:  0.022700292989611626\n",
            "Loss:  0.022679686546325684\n",
            "Loss:  0.022659068927168846\n",
            "Loss:  0.022638488560914993\n",
            "Loss:  0.022617941722273827\n",
            "Loss:  0.02259739488363266\n",
            "Loss:  0.022576870396733284\n",
            "Loss:  0.022556398063898087\n",
            "Loss:  0.022535912692546844\n",
            "Loss:  0.022515451535582542\n",
            "Loss:  0.022495007142424583\n",
            "Loss:  0.022474611178040504\n",
            "Loss:  0.022454185411334038\n",
            "Loss:  0.022433822974562645\n",
            "Loss:  0.022413460537791252\n",
            "Loss:  0.022393129765987396\n",
            "Loss:  0.022372832521796227\n",
            "Loss:  0.02235252410173416\n",
            "Loss:  0.022332239896059036\n",
            "Loss:  0.022311991080641747\n",
            "Loss:  0.02229176089167595\n",
            "Loss:  0.02227151207625866\n",
            "Loss:  0.022251326590776443\n",
            "Loss:  0.022231169044971466\n",
            "Loss:  0.02221100404858589\n",
            "Loss:  0.022190887480974197\n",
            "Loss:  0.022170770913362503\n",
            "Loss:  0.022150658071041107\n",
            "Loss:  0.0221305750310421\n",
            "Loss:  0.02211051620543003\n",
            "Loss:  0.022090472280979156\n",
            "Loss:  0.022070450708270073\n",
            "Loss:  0.022050464525818825\n",
            "Loss:  0.022030483931303024\n",
            "Loss:  0.022010520100593567\n",
            "Loss:  0.021990599110722542\n",
            "Loss:  0.021970674395561218\n",
            "Loss:  0.02195075899362564\n",
            "Loss:  0.021930880844593048\n",
            "Loss:  0.021911030635237694\n",
            "Loss:  0.021891187876462936\n",
            "Loss:  0.02187134511768818\n",
            "Loss:  0.021851573139429092\n",
            "Loss:  0.021831775084137917\n",
            "Loss:  0.02181200683116913\n",
            "Loss:  0.02179226465523243\n",
            "Loss:  0.021772539243102074\n",
            "Loss:  0.021752823144197464\n",
            "Loss:  0.02173314243555069\n",
            "Loss:  0.021713487803936005\n",
            "Loss:  0.02169383317232132\n",
            "Loss:  0.02167421393096447\n",
            "Loss:  0.021654589101672173\n",
            "Loss:  0.02163500152528286\n",
            "Loss:  0.02161543071269989\n",
            "Loss:  0.021595872938632965\n",
            "Loss:  0.021576344966888428\n",
            "Loss:  0.02155684307217598\n",
            "Loss:  0.021537339314818382\n",
            "Loss:  0.021517887711524963\n",
            "Loss:  0.0214984230697155\n",
            "Loss:  0.021478988230228424\n",
            "Loss:  0.021459557116031647\n",
            "Loss:  0.0214401762932539\n",
            "Loss:  0.02142079547047615\n",
            "Loss:  0.02140141651034355\n",
            "Loss:  0.02138207107782364\n",
            "Loss:  0.02136276289820671\n",
            "Loss:  0.02134346030652523\n",
            "Loss:  0.02132418006658554\n",
            "Loss:  0.021304894238710403\n",
            "Loss:  0.02128566801548004\n",
            "Loss:  0.02126643806695938\n",
            "Loss:  0.02124723047018051\n",
            "Loss:  0.021228037774562836\n",
            "Loss:  0.021208859980106354\n",
            "Loss:  0.021189717575907707\n",
            "Loss:  0.021170563995838165\n",
            "Loss:  0.021151477470993996\n",
            "Loss:  0.02113238163292408\n",
            "Loss:  0.02111329883337021\n",
            "Loss:  0.02109423838555813\n",
            "Loss:  0.021075187250971794\n",
            "Loss:  0.0210561640560627\n",
            "Loss:  0.021037189289927483\n",
            "Loss:  0.021018195897340775\n",
            "Loss:  0.020999234169721603\n",
            "Loss:  0.02098027616739273\n",
            "Loss:  0.02096136100590229\n",
            "Loss:  0.0209424439817667\n",
            "Loss:  0.020923560485243797\n",
            "Loss:  0.020904695615172386\n",
            "Loss:  0.020885823294520378\n",
            "Loss:  0.020867004990577698\n",
            "Loss:  0.020848190411925316\n",
            "Loss:  0.02082938700914383\n",
            "Loss:  0.020810594782233238\n",
            "Loss:  0.02079182304441929\n",
            "Loss:  0.02077309414744377\n",
            "Loss:  0.020754359662532806\n",
            "Loss:  0.020735671743750572\n",
            "Loss:  0.02071698196232319\n",
            "Loss:  0.020698320120573044\n",
            "Loss:  0.0206796545535326\n",
            "Loss:  0.02066100761294365\n",
            "Loss:  0.020642424002289772\n",
            "Loss:  0.020623812451958656\n",
            "Loss:  0.020605241879820824\n",
            "Loss:  0.020586691796779633\n",
            "Loss:  0.0205681174993515\n",
            "Loss:  0.02054959535598755\n",
            "Loss:  0.02053109183907509\n",
            "Loss:  0.020512592047452927\n",
            "Loss:  0.020494136959314346\n",
            "Loss:  0.020475687459111214\n",
            "Loss:  0.020457252860069275\n",
            "Loss:  0.020438840612769127\n",
            "Loss:  0.02042044699192047\n",
            "Loss:  0.020402057096362114\n",
            "Loss:  0.02038370445370674\n",
            "Loss:  0.020365355536341667\n",
            "Loss:  0.02034704014658928\n",
            "Loss:  0.02032872661948204\n",
            "Loss:  0.020310435444116592\n",
            "Loss:  0.020292161032557487\n",
            "Loss:  0.020273922011256218\n",
            "Loss:  0.02025565691292286\n",
            "Loss:  0.020237471908330917\n",
            "Loss:  0.020219257101416588\n",
            "Loss:  0.020201070234179497\n",
            "Loss:  0.020182909443974495\n",
            "Loss:  0.02016478031873703\n",
            "Loss:  0.020146630704402924\n",
            "Loss:  0.020128542557358742\n",
            "Loss:  0.02011040598154068\n",
            "Loss:  0.02009233459830284\n",
            "Loss:  0.020074298605322838\n",
            "Loss:  0.02005624771118164\n",
            "Loss:  0.02003823220729828\n",
            "Loss:  0.02002020925283432\n",
            "Loss:  0.020002221688628197\n",
            "Loss:  0.019984250888228416\n",
            "Loss:  0.019966309890151024\n",
            "Loss:  0.019948357716202736\n",
            "Loss:  0.019930459558963776\n",
            "Loss:  0.019912563264369965\n",
            "Loss:  0.01989467814564705\n",
            "Loss:  0.01987677998840809\n",
            "Loss:  0.0198589488863945\n",
            "Loss:  0.019841132685542107\n",
            "Loss:  0.01982332393527031\n",
            "Loss:  0.019805509597063065\n",
            "Loss:  0.0197877436876297\n",
            "Loss:  0.019769983366131783\n",
            "Loss:  0.01975223422050476\n",
            "Loss:  0.019734486937522888\n",
            "Loss:  0.019716788083314896\n",
            "Loss:  0.019699130207300186\n",
            "Loss:  0.0196814127266407\n",
            "Loss:  0.01966375671327114\n",
            "Loss:  0.019646134227514267\n",
            "Loss:  0.019628513604402542\n",
            "Loss:  0.019610900431871414\n",
            "Loss:  0.01959330402314663\n",
            "Loss:  0.01957573927938938\n",
            "Loss:  0.019558191299438477\n",
            "Loss:  0.01954064704477787\n",
            "Loss:  0.01952313631772995\n",
            "Loss:  0.019505629315972328\n",
            "Loss:  0.019488144665956497\n",
            "Loss:  0.01947067305445671\n",
            "Loss:  0.019453221932053566\n",
            "Loss:  0.019435781985521317\n",
            "Loss:  0.0194183811545372\n",
            "Loss:  0.01940097101032734\n",
            "Loss:  0.01938357762992382\n",
            "Loss:  0.019366221502423286\n",
            "Loss:  0.01934889517724514\n",
            "Loss:  0.01933152973651886\n",
            "Loss:  0.01931421272456646\n",
            "Loss:  0.01929692178964615\n",
            "Loss:  0.019279643893241882\n",
            "Loss:  0.01926237717270851\n",
            "Loss:  0.01924513466656208\n",
            "Loss:  0.019227908924221992\n",
            "Loss:  0.019210679456591606\n",
            "Loss:  0.01919347047805786\n",
            "Loss:  0.019176308065652847\n",
            "Loss:  0.019159140065312386\n",
            "Loss:  0.01914198137819767\n",
            "Loss:  0.019124845042824745\n",
            "Loss:  0.019107749685645103\n",
            "Loss:  0.019090643152594566\n",
            "Loss:  0.019073577597737312\n",
            "Loss:  0.019056499004364014\n",
            "Loss:  0.019039461389183998\n",
            "Loss:  0.01902243122458458\n",
            "Loss:  0.019005408510565758\n",
            "Loss:  0.018988419324159622\n",
            "Loss:  0.018971435725688934\n",
            "Loss:  0.018954463303089142\n",
            "Loss:  0.01893751509487629\n",
            "Loss:  0.018920592963695526\n",
            "Loss:  0.01890367642045021\n",
            "Loss:  0.018886754289269447\n",
            "Loss:  0.018869904801249504\n",
            "Loss:  0.01885301060974598\n",
            "Loss:  0.01883617416024208\n",
            "Loss:  0.018819348886609077\n",
            "Loss:  0.01880253292620182\n",
            "Loss:  0.018785715103149414\n",
            "Loss:  0.018768951296806335\n",
            "Loss:  0.01875215955078602\n",
            "Loss:  0.018735423684120178\n",
            "Loss:  0.018718678504228592\n",
            "Loss:  0.01870197430253029\n",
            "Loss:  0.018685266375541687\n",
            "Loss:  0.018668590113520622\n",
            "Loss:  0.01865190640091896\n",
            "Loss:  0.01863526552915573\n",
            "Loss:  0.018618615344166756\n",
            "Loss:  0.018601998686790466\n",
            "Loss:  0.01858540065586567\n",
            "Loss:  0.018568798899650574\n",
            "Loss:  0.018552232533693314\n",
            "Loss:  0.0185356717556715\n",
            "Loss:  0.01851913332939148\n",
            "Loss:  0.01850261725485325\n",
            "Loss:  0.01848609186708927\n",
            "Loss:  0.018469596281647682\n",
            "Loss:  0.01845312863588333\n",
            "Loss:  0.018436679616570473\n",
            "Loss:  0.018420230597257614\n",
            "Loss:  0.018403800204396248\n",
            "Loss:  0.018387384712696075\n",
            "Loss:  0.018370982259511948\n",
            "Loss:  0.01835460029542446\n",
            "Loss:  0.01833822764456272\n",
            "Loss:  0.018321899697184563\n",
            "Loss:  0.01830555684864521\n",
            "Loss:  0.01828925684094429\n",
            "Loss:  0.018272949382662773\n",
            "Loss:  0.01825665310025215\n",
            "Loss:  0.018240390345454216\n",
            "Loss:  0.018224133178591728\n",
            "Loss:  0.01820790022611618\n",
            "Loss:  0.018191678449511528\n",
            "Loss:  0.018175484612584114\n",
            "Loss:  0.018159277737140656\n",
            "Loss:  0.01814310997724533\n",
            "Loss:  0.018126949667930603\n",
            "Loss:  0.01811080425977707\n",
            "Loss:  0.01809467375278473\n",
            "Loss:  0.018078556284308434\n",
            "Loss:  0.018062453716993332\n",
            "Loss:  0.018046393990516663\n",
            "Loss:  0.018030323088169098\n",
            "Loss:  0.018014268949627876\n",
            "Loss:  0.01799822971224785\n",
            "Loss:  0.017982233315706253\n",
            "Loss:  0.01796620339155197\n",
            "Loss:  0.01795022375881672\n",
            "Loss:  0.01793426275253296\n",
            "Loss:  0.01791829615831375\n",
            "Loss:  0.017902357503771782\n",
            "Loss:  0.017886444926261902\n",
            "Loss:  0.01787053421139717\n",
            "Loss:  0.01785462535917759\n",
            "Loss:  0.017838748171925545\n",
            "Loss:  0.017822906374931335\n",
            "Loss:  0.017807042226195335\n",
            "Loss:  0.01779121905565262\n",
            "Loss:  0.017775384709239006\n",
            "Loss:  0.01775958761572838\n",
            "Loss:  0.017743797972798347\n",
            "Loss:  0.017728038132190704\n",
            "Loss:  0.017712289467453957\n",
            "Loss:  0.01769653707742691\n",
            "Loss:  0.017680808901786804\n",
            "Loss:  0.017665104940533638\n",
            "Loss:  0.017649393528699875\n",
            "Loss:  0.017633719369769096\n",
            "Loss:  0.017618056386709213\n",
            "Loss:  0.017602406442165375\n",
            "Loss:  0.01758679747581482\n",
            "Loss:  0.017571158707141876\n",
            "Loss:  0.01755554974079132\n",
            "Loss:  0.017539944499731064\n",
            "Loss:  0.017524389550089836\n",
            "Loss:  0.01750882714986801\n",
            "Loss:  0.017493275925517082\n",
            "Loss:  0.017477767542004585\n",
            "Loss:  0.01746225543320179\n",
            "Loss:  0.017446747049689293\n",
            "Loss:  0.017431266605854034\n",
            "Loss:  0.017415793612599373\n",
            "Loss:  0.017400342971086502\n",
            "Loss:  0.017384914681315422\n",
            "Loss:  0.017369473353028297\n",
            "Loss:  0.017354069277644157\n",
            "Loss:  0.017338687554001808\n",
            "Loss:  0.017323296517133713\n",
            "Loss:  0.01730794459581375\n",
            "Loss:  0.01729256473481655\n",
            "Loss:  0.017277255654335022\n",
            "Loss:  0.017261927947402\n",
            "Loss:  0.017246628180146217\n",
            "Loss:  0.017231324687600136\n",
            "Loss:  0.01721607707440853\n",
            "Loss:  0.017200792208313942\n",
            "Loss:  0.017185552045702934\n",
            "Loss:  0.017170321196317673\n",
            "Loss:  0.017155107110738754\n",
            "Loss:  0.01713990792632103\n",
            "Loss:  0.017124710604548454\n",
            "Loss:  0.01710956171154976\n",
            "Loss:  0.01709439419209957\n",
            "Loss:  0.01707925833761692\n",
            "Loss:  0.017064126208424568\n",
            "Loss:  0.017049025744199753\n",
            "Loss:  0.017033912241458893\n",
            "Loss:  0.01701882854104042\n",
            "Loss:  0.017003759741783142\n",
            "Loss:  0.016988713294267654\n",
            "Loss:  0.01697366125881672\n",
            "Loss:  0.016958635300397873\n",
            "Loss:  0.01694364845752716\n",
            "Loss:  0.01692863367497921\n",
            "Loss:  0.01691366359591484\n",
            "Loss:  0.016898687928915024\n",
            "Loss:  0.01688373275101185\n",
            "Loss:  0.01686878316104412\n",
            "Loss:  0.01685386709868908\n",
            "Loss:  0.016838952898979187\n",
            "Loss:  0.016824064776301384\n",
            "Loss:  0.016809191554784775\n",
            "Loss:  0.016794312745332718\n",
            "Loss:  0.016779474914073944\n",
            "Loss:  0.01676461659371853\n",
            "Loss:  0.0167497918009758\n",
            "Loss:  0.016734972596168518\n",
            "Loss:  0.016720173880457878\n",
            "Loss:  0.016705401241779327\n",
            "Loss:  0.016690636053681374\n",
            "Loss:  0.01667589135468006\n",
            "Loss:  0.01666114293038845\n",
            "Loss:  0.016646413132548332\n",
            "Loss:  0.016631728038191795\n",
            "Loss:  0.016617028042674065\n",
            "Loss:  0.016602344810962677\n",
            "Loss:  0.01658768206834793\n",
            "Loss:  0.016573013737797737\n",
            "Loss:  0.016558382660150528\n",
            "Loss:  0.016543762758374214\n",
            "Loss:  0.01652914099395275\n",
            "Loss:  0.016514554619789124\n",
            "Loss:  0.0164999607950449\n",
            "Loss:  0.016485394909977913\n",
            "Loss:  0.01647084578871727\n",
            "Loss:  0.01645631343126297\n",
            "Loss:  0.016441799700260162\n",
            "Loss:  0.016427278518676758\n",
            "Loss:  0.0164127666503191\n",
            "Loss:  0.016398301348090172\n",
            "Loss:  0.016383817419409752\n",
            "Loss:  0.016369357705116272\n",
            "Loss:  0.016354920342564583\n",
            "Loss:  0.016340481117367744\n",
            "Loss:  0.016326071694493294\n",
            "Loss:  0.016311677172780037\n",
            "Loss:  0.01629728637635708\n",
            "Loss:  0.016282910481095314\n",
            "Loss:  0.01626855507493019\n",
            "Loss:  0.01625422202050686\n",
            "Loss:  0.016239900141954422\n",
            "Loss:  0.016225561499595642\n",
            "Loss:  0.016211269423365593\n",
            "Loss:  0.016196981072425842\n",
            "Loss:  0.016182705760002136\n",
            "Loss:  0.01616843044757843\n",
            "Loss:  0.016154196113348007\n",
            "Loss:  0.016139959916472435\n",
            "Loss:  0.016125747933983803\n",
            "Loss:  0.016111530363559723\n",
            "Loss:  0.01609734073281288\n",
            "Loss:  0.01608317159116268\n",
            "Loss:  0.01606898568570614\n",
            "Loss:  0.01605483703315258\n",
            "Loss:  0.016040697693824768\n",
            "Loss:  0.016026565805077553\n",
            "Loss:  0.016012443229556084\n",
            "Loss:  0.015998346731066704\n",
            "Loss:  0.015984272584319115\n",
            "Loss:  0.015970205888152122\n",
            "Loss:  0.015956135466694832\n",
            "Loss:  0.015942107886075974\n",
            "Loss:  0.01592806912958622\n",
            "Loss:  0.01591407135128975\n",
            "Loss:  0.015900054946541786\n",
            "Loss:  0.015886060893535614\n",
            "Loss:  0.01587209478020668\n",
            "Loss:  0.015858126804232597\n",
            "Loss:  0.015844162553548813\n",
            "Loss:  0.01583024114370346\n",
            "Loss:  0.01581631600856781\n",
            "Loss:  0.0158024150878191\n",
            "Loss:  0.015788501128554344\n",
            "Loss:  0.01577463187277317\n",
            "Loss:  0.015760736539959908\n",
            "Loss:  0.01574689894914627\n",
            "Loss:  0.015733052045106888\n",
            "Loss:  0.015719210729002953\n",
            "Loss:  0.0157054141163826\n",
            "Loss:  0.015691597014665604\n",
            "Loss:  0.01567782461643219\n",
            "Loss:  0.01566404290497303\n",
            "Loss:  0.015650281682610512\n",
            "Loss:  0.01563653163611889\n",
            "Loss:  0.015622803941369057\n",
            "Loss:  0.015609079971909523\n",
            "Loss:  0.015595363453030586\n",
            "Loss:  0.015581664629280567\n",
            "Loss:  0.015567990951240063\n",
            "Loss:  0.01555431168526411\n",
            "Loss:  0.015540667809545994\n",
            "Loss:  0.015527001582086086\n",
            "Loss:  0.015513385646045208\n",
            "Loss:  0.015499761328101158\n",
            "Loss:  0.015486168675124645\n",
            "Loss:  0.015472570434212685\n",
            "Loss:  0.015458992682397366\n",
            "Loss:  0.015445408411324024\n",
            "Loss:  0.01543186791241169\n",
            "Loss:  0.015418336726725101\n",
            "Loss:  0.015404801815748215\n",
            "Loss:  0.015391278080642223\n",
            "Loss:  0.015377786941826344\n",
            "Loss:  0.015364285558462143\n",
            "Loss:  0.015350806526839733\n",
            "Loss:  0.015337356366217136\n",
            "Loss:  0.01532390434294939\n",
            "Loss:  0.01531046349555254\n",
            "Loss:  0.015297038480639458\n",
            "Loss:  0.015283619984984398\n",
            "Loss:  0.015270223841071129\n",
            "Loss:  0.015256847254931927\n",
            "Loss:  0.015243473462760448\n",
            "Loss:  0.01523011177778244\n",
            "Loss:  0.01521676778793335\n",
            "Loss:  0.015203442424535751\n",
            "Loss:  0.015190133824944496\n",
            "Loss:  0.015176820568740368\n",
            "Loss:  0.015163532458245754\n",
            "Loss:  0.015150253660976887\n",
            "Loss:  0.015136973932385445\n",
            "Loss:  0.015123706310987473\n",
            "Loss:  0.015110481530427933\n",
            "Loss:  0.015097251161932945\n",
            "Loss:  0.015084031969308853\n",
            "Loss:  0.01507082674652338\n",
            "Loss:  0.015057630836963654\n",
            "Loss:  0.015044459141790867\n",
            "Loss:  0.015031278133392334\n",
            "Loss:  0.015018131583929062\n",
            "Loss:  0.015004987828433514\n",
            "Loss:  0.01499185897409916\n",
            "Loss:  0.014978744089603424\n",
            "Loss:  0.014965648762881756\n",
            "Loss:  0.014952549710869789\n",
            "Loss:  0.014939474873244762\n",
            "Loss:  0.014926416799426079\n",
            "Loss:  0.014913362450897694\n",
            "Loss:  0.014900309965014458\n",
            "Loss:  0.014887284487485886\n",
            "Loss:  0.014874271117150784\n",
            "Loss:  0.01486127357929945\n",
            "Loss:  0.014848259277641773\n",
            "Loss:  0.014835293404757977\n",
            "Loss:  0.014822320081293583\n",
            "Loss:  0.01480936724692583\n",
            "Loss:  0.014796431176364422\n",
            "Loss:  0.014783498831093311\n",
            "Loss:  0.014770579524338245\n",
            "Loss:  0.014757689088582993\n",
            "Loss:  0.014744791202247143\n",
            "Loss:  0.014731908217072487\n",
            "Loss:  0.014719038270413876\n",
            "Loss:  0.014706173911690712\n",
            "Loss:  0.014693332836031914\n",
            "Loss:  0.014680495485663414\n",
            "Loss:  0.014667685143649578\n",
            "Loss:  0.014654883183538914\n",
            "Loss:  0.014642084017395973\n",
            "Loss:  0.014629308134317398\n",
            "Loss:  0.01461651548743248\n",
            "Loss:  0.014603786170482635\n",
            "Loss:  0.014591042883694172\n",
            "Loss:  0.014578304253518581\n",
            "Loss:  0.014565590769052505\n",
            "Loss:  0.014552868902683258\n",
            "Loss:  0.014540170319378376\n",
            "Loss:  0.014527492225170135\n",
            "Loss:  0.014514820650219917\n",
            "Loss:  0.014502160251140594\n",
            "Loss:  0.014489510096609592\n",
            "Loss:  0.014476878568530083\n",
            "Loss:  0.014464247040450573\n",
            "Loss:  0.01445163507014513\n",
            "Loss:  0.014439042657613754\n",
            "Loss:  0.014426462352275848\n",
            "Loss:  0.014413872733712196\n",
            "Loss:  0.0144013287499547\n",
            "Loss:  0.01438876148313284\n",
            "Loss:  0.01437622681260109\n",
            "Loss:  0.014363698661327362\n",
            "Loss:  0.014351193793118\n",
            "Loss:  0.014338688924908638\n",
            "Loss:  0.014326196163892746\n",
            "Loss:  0.014313716441392899\n",
            "Loss:  0.014301249757409096\n",
            "Loss:  0.014288804493844509\n",
            "Loss:  0.014276369474828243\n",
            "Loss:  0.014263923279941082\n",
            "Loss:  0.014251500368118286\n",
            "Loss:  0.01423910353332758\n",
            "Loss:  0.014226699247956276\n",
            "Loss:  0.014214303344488144\n",
            "Loss:  0.014201940037310123\n",
            "Loss:  0.014189583249390125\n",
            "Loss:  0.014177221804857254\n",
            "Loss:  0.014164887368679047\n",
            "Loss:  0.014152547344565392\n",
            "Loss:  0.014140227809548378\n",
            "Loss:  0.01412793155759573\n",
            "Loss:  0.014115631580352783\n",
            "Loss:  0.014103368856012821\n",
            "Loss:  0.01409110426902771\n",
            "Loss:  0.014078824780881405\n",
            "Loss:  0.014066601172089577\n",
            "Loss:  0.01405436359345913\n",
            "Loss:  0.014042136259377003\n",
            "Loss:  0.014029933139681816\n",
            "Loss:  0.014017730951309204\n",
            "Loss:  0.014005545526742935\n",
            "Loss:  0.013993381522595882\n",
            "Loss:  0.013981197029352188\n",
            "Loss:  0.0139690600335598\n",
            "Loss:  0.013956926763057709\n",
            "Loss:  0.013944799080491066\n",
            "Loss:  0.013932669535279274\n",
            "Loss:  0.013920591212809086\n",
            "Loss:  0.01390848308801651\n",
            "Loss:  0.013896416872739792\n",
            "Loss:  0.013884332962334156\n",
            "Loss:  0.013872290961444378\n",
            "Loss:  0.01386023499071598\n",
            "Loss:  0.013848206959664822\n",
            "Loss:  0.013836176134645939\n",
            "Loss:  0.01382417231798172\n",
            "Loss:  0.01381215825676918\n",
            "Loss:  0.013800177723169327\n",
            "Loss:  0.01378818042576313\n",
            "Loss:  0.013776218518614769\n",
            "Loss:  0.0137642752379179\n",
            "Loss:  0.01375232357531786\n",
            "Loss:  0.01374039612710476\n",
            "Loss:  0.013728465884923935\n",
            "Loss:  0.013716545887291431\n",
            "Loss:  0.01370466873049736\n",
            "Loss:  0.013692773878574371\n",
            "Loss:  0.013680889271199703\n",
            "Loss:  0.013669039122760296\n",
            "Loss:  0.01365717500448227\n",
            "Loss:  0.013645341619849205\n",
            "Loss:  0.013633501715958118\n",
            "Loss:  0.013621693477034569\n",
            "Loss:  0.013609886169433594\n",
            "Loss:  0.01359808724373579\n",
            "Loss:  0.013586304150521755\n",
            "Loss:  0.013574539683759212\n",
            "Loss:  0.013562769629061222\n",
            "Loss:  0.01355102937668562\n",
            "Loss:  0.013539283536374569\n",
            "Loss:  0.013527545146644115\n",
            "Loss:  0.013515829108655453\n",
            "Loss:  0.013504108414053917\n",
            "Loss:  0.013492405414581299\n",
            "Loss:  0.013480734080076218\n",
            "Loss:  0.013469071127474308\n",
            "Loss:  0.013457403518259525\n",
            "Loss:  0.013445748947560787\n",
            "Loss:  0.013434099033474922\n",
            "Loss:  0.013422475196421146\n",
            "Loss:  0.013410866260528564\n",
            "Loss:  0.013399261049926281\n",
            "Loss:  0.013387668877840042\n",
            "Loss:  0.013376086950302124\n",
            "Loss:  0.01336451806128025\n",
            "Loss:  0.013352954760193825\n",
            "Loss:  0.013341394253075123\n",
            "Loss:  0.013329855166375637\n",
            "Loss:  0.013318320736289024\n",
            "Loss:  0.013306796550750732\n",
            "Loss:  0.013295289129018784\n",
            "Loss:  0.013283809646964073\n",
            "Loss:  0.013272319920361042\n",
            "Loss:  0.013260837644338608\n",
            "Loss:  0.013249366544187069\n",
            "Loss:  0.01323790941387415\n",
            "Loss:  0.01322647463530302\n",
            "Loss:  0.013215038925409317\n",
            "Loss:  0.013203626498579979\n",
            "Loss:  0.013192218728363514\n",
            "Loss:  0.013180817477405071\n",
            "Loss:  0.013169428333640099\n",
            "Loss:  0.01315805222839117\n",
            "Loss:  0.01314667146652937\n",
            "Loss:  0.01313532330095768\n",
            "Loss:  0.01312396489083767\n",
            "Loss:  0.013112641870975494\n",
            "Loss:  0.013101321645081043\n",
            "Loss:  0.01308999303728342\n",
            "Loss:  0.013078700751066208\n",
            "Loss:  0.013067410327494144\n",
            "Loss:  0.013056130148470402\n",
            "Loss:  0.01304486021399498\n",
            "Loss:  0.013033604249358177\n",
            "Loss:  0.01302235946059227\n",
            "Loss:  0.013011133298277855\n",
            "Loss:  0.012999908067286015\n",
            "Loss:  0.012988672591745853\n",
            "Loss:  0.012977488338947296\n",
            "Loss:  0.012966271489858627\n",
            "Loss:  0.012955108657479286\n",
            "Loss:  0.012943926267325878\n",
            "Loss:  0.012932761572301388\n",
            "Loss:  0.012921598739922047\n",
            "Loss:  0.012910464778542519\n",
            "Loss:  0.012899336405098438\n",
            "Loss:  0.012888208962976933\n",
            "Loss:  0.012877102009952068\n",
            "Loss:  0.012866003438830376\n",
            "Loss:  0.012854909524321556\n",
            "Loss:  0.012843836098909378\n",
            "Loss:  0.01283276453614235\n",
            "Loss:  0.012821709737181664\n",
            "Loss:  0.012810660526156425\n",
            "Loss:  0.01279962994158268\n",
            "Loss:  0.01278859842568636\n",
            "Loss:  0.012777591124176979\n",
            "Loss:  0.01276658196002245\n",
            "Loss:  0.012755594216287136\n",
            "Loss:  0.012744605541229248\n",
            "Loss:  0.012733634561300278\n",
            "Loss:  0.012722663581371307\n",
            "Loss:  0.012711708433926105\n",
            "Loss:  0.012700757011771202\n",
            "Loss:  0.01268982607871294\n",
            "Loss:  0.012678896076977253\n",
            "Loss:  0.012667994946241379\n",
            "Loss:  0.012657102197408676\n",
            "Loss:  0.012646210379898548\n",
            "Loss:  0.01263534463942051\n",
            "Loss:  0.012624453753232956\n",
            "Loss:  0.012613602913916111\n",
            "Loss:  0.012602746486663818\n",
            "Loss:  0.012591901235282421\n",
            "Loss:  0.01258107926696539\n",
            "Loss:  0.012570252642035484\n",
            "Loss:  0.012559434399008751\n",
            "Loss:  0.012548645958304405\n",
            "Loss:  0.012537866830825806\n",
            "Loss:  0.01252707839012146\n",
            "Loss:  0.012516316957771778\n",
            "Loss:  0.01250556018203497\n",
            "Loss:  0.012494814582169056\n",
            "Loss:  0.012484081089496613\n",
            "Loss:  0.012473344802856445\n",
            "Loss:  0.01246262900531292\n",
            "Loss:  0.012451933696866035\n",
            "Loss:  0.012441216967999935\n",
            "Loss:  0.012430528178811073\n",
            "Loss:  0.012419860810041428\n",
            "Loss:  0.012409185990691185\n",
            "Loss:  0.012398540042340755\n",
            "Loss:  0.012387902475893497\n",
            "Loss:  0.012377270497381687\n",
            "Loss:  0.012366630136966705\n",
            "Loss:  0.012356018647551537\n",
            "Loss:  0.012345407158136368\n",
            "Loss:  0.012334822677075863\n",
            "Loss:  0.012324218638241291\n",
            "Loss:  0.012313660234212875\n",
            "Loss:  0.012303097173571587\n",
            "Loss:  0.012292535044252872\n",
            "Loss:  0.012281989678740501\n",
            "Loss:  0.012271452695131302\n",
            "Loss:  0.01226092129945755\n",
            "Loss:  0.01225041039288044\n",
            "Loss:  0.01223989948630333\n",
            "Loss:  0.012229410000145435\n",
            "Loss:  0.012218914926052094\n",
            "Loss:  0.012208440341055393\n",
            "Loss:  0.012197979725897312\n",
            "Loss:  0.012187523767352104\n",
            "Loss:  0.01217707246541977\n",
            "Loss:  0.012166645377874374\n",
            "Loss:  0.012156210839748383\n",
            "Loss:  0.01214580424129963\n",
            "Loss:  0.012135382741689682\n",
            "Loss:  0.012124997563660145\n",
            "Loss:  0.01211459282785654\n",
            "Loss:  0.01210423931479454\n",
            "Loss:  0.012093850411474705\n",
            "Loss:  0.012083503417670727\n",
            "Loss:  0.012073150835931301\n",
            "Loss:  0.01206280943006277\n",
            "Loss:  0.012052462436258793\n",
            "Loss:  0.012042168527841568\n",
            "Loss:  0.01203184574842453\n",
            "Loss:  0.01202155090868473\n",
            "Loss:  0.012011272832751274\n",
            "Loss:  0.012000993825495243\n",
            "Loss:  0.011990712024271488\n",
            "Loss:  0.01198046188801527\n",
            "Loss:  0.011970208957791328\n",
            "Loss:  0.011959957890212536\n",
            "Loss:  0.011949722655117512\n",
            "Loss:  0.011939506977796555\n",
            "Loss:  0.01192929595708847\n",
            "Loss:  0.01191909983754158\n",
            "Loss:  0.01190890558063984\n",
            "Loss:  0.011898718774318695\n",
            "Loss:  0.0118885338306427\n",
            "Loss:  0.01187838427722454\n",
            "Loss:  0.01186822447925806\n",
            "Loss:  0.011858097277581692\n",
            "Loss:  0.011847946792840958\n",
            "Loss:  0.011837824247777462\n",
            "Loss:  0.011827710084617138\n",
            "Loss:  0.011817608028650284\n",
            "Loss:  0.011807490140199661\n",
            "Loss:  0.01179741695523262\n",
            "Loss:  0.01178732793778181\n",
            "Loss:  0.011777263134717941\n",
            "Loss:  0.011767204850912094\n",
            "Loss:  0.011757160536944866\n",
            "Loss:  0.011747113429009914\n",
            "Loss:  0.011737062595784664\n",
            "Loss:  0.011727065779268742\n",
            "Loss:  0.0117170549929142\n",
            "Loss:  0.011707054451107979\n",
            "Loss:  0.011697052977979183\n",
            "Loss:  0.011687073856592178\n",
            "Loss:  0.011677107773721218\n",
            "Loss:  0.011667135171592236\n",
            "Loss:  0.011657185852527618\n",
            "Loss:  0.011647236533463001\n",
            "Loss:  0.011637292802333832\n",
            "Loss:  0.011627376079559326\n",
            "Loss:  0.011617469601333141\n",
            "Loss:  0.011607556603848934\n",
            "Loss:  0.01159765012562275\n",
            "Loss:  0.011587752029299736\n",
            "Loss:  0.011577875353395939\n",
            "Loss:  0.011568001471459866\n",
            "Loss:  0.011558144353330135\n",
            "Loss:  0.011548292823135853\n",
            "Loss:  0.011538450606167316\n",
            "Loss:  0.011528619565069675\n",
            "Loss:  0.011518796905875206\n",
            "Loss:  0.011508971452713013\n",
            "Loss:  0.011499163694679737\n",
            "Loss:  0.011489365249872208\n",
            "Loss:  0.01147958729416132\n",
            "Loss:  0.011469815857708454\n",
            "Loss:  0.011460023000836372\n",
            "Loss:  0.011450272053480148\n",
            "Loss:  0.01144052017480135\n",
            "Loss:  0.011430775746703148\n",
            "Loss:  0.011421051807701588\n",
            "Loss:  0.01141133438795805\n",
            "Loss:  0.01140162069350481\n",
            "Loss:  0.01139190886169672\n",
            "Loss:  0.011382213793694973\n",
            "Loss:  0.011372520588338375\n",
            "Loss:  0.01136284600943327\n",
            "Loss:  0.011353177949786186\n",
            "Loss:  0.011343531310558319\n",
            "Loss:  0.011333875358104706\n",
            "Loss:  0.011324219405651093\n",
            "Loss:  0.011314600706100464\n",
            "Loss:  0.01130497083067894\n",
            "Loss:  0.01129535399377346\n",
            "Loss:  0.01128575298935175\n",
            "Loss:  0.011276167817413807\n",
            "Loss:  0.011266575194895267\n",
            "Loss:  0.011256995610892773\n",
            "Loss:  0.011247429996728897\n",
            "Loss:  0.011237871833145618\n",
            "Loss:  0.011228322051465511\n",
            "Loss:  0.011218767613172531\n",
            "Loss:  0.011209236457943916\n",
            "Loss:  0.011199723929166794\n",
            "Loss:  0.011190193705260754\n",
            "Loss:  0.01118069514632225\n",
            "Loss:  0.011171186342835426\n",
            "Loss:  0.011161701753735542\n",
            "Loss:  0.011152217164635658\n",
            "Loss:  0.011142747476696968\n",
            "Loss:  0.011133294552564621\n",
            "Loss:  0.011123839765787125\n",
            "Loss:  0.011114400811493397\n",
            "Loss:  0.011104962788522243\n",
            "Loss:  0.011095544323325157\n",
            "Loss:  0.011086123064160347\n",
            "Loss:  0.011076715774834156\n",
            "Loss:  0.01106730941683054\n",
            "Loss:  0.011057903990149498\n",
            "Loss:  0.011048544198274612\n",
            "Loss:  0.011039156466722488\n",
            "Loss:  0.011029795743525028\n",
            "Loss:  0.011020452715456486\n",
            "Loss:  0.01101109478622675\n",
            "Loss:  0.011001762002706528\n",
            "Loss:  0.010992434807121754\n",
            "Loss:  0.010983102023601532\n",
            "Loss:  0.010973798111081123\n",
            "Loss:  0.010964496992528439\n",
            "Loss:  0.010955200530588627\n",
            "Loss:  0.010945908725261688\n",
            "Loss:  0.010936638340353966\n",
            "Loss:  0.010927371680736542\n",
            "Loss:  0.01091810967773199\n",
            "Loss:  0.010908862575888634\n",
            "Loss:  0.01089962013065815\n",
            "Loss:  0.010890383273363113\n",
            "Loss:  0.010881154797971249\n",
            "Loss:  0.010871939361095428\n",
            "Loss:  0.010862733237445354\n",
            "Loss:  0.010853533633053303\n",
            "Loss:  0.010844336822628975\n",
            "Loss:  0.010835167020559311\n",
            "Loss:  0.0108259916305542\n",
            "Loss:  0.010816825553774834\n",
            "Loss:  0.010807664133608341\n",
            "Loss:  0.010798522271215916\n",
            "Loss:  0.010789381340146065\n",
            "Loss:  0.01078026182949543\n",
            "Loss:  0.010771125555038452\n",
            "Loss:  0.010762018151581287\n",
            "Loss:  0.010752915404736996\n",
            "Loss:  0.010743808001279831\n",
            "Loss:  0.010734718292951584\n",
            "Loss:  0.010725641623139381\n",
            "Loss:  0.010716578923165798\n",
            "Loss:  0.010707522742450237\n",
            "Loss:  0.010698455385863781\n",
            "Loss:  0.010689415968954563\n",
            "Loss:  0.01068036537617445\n",
            "Loss:  0.010671352036297321\n",
            "Loss:  0.010662333108484745\n",
            "Loss:  0.010653325356543064\n",
            "Loss:  0.010644319467246532\n",
            "Loss:  0.010635333135724068\n",
            "Loss:  0.010626323521137238\n",
            "Loss:  0.01061735488474369\n",
            "Loss:  0.01060838345438242\n",
            "Loss:  0.010599429719150066\n",
            "Loss:  0.010590482503175735\n",
            "Loss:  0.010581526905298233\n",
            "Loss:  0.010572588071227074\n",
            "Loss:  0.01056367065757513\n",
            "Loss:  0.010554740205407143\n",
            "Loss:  0.010545823723077774\n",
            "Loss:  0.010536921210587025\n",
            "Loss:  0.01052804384380579\n",
            "Loss:  0.010519158095121384\n",
            "Loss:  0.010510274209082127\n",
            "Loss:  0.010501403361558914\n",
            "Loss:  0.01049255020916462\n",
            "Loss:  0.010483700782060623\n",
            "Loss:  0.010474856942892075\n",
            "Loss:  0.010466023348271847\n",
            "Loss:  0.010457185097038746\n",
            "Loss:  0.010448369197547436\n",
            "Loss:  0.010439565405249596\n",
            "Loss:  0.010430766269564629\n",
            "Loss:  0.01042197272181511\n",
            "Loss:  0.01041317917406559\n",
            "Loss:  0.010404403321444988\n",
            "Loss:  0.01039563026279211\n",
            "Loss:  0.010386881418526173\n",
            "Loss:  0.010378116741776466\n",
            "Loss:  0.01036936603486538\n",
            "Loss:  0.010360635817050934\n",
            "Loss:  0.01035190187394619\n",
            "Loss:  0.010343178175389767\n",
            "Loss:  0.01033447403460741\n",
            "Loss:  0.010325762443244457\n",
            "Loss:  0.010317075066268444\n",
            "Loss:  0.010308380238711834\n",
            "Loss:  0.010299704037606716\n",
            "Loss:  0.010291040875017643\n",
            "Loss:  0.010282362811267376\n",
            "Loss:  0.010273722000420094\n",
            "Loss:  0.01026507094502449\n",
            "Loss:  0.01025641430169344\n",
            "Loss:  0.010247788392007351\n",
            "Loss:  0.010239170864224434\n",
            "Loss:  0.010230554267764091\n",
            "Loss:  0.010221942327916622\n",
            "Loss:  0.010213352739810944\n",
            "Loss:  0.010204754769802094\n",
            "Loss:  0.010196170769631863\n",
            "Loss:  0.010187595151364803\n",
            "Loss:  0.010179026052355766\n",
            "Loss:  0.010170475579798222\n",
            "Loss:  0.010161922313272953\n",
            "Loss:  0.010153367184102535\n",
            "Loss:  0.010144834406673908\n",
            "Loss:  0.010136306285858154\n",
            "Loss:  0.010127801448106766\n",
            "Loss:  0.010119271464645863\n",
            "Loss:  0.010110774077475071\n",
            "Loss:  0.010102283209562302\n",
            "Loss:  0.010093795135617256\n",
            "Loss:  0.010085311718285084\n",
            "Loss:  0.010076837614178658\n",
            "Loss:  0.01006839144974947\n",
            "Loss:  0.01005992665886879\n",
            "Loss:  0.010051485151052475\n",
            "Loss:  0.010043038055300713\n",
            "Loss:  0.010034607723355293\n",
            "Loss:  0.010026181116700172\n",
            "Loss:  0.010017765685915947\n",
            "Loss:  0.010009357705712318\n",
            "Loss:  0.01000094972550869\n",
            "Loss:  0.009992561303079128\n",
            "Loss:  0.009984169155359268\n",
            "Loss:  0.009975804015994072\n",
            "Loss:  0.009967418387532234\n",
            "Loss:  0.009959060698747635\n",
            "Loss:  0.009950711391866207\n",
            "Loss:  0.009942353703081608\n",
            "Loss:  0.009934023022651672\n",
            "Loss:  0.009925689548254013\n",
            "Loss:  0.00991736724972725\n",
            "Loss:  0.009909052401781082\n",
            "Loss:  0.009900745935738087\n",
            "Loss:  0.009892447851598263\n",
            "Loss:  0.009884151630103588\n",
            "Loss:  0.009875854477286339\n",
            "Loss:  0.009867584332823753\n",
            "Loss:  0.009859323501586914\n",
            "Loss:  0.009851065464317799\n",
            "Loss:  0.009842809289693832\n",
            "Loss:  0.009834561496973038\n",
            "Loss:  0.00982632115483284\n",
            "Loss:  0.00981809664517641\n",
            "Loss:  0.009809870272874832\n",
            "Loss:  0.009801652282476425\n",
            "Loss:  0.009793446399271488\n",
            "Loss:  0.009785253554582596\n",
            "Loss:  0.00977705791592598\n",
            "Loss:  0.009768867865204811\n",
            "Loss:  0.009760690852999687\n",
            "Loss:  0.009752520360052586\n",
            "Loss:  0.009744354523718357\n",
            "Loss:  0.009736201725900173\n",
            "Loss:  0.009728060103952885\n",
            "Loss:  0.009719924069941044\n",
            "Loss:  0.009711790829896927\n",
            "Loss:  0.009703666903078556\n",
            "Loss:  0.009695549495518208\n",
            "Loss:  0.009687434881925583\n",
            "Loss:  0.009679346345365047\n",
            "Loss:  0.009671240113675594\n",
            "Loss:  0.009663143195211887\n",
            "Loss:  0.009655080735683441\n",
            "Loss:  0.009647008031606674\n",
            "Loss:  0.009638944640755653\n",
            "Loss:  0.009630896151065826\n",
            "Loss:  0.009622842073440552\n",
            "Loss:  0.009614791721105576\n",
            "Loss:  0.009606762789189816\n",
            "Loss:  0.009598738513886929\n",
            "Loss:  0.009590724483132362\n",
            "Loss:  0.009582716971635818\n",
            "Loss:  0.009574702009558678\n",
            "Loss:  0.009566719643771648\n",
            "Loss:  0.009558723308146\n",
            "Loss:  0.009550739079713821\n",
            "Loss:  0.009542757645249367\n",
            "Loss:  0.009534794837236404\n",
            "Loss:  0.009526827372610569\n",
            "Loss:  0.009518878534436226\n",
            "Loss:  0.00951093714684248\n",
            "Loss:  0.009503011591732502\n",
            "Loss:  0.00949507113546133\n",
            "Loss:  0.00948715303093195\n",
            "Loss:  0.009479232132434845\n",
            "Loss:  0.009471329860389233\n",
            "Loss:  0.009463426657021046\n",
            "Loss:  0.009455529972910881\n",
            "Loss:  0.009447647258639336\n",
            "Loss:  0.009439760819077492\n",
            "Loss:  0.009431904181838036\n",
            "Loss:  0.009424029849469662\n",
            "Loss:  0.009416165761649609\n",
            "Loss:  0.009408331476151943\n",
            "Loss:  0.00940047949552536\n",
            "Loss:  0.009392649866640568\n",
            "Loss:  0.009384818375110626\n",
            "Loss:  0.00937701016664505\n",
            "Loss:  0.009369193576276302\n",
            "Loss:  0.009361380711197853\n",
            "Loss:  0.009353583678603172\n",
            "Loss:  0.009345803409814835\n",
            "Loss:  0.0093380156904459\n",
            "Loss:  0.009330225177109241\n",
            "Loss:  0.009322465397417545\n",
            "Loss:  0.009314696304500103\n",
            "Loss:  0.009306945838034153\n",
            "Loss:  0.009299206547439098\n",
            "Loss:  0.00929146260023117\n",
            "Loss:  0.00928373821079731\n",
            "Loss:  0.00927598774433136\n",
            "Loss:  0.009268278256058693\n",
            "Loss:  0.009260570630431175\n",
            "Loss:  0.009252858348190784\n",
            "Loss:  0.00924516562372446\n",
            "Loss:  0.00923747569322586\n",
            "Loss:  0.009229778312146664\n",
            "Loss:  0.009222120977938175\n",
            "Loss:  0.009214446879923344\n",
            "Loss:  0.009206775575876236\n",
            "Loss:  0.009199120104312897\n",
            "Loss:  0.009191465564072132\n",
            "Loss:  0.00918384175747633\n",
            "Loss:  0.009176190942525864\n",
            "Loss:  0.00916857086122036\n",
            "Loss:  0.009160962887108326\n",
            "Loss:  0.009153341874480247\n",
            "Loss:  0.00914573110640049\n",
            "Loss:  0.009138133376836777\n",
            "Loss:  0.009130545891821384\n",
            "Loss:  0.009122973307967186\n",
            "Loss:  0.009115387685596943\n",
            "Loss:  0.00910782441496849\n",
            "Loss:  0.009100249968469143\n",
            "Loss:  0.009092695079743862\n",
            "Loss:  0.00908514391630888\n",
            "Loss:  0.009077602997422218\n",
            "Loss:  0.009070063009858131\n",
            "Loss:  0.00906254630535841\n",
            "Loss:  0.009055015631020069\n",
            "Loss:  0.009047506377100945\n",
            "Loss:  0.00904000736773014\n",
            "Loss:  0.009032496251165867\n",
            "Loss:  0.009025011211633682\n",
            "Loss:  0.009017523378133774\n",
            "Loss:  0.009010041132569313\n",
            "Loss:  0.00900256261229515\n",
            "Loss:  0.008995101787149906\n",
            "Loss:  0.00898764282464981\n",
            "Loss:  0.008980185724794865\n",
            "Loss:  0.008972752839326859\n",
            "Loss:  0.008965302258729935\n",
            "Loss:  0.008957870304584503\n",
            "Loss:  0.00895044393837452\n",
            "Loss:  0.008943035267293453\n",
            "Loss:  0.008935612626373768\n",
            "Loss:  0.008928226307034492\n",
            "Loss:  0.008920819498598576\n",
            "Loss:  0.008913423866033554\n",
            "Loss:  0.008906036615371704\n",
            "Loss:  0.008898667991161346\n",
            "Loss:  0.008891292847692966\n",
            "Loss:  0.008883930742740631\n",
            "Loss:  0.008876566775143147\n",
            "Loss:  0.008869219571352005\n",
            "Loss:  0.008861885406076908\n",
            "Loss:  0.008854549378156662\n",
            "Loss:  0.008847210556268692\n",
            "Loss:  0.008839894086122513\n",
            "Loss:  0.008832585997879505\n",
            "Loss:  0.008825266733765602\n",
            "Loss:  0.008817968890070915\n",
            "Loss:  0.008810680359601974\n",
            "Loss:  0.008803389966487885\n",
            "Loss:  0.008796113543212414\n",
            "Loss:  0.008788825944066048\n",
            "Loss:  0.008781563490629196\n",
            "Loss:  0.008774292655289173\n",
            "Loss:  0.00876704417169094\n",
            "Loss:  0.008759800344705582\n",
            "Loss:  0.008752552792429924\n",
            "Loss:  0.008745322935283184\n",
            "Loss:  0.008738098666071892\n",
            "Loss:  0.008730877190828323\n",
            "Loss:  0.008723658509552479\n",
            "Loss:  0.008716456592082977\n",
            "Loss:  0.008709256537258625\n",
            "Loss:  0.008702043443918228\n",
            "Loss:  0.008694865740835667\n",
            "Loss:  0.008687674067914486\n",
            "Loss:  0.00868049543350935\n",
            "Loss:  0.008673332631587982\n",
            "Loss:  0.00866616889834404\n",
            "Loss:  0.008659026585519314\n",
            "Loss:  0.00865186844021082\n",
            "Loss:  0.008644725196063519\n",
            "Loss:  0.008637589402496815\n",
            "Loss:  0.008630473166704178\n",
            "Loss:  0.008623341098427773\n",
            "Loss:  0.008616224862635136\n",
            "Loss:  0.008609114214777946\n",
            "Loss:  0.00860201008617878\n",
            "Loss:  0.008594912476837635\n",
            "Loss:  0.008587819524109364\n",
            "Loss:  0.008580734953284264\n",
            "Loss:  0.008573662489652634\n",
            "Loss:  0.008566590957343578\n",
            "Loss:  0.008559525944292545\n",
            "Loss:  0.008552477695047855\n",
            "Loss:  0.008545425720512867\n",
            "Loss:  0.008538379333913326\n",
            "Loss:  0.008531350642442703\n",
            "Loss:  0.008524306118488312\n",
            "Loss:  0.008517295122146606\n",
            "Loss:  0.00851026363670826\n",
            "Loss:  0.008503259159624577\n",
            "Loss:  0.008496247231960297\n",
            "Loss:  0.00848925020545721\n",
            "Loss:  0.008482245728373528\n",
            "Loss:  0.00847527664154768\n",
            "Loss:  0.008468284271657467\n",
            "Loss:  0.008461322635412216\n",
            "Loss:  0.00845434982329607\n",
            "Loss:  0.008447390049695969\n",
            "Loss:  0.008440443314611912\n",
            "Loss:  0.008433488197624683\n",
            "Loss:  0.008426552638411522\n",
            "Loss:  0.008419620804488659\n",
            "Loss:  0.008412694558501244\n",
            "Loss:  0.008405757136642933\n",
            "Loss:  0.008398856967687607\n",
            "Loss:  0.008391953073441982\n",
            "Loss:  0.008385035209357738\n",
            "Loss:  0.008378148078918457\n",
            "Loss:  0.008371256291866302\n",
            "Loss:  0.00836437102407217\n",
            "Loss:  0.008357479237020016\n",
            "Loss:  0.008350624702870846\n",
            "Loss:  0.008343755267560482\n",
            "Loss:  0.008336905390024185\n",
            "Loss:  0.008330045267939568\n",
            "Loss:  0.008323205634951591\n",
            "Loss:  0.008316372521221638\n",
            "Loss:  0.008309529162943363\n",
            "Loss:  0.008302709087729454\n",
            "Loss:  0.008295884355902672\n",
            "Loss:  0.00828906986862421\n",
            "Loss:  0.008282252587378025\n",
            "Loss:  0.008275450207293034\n",
            "Loss:  0.008268659003078938\n",
            "Loss:  0.008261867798864841\n",
            "Loss:  0.00825509149581194\n",
            "Loss:  0.008248304948210716\n",
            "Loss:  0.008241541683673859\n",
            "Loss:  0.008234774693846703\n",
            "Loss:  0.008228013291954994\n",
            "Loss:  0.008221256546676159\n",
            "Loss:  0.008214508183300495\n",
            "Loss:  0.008207770995795727\n",
            "Loss:  0.008201043121516705\n",
            "Loss:  0.008194316178560257\n",
            "Loss:  0.008187607862055302\n",
            "Loss:  0.008180887438356876\n",
            "Loss:  0.008174179121851921\n",
            "Loss:  0.008167484775185585\n",
            "Loss:  0.008160780183970928\n",
            "Loss:  0.008154097013175488\n",
            "Loss:  0.008147395215928555\n",
            "Loss:  0.008140728808939457\n",
            "Loss:  0.00813404843211174\n",
            "Loss:  0.008127382025122643\n",
            "Loss:  0.008120733313262463\n",
            "Loss:  0.008114068768918514\n",
            "Loss:  0.00810743123292923\n",
            "Loss:  0.008100790902972221\n",
            "Loss:  0.00809414405375719\n",
            "Loss:  0.00808752328157425\n",
            "Loss:  0.00808089878410101\n",
            "Loss:  0.008074289187788963\n",
            "Loss:  0.008067681454122066\n",
            "Loss:  0.008061069995164871\n",
            "Loss:  0.008054467849433422\n",
            "Loss:  0.008047864772379398\n",
            "Loss:  0.008041278459131718\n",
            "Loss:  0.008034701459109783\n",
            "Loss:  0.0080281225964427\n",
            "Loss:  0.008021564222872257\n",
            "Loss:  0.008014998398721218\n",
            "Loss:  0.008008436299860477\n",
            "Loss:  0.008001888170838356\n",
            "Loss:  0.00799535121768713\n",
            "Loss:  0.007988814264535904\n",
            "Loss:  0.007982282899320126\n",
            "Loss:  0.007975749671459198\n",
            "Loss:  0.007969236001372337\n",
            "Loss:  0.007962726056575775\n",
            "Loss:  0.007956207729876041\n",
            "Loss:  0.00794970989227295\n",
            "Loss:  0.007943210192024708\n",
            "Loss:  0.007936721667647362\n",
            "Loss:  0.007930236868560314\n",
            "Loss:  0.007923759520053864\n",
            "Loss:  0.007917291484773159\n",
            "Loss:  0.007910815067589283\n",
            "Loss:  0.007904366590082645\n",
            "Loss:  0.007897910661995411\n",
            "Loss:  0.007891451939940453\n",
            "Loss:  0.00788501650094986\n",
            "Loss:  0.00787857174873352\n",
            "Loss:  0.007872141897678375\n",
            "Loss:  0.007865723222494125\n",
            "Loss:  0.00785929337143898\n",
            "Loss:  0.007852890528738499\n",
            "Loss:  0.007846477441489697\n",
            "Loss:  0.007840068079531193\n",
            "Loss:  0.007833685725927353\n",
            "Loss:  0.007827294059097767\n",
            "Loss:  0.007820915430784225\n",
            "Loss:  0.007814528420567513\n",
            "Loss:  0.007808151189237833\n",
            "Loss:  0.0078017832711339\n",
            "Loss:  0.007795421406626701\n",
            "Loss:  0.007789064198732376\n",
            "Loss:  0.007782720495015383\n",
            "Loss:  0.007776377256959677\n",
            "Loss:  0.007770031690597534\n",
            "Loss:  0.007763701491057873\n",
            "Loss:  0.007757376879453659\n",
            "Loss:  0.0077510555274784565\n",
            "Loss:  0.0077447425574064255\n",
            "Loss:  0.007738431915640831\n",
            "Loss:  0.00773212593048811\n",
            "Loss:  0.007725825998932123\n",
            "Loss:  0.007719540502876043\n",
            "Loss:  0.0077132428996264935\n",
            "Loss:  0.007706967182457447\n",
            "Loss:  0.007700702175498009\n",
            "Loss:  0.007694421336054802\n",
            "Loss:  0.007688163314014673\n",
            "Loss:  0.007681906223297119\n",
            "Loss:  0.007675646338611841\n",
            "Loss:  0.0076694064773619175\n",
            "Loss:  0.007663169410079718\n",
            "Loss:  0.0076569258235394955\n",
            "Loss:  0.007650702726095915\n",
            "Loss:  0.007644482422620058\n",
            "Loss:  0.007638260256499052\n",
            "Loss:  0.007632048334926367\n",
            "Loss:  0.007625839672982693\n",
            "Loss:  0.007619637064635754\n",
            "Loss:  0.007613443303853273\n",
            "Loss:  0.007607252802699804\n",
            "Loss:  0.00760106323286891\n",
            "Loss:  0.007594895549118519\n",
            "Loss:  0.007588725537061691\n",
            "Loss:  0.0075825476087629795\n",
            "Loss:  0.007576391566544771\n",
            "Loss:  0.007570241112262011\n",
            "Loss:  0.007564084604382515\n",
            "Loss:  0.007557936478406191\n",
            "Loss:  0.007551802787929773\n",
            "Loss:  0.007545665372163057\n",
            "Loss:  0.007539542857557535\n",
            "Loss:  0.007533420342952013\n",
            "Loss:  0.007527304347604513\n",
            "Loss:  0.007521191146224737\n",
            "Loss:  0.007515089586377144\n",
            "Loss:  0.007508988957852125\n",
            "Loss:  0.007502890191972256\n",
            "Loss:  0.007496810983866453\n",
            "Loss:  0.007490723859518766\n",
            "Loss:  0.007484635803848505\n",
            "Loss:  0.0074785733595490456\n",
            "Loss:  0.007472510449588299\n",
            "Loss:  0.007466448005288839\n",
            "Loss:  0.007460400462150574\n",
            "Loss:  0.007454348262399435\n",
            "Loss:  0.007448300719261169\n",
            "Loss:  0.007442261092364788\n",
            "Loss:  0.007436225190758705\n",
            "Loss:  0.007430205121636391\n",
            "Loss:  0.007424195297062397\n",
            "Loss:  0.007418171968311071\n",
            "Loss:  0.007412157021462917\n",
            "Loss:  0.007406155578792095\n",
            "Loss:  0.007400150876492262\n",
            "Loss:  0.007394162472337484\n",
            "Loss:  0.007388178259134293\n",
            "Loss:  0.007382184732705355\n",
            "Loss:  0.007376215886324644\n",
            "Loss:  0.0073702456429600716\n",
            "Loss:  0.007364271674305201\n",
            "Loss:  0.007358319126069546\n",
            "Loss:  0.007352353539317846\n",
            "Loss:  0.007346400525420904\n",
            "Loss:  0.007340454030781984\n",
            "Loss:  0.007334509398788214\n",
            "Loss:  0.007328586652874947\n",
            "Loss:  0.0073226504027843475\n",
            "Loss:  0.007316723931580782\n",
            "Loss:  0.007310813292860985\n",
            "Loss:  0.0073048812337219715\n",
            "Loss:  0.007298977579921484\n",
            "Loss:  0.0072930846363306046\n",
            "Loss:  0.007287173066288233\n",
            "Loss:  0.0072812847793102264\n",
            "Loss:  0.007275402080267668\n",
            "Loss:  0.007269504480063915\n",
            "Loss:  0.0072636534459888935\n",
            "Loss:  0.0072577702812850475\n",
            "Loss:  0.007251912262290716\n",
            "Loss:  0.007246049586683512\n",
            "Loss:  0.007240196689963341\n",
            "Loss:  0.0072343493811786175\n",
            "Loss:  0.007228508125990629\n",
            "Loss:  0.007222671993076801\n",
            "Loss:  0.0072168405167758465\n",
            "Loss:  0.007211015094071627\n",
            "Loss:  0.007205194793641567\n",
            "Loss:  0.007199379149824381\n",
            "Loss:  0.007193554658442736\n",
            "Loss:  0.007187761832028627\n",
            "Loss:  0.007181951310485601\n",
            "Loss:  0.007176164537668228\n",
            "Loss:  0.007170361466705799\n",
            "Loss:  0.007164597976952791\n",
            "Loss:  0.007158799562603235\n",
            "Loss:  0.007153023034334183\n",
            "Loss:  0.007147259544581175\n",
            "Loss:  0.007141487207263708\n",
            "Loss:  0.007135738153010607\n",
            "Loss:  0.007129982579499483\n",
            "Loss:  0.007124230265617371\n",
            "Loss:  0.00711848633363843\n",
            "Loss:  0.0071127456612885\n",
            "Loss:  0.007107018958777189\n",
            "Loss:  0.00710128853097558\n",
            "Loss:  0.007095566019415855\n",
            "Loss:  0.007089854683727026\n",
            "Loss:  0.007084141485393047\n",
            "Loss:  0.007078425493091345\n",
            "Loss:  0.00707272719591856\n",
            "Loss:  0.007067028898745775\n",
            "Loss:  0.0070613413117825985\n",
            "Loss:  0.007055652793496847\n",
            "Loss:  0.007049969397485256\n",
            "Loss:  0.00704429903998971\n",
            "Loss:  0.007038617506623268\n",
            "Loss:  0.007032955996692181\n",
            "Loss:  0.007027292158454657\n",
            "Loss:  0.0070216418243944645\n",
            "Loss:  0.007015985436737537\n",
            "Loss:  0.007010345347225666\n",
            "Loss:  0.007004705723375082\n",
            "Loss:  0.006999076809734106\n",
            "Loss:  0.006993445102125406\n",
            "Loss:  0.006987818516790867\n",
            "Loss:  0.006982199382036924\n",
            "Loss:  0.006976583506911993\n",
            "Loss:  0.006970982067286968\n",
            "Loss:  0.006965381558984518\n",
            "Loss:  0.0069597698748111725\n",
            "Loss:  0.006954181008040905\n",
            "Loss:  0.006948594469577074\n",
            "Loss:  0.006942998617887497\n",
            "Loss:  0.006937416736036539\n",
            "Loss:  0.006931854411959648\n",
            "Loss:  0.006926281377673149\n",
            "Loss:  0.006920712534338236\n",
            "Loss:  0.006915153935551643\n",
            "Loss:  0.006909599993377924\n",
            "Loss:  0.006904060021042824\n",
            "Loss:  0.006898511666804552\n",
            "Loss:  0.006892973091453314\n",
            "Loss:  0.006887444760650396\n",
            "Loss:  0.006881924346089363\n",
            "Loss:  0.006876388564705849\n",
            "Loss:  0.006870866287499666\n",
            "Loss:  0.006865363568067551\n",
            "Loss:  0.006859855260699987\n",
            "Loss:  0.006854338571429253\n",
            "Loss:  0.006848836317658424\n",
            "Loss:  0.006843345705419779\n",
            "Loss:  0.006837853230535984\n",
            "Loss:  0.006832378916442394\n",
            "Loss:  0.006826888304203749\n",
            "Loss:  0.00682142423465848\n",
            "Loss:  0.006815961562097073\n",
            "Loss:  0.006810493301600218\n",
            "Loss:  0.006805036682635546\n",
            "Loss:  0.006799585651606321\n",
            "Loss:  0.006794131826609373\n",
            "Loss:  0.006788690108805895\n",
            "Loss:  0.006783248391002417\n",
            "Loss:  0.00677781505510211\n",
            "Loss:  0.006772384513169527\n",
            "Loss:  0.006766949780285358\n",
            "Loss:  0.006761537399142981\n",
            "Loss:  0.006756115704774857\n",
            "Loss:  0.00675071170553565\n",
            "Loss:  0.006745307240635157\n",
            "Loss:  0.006739920005202293\n",
            "Loss:  0.006734511349350214\n",
            "Loss:  0.006729127373546362\n",
            "Loss:  0.006723738741129637\n",
            "Loss:  0.006718358490616083\n",
            "Loss:  0.006712981965392828\n",
            "Loss:  0.006707608234137297\n",
            "Loss:  0.006702245678752661\n",
            "Loss:  0.006696880329400301\n",
            "Loss:  0.006691532675176859\n",
            "Loss:  0.006686183158308268\n",
            "Loss:  0.006680825259536505\n",
            "Loss:  0.006675500888377428\n",
            "Loss:  0.006670153234153986\n",
            "Loss:  0.006664818152785301\n",
            "Loss:  0.0066594891250133514\n",
            "Loss:  0.0066541749984025955\n",
            "Loss:  0.006648862734436989\n",
            "Loss:  0.00664354907348752\n",
            "Loss:  0.006638237275183201\n",
            "Loss:  0.006632933393120766\n",
            "Loss:  0.006627633702009916\n",
            "Loss:  0.006622333079576492\n",
            "Loss:  0.006617051549255848\n",
            "Loss:  0.006611758843064308\n",
            "Loss:  0.006606483832001686\n",
            "Loss:  0.006601207423955202\n",
            "Loss:  0.006595938932150602\n",
            "Loss:  0.006590670440346003\n",
            "Loss:  0.006585416384041309\n",
            "Loss:  0.006580161862075329\n",
            "Loss:  0.006574907340109348\n",
            "Loss:  0.006569666787981987\n",
            "Loss:  0.0065644229762256145\n",
            "Loss:  0.006559188477694988\n",
            "Loss:  0.006553947925567627\n",
            "Loss:  0.006548728793859482\n",
            "Loss:  0.006543503608554602\n",
            "Loss:  0.006538278888911009\n",
            "Loss:  0.006533067207783461\n",
            "Loss:  0.006527867168188095\n",
            "Loss:  0.006522651761770248\n",
            "Loss:  0.006517457775771618\n",
            "Loss:  0.006512261461466551\n",
            "Loss:  0.0065070767886936665\n",
            "Loss:  0.006501889321953058\n",
            "Loss:  0.0064967041835188866\n",
            "Loss:  0.006491532549262047\n",
            "Loss:  0.006486351601779461\n",
            "Loss:  0.006481196265667677\n",
            "Loss:  0.0064760418608784676\n",
            "Loss:  0.006470867898315191\n",
            "Loss:  0.006465731654316187\n",
            "Loss:  0.006460580043494701\n",
            "Loss:  0.006455438677221537\n",
            "Loss:  0.006450302433222532\n",
            "Loss:  0.0064451610669493675\n",
            "Loss:  0.006440035067498684\n",
            "Loss:  0.006434908136725426\n",
            "Loss:  0.006429785862565041\n",
            "Loss:  0.006424678023904562\n",
            "Loss:  0.006419559009373188\n",
            "Loss:  0.006414467468857765\n",
            "Loss:  0.0064093563705682755\n",
            "Loss:  0.0064042662270367146\n",
            "Loss:  0.006399169098585844\n",
            "Loss:  0.006394080352038145\n",
            "Loss:  0.006388999056071043\n",
            "Loss:  0.006383913569152355\n",
            "Loss:  0.006378835067152977\n",
            "Loss:  0.0063737742602825165\n",
            "Loss:  0.00636870926246047\n",
            "Loss:  0.006363649386912584\n",
            "Loss:  0.006358589977025986\n",
            "Loss:  0.006353539880365133\n",
            "Loss:  0.006348480470478535\n",
            "Loss:  0.006343449465930462\n",
            "Loss:  0.006338413804769516\n",
            "Loss:  0.006333380937576294\n",
            "Loss:  0.0063283550553023815\n",
            "Loss:  0.006323325447738171\n",
            "Loss:  0.006318301428109407\n",
            "Loss:  0.006313294172286987\n",
            "Loss:  0.006308276206254959\n",
            "Loss:  0.006303276866674423\n",
            "Loss:  0.006298271007835865\n",
            "Loss:  0.006293283775448799\n",
            "Loss:  0.006288284435868263\n",
            "Loss:  0.006283290218561888\n",
            "Loss:  0.006278310902416706\n",
            "Loss:  0.006273331586271524\n",
            "Loss:  0.006268346216529608\n",
            "Loss:  0.0062633841298520565\n",
            "Loss:  0.006258413661271334\n",
            "Loss:  0.006253455765545368\n",
            "Loss:  0.006248497404158115\n",
            "Loss:  0.006243543233722448\n",
            "Loss:  0.006238603498786688\n",
            "Loss:  0.006233649328351021\n",
            "Loss:  0.0062287235632538795\n",
            "Loss:  0.006223791744560003\n",
            "Loss:  0.006218867842108011\n",
            "Loss:  0.006213926710188389\n",
            "Loss:  0.00620900746434927\n",
            "Loss:  0.006204087752848864\n",
            "Loss:  0.006199185736477375\n",
            "Loss:  0.006194261834025383\n",
            "Loss:  0.006189361680299044\n",
            "Loss:  0.006184475030750036\n",
            "Loss:  0.006179575342684984\n",
            "Loss:  0.006174685899168253\n",
            "Loss:  0.006169797386974096\n",
            "Loss:  0.006164909806102514\n",
            "Loss:  0.006160031072795391\n",
            "Loss:  0.006155163515359163\n",
            "Loss:  0.006150298286229372\n",
            "Loss:  0.00614542979747057\n",
            "Loss:  0.006140572484582663\n",
            "Loss:  0.006135719362646341\n",
            "Loss:  0.006130864843726158\n",
            "Loss:  0.006126020103693008\n",
            "Loss:  0.006121177691966295\n",
            "Loss:  0.0061163390055298805\n",
            "Loss:  0.0061115119606256485\n",
            "Loss:  0.006106670945882797\n",
            "Loss:  0.006101846229285002\n",
            "Loss:  0.006097020581364632\n",
            "Loss:  0.006092218216508627\n",
            "Loss:  0.0060874102637171745\n",
            "Loss:  0.006082587409764528\n",
            "Loss:  0.006077786907553673\n",
            "Loss:  0.006072984542697668\n",
            "Loss:  0.006068199872970581\n",
            "Loss:  0.006063397042453289\n",
            "Loss:  0.006058622617274523\n",
            "Loss:  0.0060538421384990215\n",
            "Loss:  0.006049069110304117\n",
            "Loss:  0.006044281180948019\n",
            "Loss:  0.006039520725607872\n",
            "Loss:  0.00603475933894515\n",
            "Loss:  0.006029990967363119\n",
            "Loss:  0.006025239359587431\n",
            "Loss:  0.0060204993933439255\n",
            "Loss:  0.006015743128955364\n",
            "Loss:  0.0060109943151474\n",
            "Loss:  0.006006257142871618\n",
            "Loss:  0.0060015213675796986\n",
            "Loss:  0.005996793508529663\n",
            "Loss:  0.005992067977786064\n",
            "Loss:  0.005987341981381178\n",
            "Loss:  0.005982639268040657\n",
            "Loss:  0.005977914202958345\n",
            "Loss:  0.005973207298666239\n",
            "Loss:  0.005968509707599878\n",
            "Loss:  0.005963795352727175\n",
            "Loss:  0.005959099158644676\n",
            "Loss:  0.00595440249890089\n",
            "Loss:  0.005949714221060276\n",
            "Loss:  0.005945032462477684\n",
            "Loss:  0.005940352566540241\n",
            "Loss:  0.00593568105250597\n",
            "Loss:  0.005931004416197538\n",
            "Loss:  0.005926338490098715\n",
            "Loss:  0.005921683739870787\n",
            "Loss:  0.005917016416788101\n",
            "Loss:  0.00591236911714077\n",
            "Loss:  0.005907710175961256\n",
            "Loss:  0.005903075449168682\n",
            "Loss:  0.005898416507989168\n",
            "Loss:  0.005893780384212732\n",
            "Loss:  0.005889146123081446\n",
            "Loss:  0.005884507670998573\n",
            "Loss:  0.005879885051399469\n",
            "Loss:  0.005875268019735813\n",
            "Loss:  0.005870653782039881\n",
            "Loss:  0.005866044666618109\n",
            "Loss:  0.005861418787389994\n",
            "Loss:  0.005856821313500404\n",
            "Loss:  0.005852218251675367\n",
            "Loss:  0.005847620777785778\n",
            "Loss:  0.005843035411089659\n",
            "Loss:  0.00583844818174839\n",
            "Loss:  0.005833860486745834\n",
            "Loss:  0.005829279311001301\n",
            "Loss:  0.005824704188853502\n",
            "Loss:  0.0058201332576572895\n",
            "Loss:  0.005815563723444939\n",
            "Loss:  0.005810998845845461\n",
            "Loss:  0.005806436762213707\n",
            "Loss:  0.005801869556307793\n",
            "Loss:  0.005797326564788818\n",
            "Loss:  0.0057927812449634075\n",
            "Loss:  0.005788235459476709\n",
            "Loss:  0.005783700384199619\n",
            "Loss:  0.0057791657745838165\n",
            "Loss:  0.005774627905339003\n",
            "Loss:  0.005770100746303797\n",
            "Loss:  0.005765577778220177\n",
            "Loss:  0.005761057138442993\n",
            "Loss:  0.005756540223956108\n",
            "Loss:  0.005752031225711107\n",
            "Loss:  0.005747518036514521\n",
            "Loss:  0.005743018351495266\n",
            "Loss:  0.005738524720072746\n",
            "Loss:  0.005734019447118044\n",
            "Loss:  0.0057295262813568115\n",
            "Loss:  0.005725041497498751\n",
            "Loss:  0.005720562767237425\n",
            "Loss:  0.005716072861105204\n",
            "Loss:  0.0057115997187793255\n",
            "Loss:  0.005707129370421171\n",
            "Loss:  0.005702666472643614\n",
            "Loss:  0.005698197986930609\n",
            "Loss:  0.005693740677088499\n",
            "Loss:  0.005689284298568964\n",
            "Loss:  0.005684829782694578\n",
            "Loss:  0.005680382717400789\n",
            "Loss:  0.005675937049090862\n",
            "Loss:  0.005671503022313118\n",
            "Loss:  0.005667060613632202\n",
            "Loss:  0.005662635434418917\n",
            "Loss:  0.005658205598592758\n",
            "Loss:  0.005653786472976208\n",
            "Loss:  0.00564936175942421\n",
            "Loss:  0.0056449417024850845\n",
            "Loss:  0.0056405337527394295\n",
            "Loss:  0.005636115558445454\n",
            "Loss:  0.005631712730973959\n",
            "Loss:  0.005627312697470188\n",
            "Loss:  0.005622919648885727\n",
            "Loss:  0.005618527997285128\n",
            "Loss:  0.005614143796265125\n",
            "Loss:  0.005609764251857996\n",
            "Loss:  0.005605380050837994\n",
            "Loss:  0.005600997246801853\n",
            "Loss:  0.005596627481281757\n",
            "Loss:  0.005592260509729385\n",
            "Loss:  0.005587884224951267\n",
            "Loss:  0.0055835191160440445\n",
            "Loss:  0.005579167976975441\n",
            "Loss:  0.005574819631874561\n",
            "Loss:  0.0055704740807414055\n",
            "Loss:  0.005566119682043791\n",
            "Loss:  0.005561781115829945\n",
            "Loss:  0.005557440686970949\n",
            "Loss:  0.005553111433982849\n",
            "Loss:  0.005548777058720589\n",
            "Loss:  0.005544450134038925\n",
            "Loss:  0.005540119484066963\n",
            "Loss:  0.0055358074605464935\n",
            "Loss:  0.005531495902687311\n",
            "Loss:  0.005527179688215256\n",
            "Loss:  0.005522878374904394\n",
            "Loss:  0.005518571939319372\n",
            "Loss:  0.005514268763363361\n",
            "Loss:  0.005509978160262108\n",
            "Loss:  0.005505685694515705\n",
            "Loss:  0.0055013978853821754\n",
            "Loss:  0.005497116595506668\n",
            "Loss:  0.0054928334429860115\n",
            "Loss:  0.00548854935914278\n",
            "Loss:  0.005484285764396191\n",
            "Loss:  0.005480004474520683\n",
            "Loss:  0.0054757483303546906\n",
            "Loss:  0.005471482407301664\n",
            "Loss:  0.00546722998842597\n",
            "Loss:  0.005462970118969679\n",
            "Loss:  0.005458711180835962\n",
            "Loss:  0.005454478319734335\n",
            "Loss:  0.005450231023132801\n",
            "Loss:  0.005445991642773151\n",
            "Loss:  0.005441746674478054\n",
            "Loss:  0.005437525920569897\n",
            "Loss:  0.005433303769677877\n",
            "Loss:  0.005429070442914963\n",
            "Loss:  0.005424848757684231\n",
            "Loss:  0.005420631729066372\n",
            "Loss:  0.005416424944996834\n",
            "Loss:  0.005412214435636997\n",
            "Loss:  0.005408004391938448\n",
            "Loss:  0.005403805989772081\n",
            "Loss:  0.005399609915912151\n",
            "Loss:  0.005395412445068359\n",
            "Loss:  0.005391224753111601\n",
            "Loss:  0.005387032404541969\n",
            "Loss:  0.005382854491472244\n",
            "Loss:  0.005378672853112221\n",
            "Loss:  0.005374503321945667\n",
            "Loss:  0.005370330065488815\n",
            "Loss:  0.005366163793951273\n",
            "Loss:  0.005361990537494421\n",
            "Loss:  0.005357836373150349\n",
            "Loss:  0.005353673826903105\n",
            "Loss:  0.005349522456526756\n",
            "Loss:  0.005345364101231098\n",
            "Loss:  0.005341222509741783\n",
            "Loss:  0.005337074864655733\n",
            "Loss:  0.005332941189408302\n",
            "Loss:  0.00532880425453186\n",
            "Loss:  0.005324672907590866\n",
            "Loss:  0.0053205471485853195\n",
            "Loss:  0.005316417198628187\n",
            "Loss:  0.005312308203428984\n",
            "Loss:  0.005308191291987896\n",
            "Loss:  0.005304078571498394\n",
            "Loss:  0.005299972370266914\n",
            "Loss:  0.005295870825648308\n",
            "Loss:  0.005291762296110392\n",
            "Loss:  0.0052876584231853485\n",
            "Loss:  0.005283560138195753\n",
            "Loss:  0.005279480013996363\n",
            "Loss:  0.005275384988635778\n",
            "Loss:  0.005271304864436388\n",
            "Loss:  0.005267217289656401\n",
            "Loss:  0.005263146013021469\n",
            "Loss:  0.005259069614112377\n",
            "Loss:  0.005254993215203285\n",
            "Loss:  0.005250929389148951\n",
            "Loss:  0.005246875341981649\n",
            "Loss:  0.005242820829153061\n",
            "Loss:  0.0052387588657438755\n",
            "Loss:  0.005234703421592712\n",
            "Loss:  0.005230660550296307\n",
            "Loss:  0.005226612091064453\n",
            "Loss:  0.00522257573902607\n",
            "Loss:  0.005218537990003824\n",
            "Loss:  0.005214507691562176\n",
            "Loss:  0.005210469476878643\n",
            "Loss:  0.005206444766372442\n",
            "Loss:  0.005202427972108126\n",
            "Loss:  0.005198402795940638\n",
            "Loss:  0.005194393452256918\n",
            "Loss:  0.005190372932702303\n",
            "Loss:  0.005186372436583042\n",
            "Loss:  0.005182361230254173\n",
            "Loss:  0.005178355611860752\n",
            "Loss:  0.005174357909709215\n",
            "Loss:  0.005170365795493126\n",
            "Loss:  0.005166382994502783\n",
            "Loss:  0.005162392742931843\n",
            "Loss:  0.0051584006287157536\n",
            "Loss:  0.005154432263225317\n",
            "Loss:  0.00515044666826725\n",
            "Loss:  0.005146468989551067\n",
            "Loss:  0.005142501089721918\n",
            "Loss:  0.0051385327242314816\n",
            "Loss:  0.005134578328579664\n",
            "Loss:  0.0051306081004440784\n",
            "Loss:  0.005126648116856813\n",
            "Loss:  0.005122700240463018\n",
            "Loss:  0.005118752829730511\n",
            "Loss:  0.005114810541272163\n",
            "Loss:  0.005110860336571932\n",
            "Loss:  0.005106927361339331\n",
            "Loss:  0.005102995783090591\n",
            "Loss:  0.0050990586169064045\n",
            "Loss:  0.005095130298286676\n",
            "Loss:  0.005091211758553982\n",
            "Loss:  0.005087288096547127\n",
            "Loss:  0.005083366762846708\n",
            "Loss:  0.005079454742372036\n",
            "Loss:  0.005075544584542513\n",
            "Loss:  0.005071645602583885\n",
            "Loss:  0.005067737773060799\n",
            "Loss:  0.005063842982053757\n",
            "Loss:  0.005059945397078991\n",
            "Loss:  0.0050560529343783855\n",
            "Loss:  0.005052160006016493\n",
            "Loss:  0.005048273131251335\n",
            "Loss:  0.005044393707066774\n",
            "Loss:  0.005040510091930628\n",
            "Loss:  0.005036632996052504\n",
            "Loss:  0.005032756831496954\n",
            "Loss:  0.005028888583183289\n",
            "Loss:  0.00502502266317606\n",
            "Loss:  0.005021152086555958\n",
            "Loss:  0.005017301067709923\n",
            "Loss:  0.005013448651880026\n",
            "Loss:  0.005009595770388842\n",
            "Loss:  0.0050057475455105305\n",
            "Loss:  0.005001900251954794\n",
            "Loss:  0.004998051095753908\n",
            "Loss:  0.0049942173063755035\n",
            "Loss:  0.004990379326045513\n",
            "Loss:  0.0049865529872477055\n",
            "Loss:  0.00498271593824029\n",
            "Loss:  0.004978896584361792\n",
            "Loss:  0.004975066054612398\n",
            "Loss:  0.004971252754330635\n",
            "Loss:  0.004967439919710159\n",
            "Loss:  0.004963622894138098\n",
            "Loss:  0.004959809128195047\n",
            "Loss:  0.00495600700378418\n",
            "Loss:  0.0049522086046636105\n",
            "Loss:  0.004948410205543041\n",
            "Loss:  0.004944613203406334\n",
            "Loss:  0.004940821323543787\n",
            "Loss:  0.004937027581036091\n",
            "Loss:  0.004933248274028301\n",
            "Loss:  0.004929467104375362\n",
            "Loss:  0.004925685469061136\n",
            "Loss:  0.004921917803585529\n",
            "Loss:  0.004918147809803486\n",
            "Loss:  0.004914370831102133\n",
            "Loss:  0.004910594783723354\n",
            "Loss:  0.004906838294118643\n",
            "Loss:  0.004903082735836506\n",
            "Loss:  0.004899329971522093\n",
            "Loss:  0.004895572084933519\n",
            "Loss:  0.00489182211458683\n",
            "Loss:  0.004888082388788462\n",
            "Loss:  0.004884336143732071\n",
            "Loss:  0.004880603402853012\n",
            "Loss:  0.004876864142715931\n",
            "Loss:  0.0048731351271271706\n",
            "Loss:  0.004869399126619101\n",
            "Loss:  0.004865674767643213\n",
            "Loss:  0.004861955065280199\n",
            "Loss:  0.0048582348972558975\n",
            "Loss:  0.004854513332247734\n",
            "Loss:  0.004850801080465317\n",
            "Loss:  0.0048470948822796345\n",
            "Loss:  0.004843392409384251\n",
            "Loss:  0.004839690867811441\n",
            "Loss:  0.004835986532270908\n",
            "Loss:  0.0048322915099561214\n",
            "Loss:  0.004828594159334898\n",
            "Loss:  0.004824907053261995\n",
            "Loss:  0.004821219947189093\n",
            "Loss:  0.004817533306777477\n",
            "Loss:  0.00481386436149478\n",
            "Loss:  0.004810180980712175\n",
            "Loss:  0.0048065148293972015\n",
            "Loss:  0.004802833776921034\n",
            "Loss:  0.0047991713508963585\n",
            "Loss:  0.004795505665242672\n",
            "Loss:  0.004791844170540571\n",
            "Loss:  0.004788190592080355\n",
            "Loss:  0.004784529097378254\n",
            "Loss:  0.004780876450240612\n",
            "Loss:  0.004777227994054556\n",
            "Loss:  0.0047735897824168205\n",
            "Loss:  0.004769948776811361\n",
            "Loss:  0.004766305908560753\n",
            "Loss:  0.0047626737505197525\n",
            "Loss:  0.004759047646075487\n",
            "Loss:  0.004755426198244095\n",
            "Loss:  0.004751784726977348\n",
            "Loss:  0.004748158622533083\n",
            "Loss:  0.004744546953588724\n",
            "Loss:  0.004740932956337929\n",
            "Loss:  0.004737311974167824\n",
            "Loss:  0.004733705893158913\n",
            "Loss:  0.004730097483843565\n",
            "Loss:  0.004726501181721687\n",
            "Loss:  0.00472289277240634\n",
            "Loss:  0.004719289019703865\n",
            "Loss:  0.00471569923684001\n",
            "Loss:  0.004712107125669718\n",
            "Loss:  0.004708532243967056\n",
            "Loss:  0.004704933613538742\n",
            "Loss:  0.004701355472207069\n",
            "Loss:  0.004697776865214109\n",
            "Loss:  0.0046942029148340225\n",
            "Loss:  0.0046906317584216595\n",
            "Loss:  0.004687064792960882\n",
            "Loss:  0.004683490376919508\n",
            "Loss:  0.004679931793361902\n",
            "Loss:  0.00467637088149786\n",
            "Loss:  0.004672817420214415\n",
            "Loss:  0.004669266287237406\n",
            "Loss:  0.00466571282595396\n",
            "Loss:  0.004662164952605963\n",
            "Loss:  0.004658629652112722\n",
            "Loss:  0.004655079450458288\n",
            "Loss:  0.004651540890336037\n",
            "Loss:  0.004648012109100819\n",
            "Loss:  0.0046444786712527275\n",
            "Loss:  0.004640957806259394\n",
            "Loss:  0.004637423902750015\n",
            "Loss:  0.004633903969079256\n",
            "Loss:  0.004630381241440773\n",
            "Loss:  0.004626869689673185\n",
            "Loss:  0.004623355343937874\n",
            "Loss:  0.004619847517460585\n",
            "Loss:  0.004616329446434975\n",
            "Loss:  0.004612833261489868\n",
            "Loss:  0.004609330091625452\n",
            "Loss:  0.004605831578373909\n",
            "Loss:  0.004602339118719101\n",
            "Loss:  0.0045988489873707294\n",
            "Loss:  0.004595356993377209\n",
            "Loss:  0.00459187850356102\n",
            "Loss:  0.004588395357131958\n",
            "Loss:  0.004584908951073885\n",
            "Loss:  0.0045814369805157185\n",
            "Loss:  0.004577961750328541\n",
            "Loss:  0.004574486520141363\n",
            "Loss:  0.0045710220001637936\n",
            "Loss:  0.004567566327750683\n",
            "Loss:  0.0045641036704182625\n",
            "Loss:  0.004560636356472969\n",
            "Loss:  0.004557181149721146\n",
            "Loss:  0.0045537278056144714\n",
            "Loss:  0.004550282843410969\n",
            "Loss:  0.004546837415546179\n",
            "Loss:  0.004543392453342676\n",
            "Loss:  0.004539951216429472\n",
            "Loss:  0.00453651137650013\n",
            "Loss:  0.00453307805582881\n",
            "Loss:  0.0045296489261090755\n",
            "Loss:  0.004526226315647364\n",
            "Loss:  0.0045227957889437675\n",
            "Loss:  0.004519370384514332\n",
            "Loss:  0.004515958018600941\n",
            "Loss:  0.0045125349424779415\n",
            "Loss:  0.004509123507887125\n",
            "Loss:  0.004505713935941458\n",
            "Loss:  0.004502313211560249\n",
            "Loss:  0.004498897586017847\n",
            "Loss:  0.004495506174862385\n",
            "Loss:  0.004492098465561867\n",
            "Loss:  0.004488701466470957\n",
            "Loss:  0.0044853161089122295\n",
            "Loss:  0.0044819265604019165\n",
            "Loss:  0.004478537943214178\n",
            "Loss:  0.004475151188671589\n",
            "Loss:  0.004471776075661182\n",
            "Loss:  0.004468394443392754\n",
            "Loss:  0.004465021658688784\n",
            "Loss:  0.004461645148694515\n",
            "Loss:  0.004458271432667971\n",
            "Loss:  0.0044549135491251945\n",
            "Loss:  0.004451553337275982\n",
            "Loss:  0.004448193125426769\n",
            "Loss:  0.004444829188287258\n",
            "Loss:  0.0044414750300347805\n",
            "Loss:  0.004438123665750027\n",
            "Loss:  0.004434773698449135\n",
            "Loss:  0.00443143118172884\n",
            "Loss:  0.004428090061992407\n",
            "Loss:  0.004424750804901123\n",
            "Loss:  0.004421411082148552\n",
            "Loss:  0.004418076481670141\n",
            "Loss:  0.004414752125740051\n",
            "Loss:  0.004411426838487387\n",
            "Loss:  0.004408096428960562\n",
            "Loss:  0.004404774401336908\n",
            "Loss:  0.004401450045406818\n",
            "Loss:  0.0043981303460896015\n",
            "Loss:  0.004394816234707832\n",
            "Loss:  0.004391506314277649\n",
            "Loss:  0.004388196859508753\n",
            "Loss:  0.004384889733046293\n",
            "Loss:  0.004381589125841856\n",
            "Loss:  0.004378298297524452\n",
            "Loss:  0.004374994896352291\n",
            "Loss:  0.0043717012740671635\n",
            "Loss:  0.004368423018604517\n",
            "Loss:  0.004365122877061367\n",
            "Loss:  0.004361844155937433\n",
            "Loss:  0.0043585579842329025\n",
            "Loss:  0.00435527553781867\n",
            "Loss:  0.004352001938968897\n",
            "Loss:  0.0043487255461514\n",
            "Loss:  0.004345455672591925\n",
            "Loss:  0.004342184867709875\n",
            "Loss:  0.00433892197906971\n",
            "Loss:  0.004335657227784395\n",
            "Loss:  0.004332407843321562\n",
            "Loss:  0.004329149145632982\n",
            "Loss:  0.00432590302079916\n",
            "Loss:  0.00432264618575573\n",
            "Loss:  0.004319395869970322\n",
            "Loss:  0.004316149279475212\n",
            "Loss:  0.004312900826334953\n",
            "Loss:  0.004309663083404303\n",
            "Loss:  0.004306432791054249\n",
            "Loss:  0.0043031941168010235\n",
            "Loss:  0.004299962893128395\n",
            "Loss:  0.004296735394746065\n",
            "Loss:  0.004293506033718586\n",
            "Loss:  0.004290285985916853\n",
            "Loss:  0.004287072457373142\n",
            "Loss:  0.004283851478248835\n",
            "Loss:  0.0042806402780115604\n",
            "Loss:  0.00427742674946785\n",
            "Loss:  0.0042742155492305756\n",
            "Loss:  0.004271000623703003\n",
            "Loss:  0.004267808981239796\n",
            "Loss:  0.004264610819518566\n",
            "Loss:  0.0042614103294909\n",
            "Loss:  0.00425820192322135\n",
            "Loss:  0.004255021922290325\n",
            "Loss:  0.004251836333423853\n",
            "Loss:  0.004248648416250944\n",
            "Loss:  0.00424546143040061\n",
            "Loss:  0.004242277238518\n",
            "Loss:  0.004239098634570837\n",
            "Loss:  0.004235923755913973\n",
            "Loss:  0.0042327530682086945\n",
            "Loss:  0.004229583777487278\n",
            "Loss:  0.00422641821205616\n",
            "Loss:  0.004223254043608904\n",
            "Loss:  0.004220091737806797\n",
            "Loss:  0.004216935019940138\n",
            "Loss:  0.004213773645460606\n",
            "Loss:  0.004210622049868107\n",
            "Loss:  0.00420746672898531\n",
            "Loss:  0.004204324446618557\n",
            "Loss:  0.00420117424800992\n",
            "Loss:  0.004198032431304455\n",
            "Loss:  0.004194893408566713\n",
            "Loss:  0.004191751591861248\n",
            "Loss:  0.004188628867268562\n",
            "Loss:  0.004185493104159832\n",
            "Loss:  0.004182364791631699\n",
            "Loss:  0.004179240670055151\n",
            "Loss:  0.004176115617156029\n",
            "Loss:  0.004172987770289183\n",
            "Loss:  0.004169870633631945\n",
            "Loss:  0.0041667562909424305\n",
            "Loss:  0.004163643810898066\n",
            "Loss:  0.004160529002547264\n",
            "Loss:  0.00415743188932538\n",
            "Loss:  0.004154329188168049\n",
            "Loss:  0.004151223227381706\n",
            "Loss:  0.004148121923208237\n",
            "Loss:  0.004145027603954077\n",
            "Loss:  0.004141931887716055\n",
            "Loss:  0.004138844553381205\n",
            "Loss:  0.0041357530280947685\n",
            "Loss:  0.004132668953388929\n",
            "Loss:  0.004129584413021803\n",
            "Loss:  0.004126504063606262\n",
            "Loss:  0.004123425576835871\n",
            "Loss:  0.004120352678000927\n",
            "Loss:  0.004117275588214397\n",
            "Loss:  0.004114210139960051\n",
            "Loss:  0.004111145157366991\n",
            "Loss:  0.004108076449483633\n",
            "Loss:  0.004105011001229286\n",
            "Loss:  0.004101953003555536\n",
            "Loss:  0.004098896868526936\n",
            "Loss:  0.004095848649740219\n",
            "Loss:  0.004092791583389044\n",
            "Loss:  0.004089738707989454\n",
            "Loss:  0.004086697474122047\n",
            "Loss:  0.004083658568561077\n",
            "Loss:  0.00408061221241951\n",
            "Loss:  0.00407757144421339\n",
            "Loss:  0.004074539989233017\n",
            "Loss:  0.0040715038776397705\n",
            "Loss:  0.004068480804562569\n",
            "Loss:  0.004065449815243483\n",
            "Loss:  0.004062419757246971\n",
            "Loss:  0.004059403669089079\n",
            "Loss:  0.004056387580931187\n",
            "Loss:  0.004053367767482996\n",
            "Loss:  0.004050347022712231\n",
            "Loss:  0.004047330468893051\n",
            "Loss:  0.004044332075864077\n",
            "Loss:  0.004041317850351334\n",
            "Loss:  0.004038315266370773\n",
            "Loss:  0.004035313613712788\n",
            "Loss:  0.004032312426716089\n",
            "Loss:  0.004029320552945137\n",
            "Loss:  0.004026328679174185\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "N=neural_network(x,weights,bias)\n",
        "error(N,y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bh9akjSXvNdb",
        "outputId": "51e1249a-c7be-46bd-c694-d2256da30b04"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0040, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(x.data.numpy(), N.data.numpy(), label='NN')\n",
        "plt.plot(x,y, label='analytical')\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "xp-hq_6ov-DW",
        "outputId": "e6ce9117-bc9e-4116-b1ba-472f8610d75c"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f8dca4bcf10>"
            ]
          },
          "metadata": {},
          "execution_count": 66
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3gUVRfH8e/ZdEgCIYTeQu81dEUUUEQRBJGmIIoodrGBvopiw94bKoJYUFAEBamCID1I7x1CCSEJISE9ue8fu2CAhJZNZndzPs+zD7szd3Z/E2BPZu7MvWKMQSmlVNFlszqAUkopa2khUEqpIk4LgVJKFXFaCJRSqojTQqCUUkWct9UBrkTp0qVNtWrVrI6hlFJuZc2aNceNMWHnLnfLQlCtWjUiIyOtjqGUUm5FRPbntlxPDSmlVBGnhUAppYo4LQRKKVXEuWUfQW4yMjKIiooiNTXV6ihuz9/fn0qVKuHj42N1FKVUIfCYQhAVFUVQUBDVqlVDRKyO47aMMcTGxhIVFUV4eLjVcZRShcApp4ZEZLyIHBORTXmsFxH5UER2icgGEWmeY91gEdnpeAy+0gypqamEhoZqEcgnESE0NFSPrJQqQpzVRzAB6HqB9TcCtRyPYcBnACJSChgNtAZaAaNFJORKQ2gRcA79OSpVtDjl1JAxZrGIVLtAkx7At8Y+5vUKESkpIuWBjsA8Y0wcgIjMw15QfnRGLqWUi8rOguM7ISEKTh6CxKNgskC8wOYFgWUhpCqUrAIlqoBNr2spSIXVR1AROJjjdZRjWV7LzyMiw7AfTVClSpWCSZlPIsKIESN45513AHj77bdJSkrixRdf5MUXX+TNN99k3759lClTBoDAwECSkpKsjKxU4TAGjm6AbTPhwAo4tAbSL/HffkAIVG0PVdtB7a4QWqNgsxZBbtNZbIwZB4wDiIiIcMnZdPz8/Pj1118ZNWoUpUuXPm996dKleeedd3jjjTcsSKeUBRKi4N9vYdMvELsLxAZlG0CTflCpJYSEQ3AFCCoHNm8w2ZCVAYlH4MR+iNsLUZGw/x/Y9gfMeRYqRti3b9gbipWyeg89QmEVgkNA5RyvKzmWHcJ+eijn8kWFlMnpvL29GTZsGO+99x6vvvrqeevvvvtuJkyYwDPPPEOpUvoPWHmw6M2w9EPYNNV+Gij8amj7ENS7BYqH5r3d6VNDpcLtj+odIWKIfd2JA7B5Gqz/CWY9CfNG29e1fQiCyxfGXnmswioEM4CHRGQy9o7hBGPMERGZA7yWo4P4emBUfj/spd83s+Xwyfy+zVnqVwhmdPcGF2334IMP0rhxY55++unz1gUGBnL33XfzwQcf8NJLLzk1n1Iu4cQB+xf05l/Bpzi0vBfaPmA/138FNh1KYPvRRI6eTCUmMY0GFXrRY9jD+MZshGUfw4pPYdU4aDEErh1lP42kLptTCoGI/Ij9N/vSIhKF/UogHwBjzOfALKAbsAtIBoY41sWJyMvAasdbjTndceyugoODGTRoEB9++CEBAQHnrX/kkUdo2rQpTz75pAXplCog6adgybuw7CP76Z9rnoHW91/xqZtdx5IY++c25m+NPrMswMeLlIws3pu3g6FXV6d/988JuHYU/PM+rP7SfvTR6QVodqf9qEJdMmddNdT/IusN8GAe68YD452R47RL+c29ID322GM0b96cIUOGnLeuZMmSDBgwgE8++cSCZEoVgP3L4bfhEL8XGt0OnV+EErle83FRCSkZvDN3O9+vPECAjxdP3VCHbo3KUy7YH38fG3/viOHTRbsZ88cWJq8+wIQhrahwy4fQ6l6Y9TT8/ij8Owl6jdNO5cug12QVgFKlSnH77bfz9ddf57p+xIgRfPHFF2RmZhZyMqWcKCMV5j4P39xo7+S9axb0/vKKi8CczUfp8u7ffLdiPwNaVWHRUx158NqahJcuToCvFyJCxzpl+Pm+tkwY0pIjJ1Lp/dkydkQnQrlGMGQW3DrO3in9+VWwZoL9aiV1UVoICsgTTzzB8ePHc11XunRpbr31VtLS0go5lVJOcuIgjL8Bln0ILQbD8KVQrT0AiakZ/LUtmkXbj7Fs93E2RiWQlpmV69sYY4jcF8f9k9Zw36Q1lCruy28Ptuflng0pHeiX58d3rFOGn+5rS1a24bbPlrFqbxyIQJO+MHyZ/Yqk3x+Fn+6AVOf2F3oiMW5YMSMiIsy5E9Ns3bqVevXqWZTI8+jPU+Vpz98wdYj9Ms9bP4e6NwGQlpnF9ysO8PHCXcSdSj9rE19vG40rlqBxpZIE+Xvj620jJT2LPzYcZl9sMsV8vXjw2poM61AdH69L//00Kj6ZweNXcfhEKt8MaUmb6o4rkrKzYcUn9o7r0JrQ7wcoXdNpPwJ3JSJrjDER5y53m/sIlFIuYOUXMHskhNaCft9D6VoArNobx4if1xEVn0L7mqHcf00Nivl6k56ZTXxyOmsPxBO5P57vVu4nPTP7zNu1qV6Kh66rxY0Ny1Hc7/K/jiqFFGPysLb0/3IFQ75ZzYQhLWldPdR+J3K7h6F8U5gyGL68Dm4bD7U6O+1H4Um0ECilLs4YmD8aln4AdW6CXl+AXxAAu2OSGDpxNaWK+zLpnlZcXeu8KXHp1ui/6/yzsw0Z2dkYA/4++b+6JyzIjx/ubU3/cSsYMmE1E4a0olW442ql8Kvh3oUweSD80Adu+Qia3ZHvz/Q02keglLqwrAz7VUFLP4CIe6DvpDNF4ERyOkMnRuLtZWPSPa1zLQLnstkEP28vpxSB08oE+fPjvW0oV8KfO79eyexNR/5bGVIV7p5tvzlt+oP2/VBn0UKglMpbZpr9t+n1P8J1/4Ob3jlzjX56ZjbDv/uXQ/EpjLuzBZVLFbM0aplgf6bc15b6FYIZ/v2/fLl4D2f6QP0Cof9P0KAXzHvBfrWTG/aPFhQtBEqp3GWm2a+62TkHbnoXOjxlvzLH4c3Z21i+J5Y3bmtERDXXGDIlNNCPH+9tw40Ny/HqrK08O20jyemOy7S9faH3V9ByqP1qpznPaTFw0D4CpdT5MtPgpzth51y4+f3/xvtx2BB1gvFL9zKgdRVubVbJopC58/fx4uP+zXk7dDufLtrN0l2xvNG7MW1rhNqPZrq9bR/TaMUn4OUNnV86q8AVRXpE4GImTJjAQw89dNE2hw8fPvN66NChbNmy5bI/a9GiRdx8882XvZ3ycFmZMOUu+5HAze+dVwQys7J5dtpGQgP9eKZrXWsyXoTNJjzdtS4/DWuDCPT/cgXPTdvIieR0+5f+jW9AxN32/oK/XrE6ruW0ELihcwvBV199Rf369S1MpDyGMfYbsbbPsv/mHHH3eU0mLt/PpkMnGd29PiUCfCwIeelaVw9l9qMduLt9OD+uOsC1by9i0or9ZBmg2zvQfBAseds+gF0RpoXAiXr27EmLFi1o0KAB48aNA+wjjj733HM0adKENm3aEB1tH0Tr999/p3Xr1jRr1ozOnTufWX5aYmIi4eHhZGRkAHDy5EnCw8OZMmUKkZGRDBw4kKZNm5KSkkLHjh05fYPd7Nmzad68OU2aNKFTp04ArFq1irZt29KsWTPatWvH9u3bC+tHotzNgpdg3XdwzUj7+D3nOHwihXfmbqdjnTBuauQeQz8H+HrxQvf6zHzkamqXDeL53zbR/aN/2Hw00X7aq34PmPscbJxqdVTLeGYfwZ8j4ehG575nuUZw49gLNhk/fjylSpUiJSWFli1b0rt3b06dOkWbNm149dVXefrpp/nyyy/53//+x1VXXcWKFSsQEb766ivefPPNMzObAQQFBdGxY0dmzpxJz549mTx5Mr169aJPnz588sknvP3220REnH2DYExMDPfeey+LFy8mPDycuDj7QK5169ZlyZIleHt7M3/+fJ599ll++eUX5/58lPtb8Rn88559SOeOI3NtMnrGZrKN4eUeDd1ubut65YOZPKwNszYe5cXfN9Pj46U80qkWw3t8js+p4zDtfigeBtWvsTpqofPMQmCRDz/8kGnTpgFw8OBBdu7cia+v75nz8C1atGDevHkAREVF0bdvX44cOUJ6ejrh4eHnvd/QoUN588036dmzJ9988w1ffvnlBT9/xYoVdOjQ4cx7nZ78JiEhgcGDB7Nz505E5MxRhlJnbP8TZo+Cet3tl4jm8iU/e9NR5m2JZuSNdS2/VPRKiQg3NS5PuxqhjJ6xmXfn7WD+1mg+v208FX691X6p7D1zoWzROtXqmYXgIr+5F4RFixYxf/58li9fTrFixejYsSOpqan4+Pic+c3Jy8vrzIijDz/8MCNGjOCWW25h0aJFvPjii+e9Z/v27dm3bx+LFi0iKyuLhg0bXlG2559/nmuvvZZp06axb98+OnbseKW7qTzR0U3wy1Co0NQ+emcuY/knpmbw4ozN1C0XxD1Xnf9Li7sJKe7Lh/2b0bVhOZ6ZuoFbvtrEhN5f03DmrfBjP/vdyBeaSc3DaB+BkyQkJBASEkKxYsXYtm0bK1asuGj7ihXtw/VOnDgxz3aDBg1iwIABZ81tEBQURGJi4nlt27Rpw+LFi9m7dy/AmVNDOT9rwoQJl7VfysMlHbN/8fkFQb8fwTf33/TfmbuD6MRUxvZufFmDwrm6bo3K8+sD7QjwtdH7+/38E/EBJB6FnwdBZvrF38BDOOVvVES6ish2EdklIuedXBSR90RkneOxQ0RO5FiXlWPdDGfksULXrl3JzMykXr16jBw5kjZt2lyw/YsvvkifPn1o0aJFrhPdnzZw4EDi4+Pp3/+/uX/uuusu7r///jOdxaeFhYUxbtw4evXqRZMmTejbty8ATz/9NKNGjaJZs2Y6B4L6T2a6/YaxU8fto3PmMe/v+oMnmLh8H4PaVKVp5ZKFm7EQ1CobxG8PtKdRxRLcMTuLJfVHw/5/4M+ni8wNZ/kehlpEvIAdQBcgCvu0k/2NMble2C4iDwPNjDF3O14nGWMCL+czi9Iw1FOnTmX69OlMmjSpUD/XU3+eKodZT9nn+73tG2jYK9cmiakZ9PxkKUlpmcwfcQ1B/q59uWh+pGVm8eD3a5m/NZrpdebSZP8E6P4BtLjL6mhOk9cw1M44ImgF7DLG7DHGpAOTgR4XaN8f+NEJn+vxHn74YUaOHMnzzz9vdRTlaTZMsReBtg/lWQSysw2PTV7H/thk3u/bzKOLAICftxefDmzOjQ3Lcev2zhwIaWOf/vLwOqujFThnFIKKwMEcr6Mcy84jIlWBcOCvHIv9RSRSRFaISM+8PkREhjnaRcbExDghtuv76KOP2LVrF7Vr17Y6ivIk0Vvg90egSlv7/MJ5eHfeDhZsO8YL3evbh2coAny9bXzUvxk3NalEjyN3keRd0t5fkBJvdbQCVdi9Pv2AqcaYnPPWVXUcqgwA3heRXGecNsaMM8ZEGGMiwsJyH+rWHWdbc0X6c/RgaYnw8532zuE+E8Ar99/yf19/mI8X7qJvRGXubFO1cDNazNvLxvt9m9K+cR3uPPkA2QmHYNpw+6xnHsoZheAQUDnH60qOZbnpxzmnhYwxhxx/7gEWAc2uJIS/vz+xsbH6JZZPxhhiY2Px9/e3OooqCLOehrg99tm6gsqdtzo72/DB/J08MnktLaqGMKZnA7e7ccwZvGzCe32bUrreVYxJHwA7/rQPUuehnHEfwWqgloiEYy8A/bD/dn8WEakLhADLcywLAZKNMWkiUhpoD7x5JSEqVapEVFQUReW0UUHy9/enUiXXGlFSOcGGn2H9D3DNM1DtqvNWxyal8dhP61iy8zi3NqvIq7c2xM/beZPHuBsfLxsfD2jGvROzmL1vK9fPewlbtavt91t4mHwXAmNMpog8BMwBvIDxxpjNIjIGiDTGnL4ktB8w2Zz9K3s94AsRycZ+dDI2r6uNLsbHxyfXu3OVUtiPAv543N4v0OHps1YdTUhl4vJ9/LDyACkZWbx2ayP6t6pcJI8EzuXn7cW4QREM//JJmkYPp8Tkuwh48B/7RDceJN+Xj1oht8tHlVJ5yMqAr6+HuN1w/1IoWRljDGv2x/Pt8v3M2niEbGO4vn45HulUi/oVgq1O7HJOJKfz8kef81bKCyTW7UuJfl9YHemK5HX5qGcOMaGU+s/it+Dwv3D7t2QFV2Lq6gN8s3Qf244mEuTnzaC21RjSvprbjh9UGEoW8+Wxe4cy8eO1DNk2mRORnSkZ0cfqWE6jhUApTxa1Bha/DU36szusE09/sZw1++OpXz6Ysb0acUvTChTz1a+BS1G5VDFaDnmLjV+to+rMEWTV7oBXcFmrYzmF5wwaopQ6W3oyTLsPE1SeicHD6fbBEnYdS+Ld25sw85Gr6NeqihaBy9SwcmkOdXwPv+wU9k8Y6jFDUGghUMpTLXgJYnfyU8WRjJ4XxTW1w5g3ogO9mlfSjuB8uKHjNfxZZijV4xazZ/5XVsdxCi0ESnmiff/Ays/ZULEfI9eWYkDrKnxxZwvKBOn9IfklInS++yXW2+pTZukLJBzdY3WkfNNCoJSnSU+G6Q+RWKwyt+++gVuaVHDLGcVcWWCAH363fYGYLI58d7/bnyLSQqCUp1n4KsTv5d4Tg2lXtzLv3N4EL5sWAWerW78xS6s+QN2klRxcNMHqOPmihUApTxK1BrPiU2Z438DhkhF8PKCZR00k42ra9BvJempTcvHzmKRjVse5YvovRClPkZkG0x8k0ac0zyb1YWzvRnpVUAELLuZP1NVv4pudwuEfH7E6zhXTQqCUp1j6AcRs5bFTg+neqg7tauQ9851ynq7XduSngH5UPPQnaZt+tzrOFdFCoJQnOL4Ls/htFvlczebibRjVTWeXKyxeNqHubc+zNbsy6TOesA/17Wa0ECjl7oyBmY+TIb48ldiPl3s0JNjDZxNzNa1qlmNahaconn6MjAWvWR3nsmkhUMrdbfgZ9i7mfQZQpWp1utT3jGEP3E3XbrfwY+Z1eK3+Ao5ssDrOZdFCoJQ7S46DOaOIKdGYz5Ku5tFOtfR+AYs0rxLCoirDiTeBZP/+KGRnXXwjF6GFQCl3tmAMJuUEI5LvolmVUlxdSzuIrTSkUzPGpA/EdvhfiBxvdZxL5pRCICJdRWS7iOwSkZG5rL9LRGJEZJ3jMTTHusEistPxGOyMPEoVCYfWwJoJbKs6gCWJ5Xi0c209GrBY2+qhRFW6mUhbI8xfL8Op41ZHuiT5LgQi4gV8AtwI1Af6i0j9XJr+ZIxp6nh85di2FDAaaA20AkY7pq9USl1IdhbMfAITWIZHjnSlWZWSdNCjAcuJCA9dV4tnUgZh0k7B/BetjnRJnHFE0ArYZYzZY4xJByYDPS5x2xuAecaYOGNMPDAP6OqETEp5tn+/hcNrWVVrBDsTRPsGXEjHOmH4l6/HFO+bYe0kiHL92RSdUQgqAgdzvI5yLDtXbxHZICJTRaTyZW6rlDotOc4+xHTV9rx+sCE1ywRyTe0wq1MpBxFheMcajEnsTqp/GMx8wuU7jgurs/h3oJoxpjH23/onXu4biMgwEYkUkciYmBinB1TKbfz1MqSeZGfEaNZFJTCwdRU9GnAxNzYsT+nQUD72vguOrLMfwbkwZxSCQ0DlHK8rOZadYYyJNcakOV5+BbS41G1zvMc4Y0yEMSYiLEx/+1FF1NGNsGYCtBzK+J0BBPh40at5JatTqXN42YT7OtTg4+NNSSjTChaMgZR4q2PlyRmFYDVQS0TCRcQX6AfMyNlARMrneHkLsNXxfA5wvYiEODqJr3csU0qdyxiYPQr8S3Ky3VP8tvYwtzSpQIkAvYvYFfVuUZEyQf68wRBIPQF/v2l1pDzluxAYYzKBh7B/gW8FfjbGbBaRMSJyi6PZIyKyWUTWA48Adzm2jQNexl5MVgNjHMuUUufaMh32LYHrnmPallOkZGRxR5uqVqdSefDz9uKeq8L54UAJYuv0g1XjIGa71bFyJcYNZ9aJiIgwkZGu3xOvlNNkpMDHrcA/GDPsb67/YCnFfL2Y/tBVVidTF5CYmkH7sX/RqaoX7x29Gyq3hDt+sSyPiKwxxkScu1zvLFbKHSz/GBIOQNexrNqfwM5jSQzUowGXF+Tvw7AO1Zm2PY39jR6CXfNhx1yrY51HC4FSri7xKCx5D+reDOFX83NkFEF+3nRvXMHqZOoS3HNVdcoF+/P43paY0FowZxRkZVgd6yxaCJRydX+9Alnp0GUMp9Iy+XPTEW5qXJ4AXy+rk6lLEODrxVM31OHfQ8msqPk4xO5yuXGItBAo5cqOboS130Hr+yC0BrM3HSU5PYveLfSSUXdya7OKNKwYzBNry5JVrQMsGutSl5NqIVDKVRkDc56FgBDo8BQAv/wbRdXQYkRU1SG53InNJjzXrT6HT6YxNfR+exFY/LbVsc7QQqCUq9oxB/Yuho6jIKAkh06ksHxPLL2aVdI7id1Q2xqh3NCgLM+vsHG81m32y0nj9lgdC9BCoJRrysqEec9DaE2IGALAtH+jMAZ6NdfhuNzVG70bU66EP4P2Xo8RL5cZnVQLgVKuaO23cHwHdH4JvHwwxvDLv4doHV6KyqWKWZ1OXaGSxXwZN6gF+9KDmezXy36T4MHVVsfSQqCUy0lLgoWvQ5W2UPcmAP49cIK9x09pJ7EHqFsumLf7NOHl2E6c9CqFmfu8vT/IQloIlHI1yz6CU8egy8vg6AuYuiaKAB8vujUqf5GNlTvo1qg8wzo3YmzqrcjB5WRsmWlpHi0ESrmSxKP2QlC/p304AiA5PZPf1x+mW6PyBPp5WxxQOctjnWtTtfN97M4uT8y0kSSnplqWRQuBUq5k0Vj7zWOdXjiz6M+NR0lKy+T2CD0t5Gnuu7YOR1qOokLmQSZ98gqpGdZMYKOFQClXcXynfQKTiCEQWuPM4p8iD1IttBitwktZGE4VlKtuHkRsaAt6nfyWV35dhRUDgWohUMpVLBgDPgHQ4ekzi/YeP8WqvXH0iais9w54KhFCe44lTBIoueFrvl95oNAjaCFQyhVERcLWGdDuYQj8bwa+qWsOYhPorbOQebbKrTB1buJB35l89Pty1uwv3GlZtBAoZTVj7DcWFQ+Dtg+eWZyZlc3UNVF0rFOGciX8rcunCoV0Ho0/qTwR8AcPfP9vofYXOKUQiEhXEdkuIrtEZGQu60eIyBYR2SAiC0Skao51WSKyzvGYce62Snm8XQvsM491eBr8gs4sXrLzONEn07STuKgIq4M0Hcht2XPwPhnF7+sPF9pH57sQiIgX8AlwI1Af6C8i9c9pthaIMMY0BqYCOSfvTDHGNHU8bkGpoiQ72340EFINWtx11qrxS/dSOtCX6+qWtSKZskLHUYjNxuig3/h2+f5C6zh2xhFBK2CXMWaPMSYdmAz0yNnAGLPQGJPseLkC0F9xlALY/CtEb4Rr/wfevmcW/3sgniU7j3Pv1dXx9dYzuEVGiYpIq2F0yVhE+uGNrDt4olA+1hn/wioCB3O8jnIsy8s9wJ85XvuLSKSIrBCRnnltJCLDHO0iY2Ji8pdYKVeQlWGfdKZsQ2jY+6xVHy3YSUgxH52cvii66nHwC+IZ36lMWr6/UD6yUH/VEJE7gAjgrRyLqzomUx4AvC8iNXLb1hgzzhgTYYyJCAsLy62JUu5l7SSI32u/ecz233/FDVEnWLg9hqFXV6e43klc9BQrhbR/hOskkqiNf3M8Ka3AP9IZheAQUDnH60qOZWcRkc7Ac8Atxpgze2aMOeT4cw+wCGjmhExKubb0ZFj0BlRuA7WuP2vVhwt2EezvzaC2ejRQZLUeTmZAaR6Xyfy0quDvK3BGIVgN1BKRcBHxBfoBZ139IyLNgC+wF4FjOZaHiIif43lpoD2wxQmZlHJtq8ZB0lHoPPrMwHIAmw8nMH9rNHdfFU6Qv4+FAZWl/ALxvuYp2nptYcfyGWRlF2yncb4LgTEmE3gImANsBX42xmwWkTEicvoqoLeAQGDKOZeJ1gMiRWQ9sBAYa4zRQqA8W2oC/PMe1OwCVdudWXwqLZNnp20iyM+bIe3CLQyoXELEEJIDKnBP2iQ2RRVsp7FTTkAaY2YBs85Z9kKO553z2G4Z0MgZGZRyG8s+htQT0On5M4vSMrO4/7s1bIw6wed3tKBEMT0aKPK8/cjsMJLGcx5hzqopNKkyrOA+qsDeWakizhjDp4t2s/7gCTKyssnMNjQtlcnjWz7BVr8nlG8CQFa2YcTP61my8zhv3taY6xuUszi5chXBrQZyYN6bNNj+MWTfAzavAvkcLQRKFZAJy/bx1pztVC9dnOJ+3tgESv77CcaWzANHbqTErxs4djKNA3HJ7DyWxHPd6nF7ROWLv7EqOry8WV7lPvrue57MdT/h3XxAgXyMFgKlCsDaA/G8NmsrneuVYdydEdhsAgmHMB8uYGeZ7uxJqUDM5mjKBvtTKSSAO9tWZVDbalbHVi6oRIvebNrzNTUXvo53kz7g5fzThloIlHKy+FPpPPj9v5QN9uedPk3tRQBg8VuIyab27a8wO0QvDVWXpnX1MEZk9eGbxLfs955E3O30z9B715VysienrOd4UjqfDmz+X6dv3F77f+IWg0GLgLoMIcV9iQ7rwHbf+vD3m5CR4vTP0EKglBPtjE5kwbZjPNq5Fo0rlfxvxd9vgM0brn7SunDKbbWtWZoxybdhAGJ3O/39tRAo5UQzNx5BBPq0yDGuYsx22PATtLoXgstbF065rbbVQ1maWZeV3f+Ccg2d/v5aCJRyopkbjtCyWinKBOeYSGbR6+BTDNo/Zl0w5dZaVS+FTWDZ/qQCeX8tBEo5yY7oRHYeS+Lmxjl+6z+yATZPgzbDoXhp68Iptxbs70OjiiVYvvt4gby/FgKlnGTmBvtpoa4Nc9wQtvA18C8BbR+yLpjyCG1qhLLu4AlS0p0/haUWAqWcwBjDzI1HaB1eijJBjtNCUZGw40/7hPQBJS/8BkpdRNvqoYQU8+VAXPLFG18mvY9AKSfYEZ3ErmNJDG6XoyPvr1egWCi0Hm5dMOUxOtQKY+WznZAco9U6ix4RKOUEMzccxibQ9fQ4QfuWwp6FjtmmAq0NpzyCzSYFUgRAC4FS+WaM4Y+NR2hTPZSwID8wxn40EFgOIu6xOp5SF6WFQAqI6dgAACAASURBVKl82h+bzJ6YU/91Eu9ZCAeWQYcnwbeYteGUugRaCJTKp9X74gBoUz30v6OB4ErQfJDFyZS6NE4pBCLSVUS2i8guERmZy3o/EfnJsX6liFTLsW6UY/l2EbnBGXmUKkyr98VRIsCHmmGBsGMOHFoD1zwF3n5WR1PqkuS7EIiIF/AJcCNQH+gvIvXPaXYPEG+MqQm8B7zh2LY+9jmOGwBdgU8d76eU24jcF0/LaiHYMLDwFQipBk0HWh1LqUvmjCOCVsAuY8weY0w6MBnocU6bHsBEx/OpQCexd3/3ACYbY9KMMXuBXY73U8otxCSmsef4KVpWKwXbfoejG6HjqAIZM16pguKMQlAROJjjdZRjWa5tHJPdJwChl7gtACIyTEQiRSQyJibGCbGVyr9IR/9ARJUSsPB1KF0bGvWxOJVSl8dtOouNMeOMMRHGmIiwsDCr4ygFwOp98fj72GiSsABitkLHkQU2r6xSBcUZheAQkHOi1UqOZbm2ERFvoAQQe4nbKuWyVu+Lo3mlILyXvAllGkD9W62OpNRlc0YhWA3UEpFwEfHF3vk745w2M4DBjue3AX8ZY4xjeT/HVUXhQC1glRMyKVXgktIy2Xw4gUHFV0LsLrj2WbC5zUG2Umfke6whY0ymiDwEzAG8gPHGmM0iMgaINMbMAL4GJonILiAOe7HA0e5nYAuQCTxojHH+0HpKFYC1B+LxMpl0PDIeyjeFujdZHUmpK+KUQeeMMbOAWecseyHH81Qg1x40Y8yrwKvOyKFUYVq9N46+XovwPxUFPd+HAhoHRqmCpqOPKnWF1u49ynt+06Fia6jZ2eo4Sl0xLQRKXYH0zGzqHppKaVssXPuNHg0ot6Y9W0pdga37jzBMfuN4WGuofo3VcZTKlyJXCLKzjdURlAdIW/Y5YXISr+v+Z3UUpfKtSBWCp6asZ9ikNVbHUO4uNYEGe79hua05IfU6WJ1GqXwrUoUgpLgvf+84RkJKhtVRlBszyz+leHYiy6rcb3UUpZyiSBWCGxuWIyPLsGBrtNVRlLtKjsMs+5g/s1pStm4bq9Mo5RRFqhA03fUpbxWbxKyNR62OotzV0g+QjFO8m9mHFlVDrE6jlFMUqUIgaYn0zp7LgZ3rSUzV00PqMiUdg1XjWFeyC0d9q1G7bJDViZRyiiJVCLh6BMbbl4dkCn9tO2Z1GuVulrwDmWl8mNmbZlVD8LLpvQPKMxStQhBYBlubB7jFazkbIpdanUa5kxMHIHI86Y0HsCg2iAg9LaQ8SNEqBIC0f5gUr0DaH/icU2mZVsdR7uLvNwAhsuq9GIP2DyiPUuQKAQEhxDS6j+tsa1i3fJ7VaZQ7OL4T1v0ILe9hWYw/XjahaeWSVqdSymmKXiEAKnZ9nDiCKb3yDaujKHew8DXw9oerRhC5P4565YMo7qfDdCnPUSQLgZd/ECsr3kWdlLWc2KRHBeoCjm6Ezb9Cm+FkBISy7uAJIqqWsjqVUk5VJAsBQN2bH+WQCSVl9mgwOv6QysOCMeBfEto9zMZDCaRmZGv/gPI4+SoEIlJKROaJyE7Hn+f9DxGRpiKyXEQ2i8gGEembY90EEdkrIuscj6b5yXM5wsuXZn7YEMonbSZt0/TC+ljlTvYvg51z4arHIKAkC7ZG42UTrqpZ2upkSjlVfo8IRgILjDG1gAWO1+dKBgYZYxoAXYH3RSRnT9tTxpimjse6fOa5LA263ceu7AqkzH4JsnWGTJWDMTD/JQgsB63uA2DelmhaVgshpLivxeGUcq78FoIewETH84lAz3MbGGN2GGN2Op4fBo4BYfn8XKdoER7GLyXvouSpPWSv+9HqOMqV7JwLB1fANU+DbzH2HT/Fjugkrq9fzupkSjldfgtBWWPMEcfzo0DZCzUWkVaAL7A7x+JXHaeM3hMRvwtsO0xEIkUkMiYmJp+xz7wnjTvfyfrs6qTNfxUy05zyvsrNZWfb+wZCwqH5IMB+NADQpf4F/4kr5ZYuWghEZL6IbMrl0SNnO2OMAfLsdRWR8sAkYIgxJtuxeBRQF2gJlAKeyWt7Y8w4Y0yEMSYiLMx5BxTXNyzPxGKDCUg+jFn9ldPeV7mxTb9A9Ca47n/g5QPA3C1HqVc+mMqlilkcTinnu2ghMMZ0NsY0zOUxHYh2fMGf/qLPdQAfEQkGZgLPGWNW5HjvI8YuDfgGaOWMnbocXjahTefeLMlqSNrCtyD1ZGFHUK4kMx3+ehnKNoIGvQA4npTGmv3xejSgPFZ+Tw3NAAY7ng8Gzrv8RkR8gWnAt8aYqeesO11EBHv/wqZ85rkitzWvxB9l78M/PZ5Ti96zIoIqJCnpWRw6kZJ3gzXfwIn90OVFsNn/e/y19RjZBq7XQqA8VH4LwVigi4jsBDo7XiMiESJy+jzL7UAH4K5cLhP9XkQ2AhuB0sAr+cxzRWw24d7bezEruw3eKz+FRJ24xhMlpGRw+xfLueqNv3jkx7XsOpZ4doPUk/YxhapdDTU6nVk8d0s0FUsG0KBCcCEnVqpw5KsQGGNijTGdjDG1HKeQ4hzLI40xQx3PvzPG+OS4RPTMZaLGmOuMMY0cp5ruMMYk5X+XrkzNMoEca/kUtuwMDs14yaoYqoCcTM1g0PhVbD+aSN+IyszfGk2X9xbz1JT1pGc6uqyWfwzJsdDlJRD7ENPJ6Zks2RlDl/plEdFhp5VnKrJ3Fuem/43X8qdvF8runEzS4W1Wx1FOkpSWyZBvVrP5UAKfDmzO2N6NWfL0tdzTPpwpa6J4aup6sk9Gw7KPoX4PqNjizLbfrdhPWma2nhZSHk0LQQ5+3l5U7TWGNOPNjh+exujQEx5h9PTNrDt4go8HNKOz4ws9NNCP/91cn6duqMP0dYdZM2kkJjMVrnvhzHb/Hojnzdnbub5+WdrWCLUqvlIFTgvBOZrUq8PW8ME0T/qb2XP+sDqOyqeE5Ax+33CYga2r0LVh+fPWP9CxBk80F5od+42N5W4lq1SNM9s9/MNaypXw563bmuhpIeXRtBDkonnf50mwhVB62StsijphdRyVDzPWHyI9M5vbIyrnul5EeCj7ezJtfty9txNXv/EX787dzoif13EsMZWPBzSnRDGfQk6tVOHSQpALW0Aw3p2epaVtG99P+oIkncnMbf0cGUW98sF5X/Gzfzmy7Q98r3mcMQOvpVbZID5auIsF244x6sZ6OgGNKhJ0do08FG9zNykrP+OeExMYO/N6XunVzOpI6jJtPXKSjYcSGN29fu6ndoyBec9DYDls7R6im29xujUqz6ETKWw9fJJO9coUfmilLKBHBHnx8iag2yvUtB2GNRNZtD3Xm6aVC5sSGYWvl42eTSvm3mDLdIhaDdc+C77FzyyuWDKAznq5qCpCtBBcSJ1uZFdpz5O+vzBm6nISkjOsTqQuUXpmNr+tO0Tn+mVyHzY6Mw3mj4awetB0YOEHVMqFaCG4EBFsXV+lpDlJ39SpvPj7ZqsTqUv017Zo4k6l0yePTmJWjYP4fXDDq+ClZ0hV0aaF4GIqNIMm/bnH+09Wr1vHkp3OGQJbFaypaw5RNtiPDrVyGan2VCz8/RbU7AI1O52/XqkiRgvBpbjueby8vHmx2BRem7WNrGy90cyVpWZk8c+uGLo2KIeXLZfz/H+PhfQkuN6Soa2UcjlaCC5FiYpIu4fpnPUPAUcjmbb2kNWJ1AWs3BtHakY2HevmctVPzHZY/TVEDIEydQs/nFIuSAvBpWr/KCaoPG8U/55352wlNUPnOHZVC7cdw8/bRtvquQwLMec5+xVCHUcVfjClXJQWgkvlF4h0folamTtpmzSf8Uv3Wp1I5WHR9mO0qxGKv4/X2St2zIVd8+CaZ6B4aWvCKeWCtBBcjkZ9oFJLng/4mYkLNxF3Kt3qROoce4+fYl9sMh3rnHNaKDMd5oyC0FrQapg14ZRyUfkqBCJSSkTmichOx58hebTLyjEpzYwcy8NFZKWI7BKRnxyzmbkumw26vkHJrDgGZ/3CN3pU4HJO3/h37bmFYOXnELsLur4O3q79z0ypwpbfI4KRwAJjTC1ggeN1blJyTEpzS47lbwDvGWNqAvHAPfnMU/AqtYAm/bnX+0/mL1vByVS9ycyVLNweQ/Ww4lQJzTHJfNIx+PtNqHU91OpiXTilXFR+C0EPYKLj+UTs8w5fEsc8xdcBp+cxvqztLdVpNDYfX57I+obvVuy3Oo1ySEnPYsWeWDrWPudoYP5LkJkCN7xuTTClXFx+C0FZY8wRx/OjQF7TOPmLSKSIrBCR01/2ocAJY8zpoT2jgDwGhXExweXx6jiSzl5r2bF4CinpegWRK1i+5zjpmdlcWzfHTWQHV8G676DNcChd07pwSrmwixYCEZkvIptyefTI2c7Yp/PK606rqsaYCGAA8L6I1LjcoCIyzFFMImNiXODu3jbDSSlRk8czxzN15S6r0yhg4bYYAny8aBVeyr4gOwtmPgFBFexXCimlcnXRQuCYlL5hLo/pQLSIlAdw/JnrEJ3GmEOOP/cAi4BmQCxQUkROD/RSCcjzTi1jzDhjTIQxJiIsLJdhAwqblw8BPd6hqu0YKYve/W8CdGUJYwx/bTtG+5qh+Hk7LhuNHA9HN9jHE/ILsjagUi4sv6eGZgCDHc8HA9PPbSAiISLi53heGmgPbHEcQSwEbrvQ9i6tekeOVb6RQZm/MHfpKqvTFGlbjpzk0IkUupyeZP7UcfjrZQjvAA1utTacUi4uv4VgLNBFRHYCnR2vEZEIEfnK0aYeECki67F/8Y81xmxxrHsGGCEiu7D3GXydzzyFLqz3WxjxotTi58jM1L4Cq8zdHI0IdKrnKATzR0P6Kej2Nui8AkpdUL7G3zXGxALnDd9ojIkEhjqeLwMa5bH9HqBVfjJYTUpWZn+Tx2i3fiwr50yk9U13Wx2pSJq3JZoWVUIoHegH+5bC2u+g/WMQVsfqaEq5PL2z2Alqd3+S3bZwakS+TFZKgtVxipyDcclsOXKS6xuUtU8488djULKKdhArdYm0EDiBzduHIx1ep1R2PPunPmd1nCJn/tZoALrULwdLP4TjO+Cmd8G32EW2VEqBFgKnaduhK7/7dqXq7u/JjvrX6jhFytzN0dQqE0i4HIXFb0GDXnoHsVKXQQuBk3jZBK8uLxBrgkmcMhyydOiJwnAiOZ1V++LoUq+M/ZSQt599PCGl1CXTQuBEXVvU5fPiD1AiYRvpi9+zOk6R8Ne2Y2RlG/r5LIK9i6HLSxBUzupYSrkVLQRO5O1l46a+w5iZ1Qrb4jchZofVkTzevC3RNAxKovLq16Da1dD8LqsjKeV2tBA4WYuqIaxr+CxJ2b4k//IAZOsdxwXlWGIqC7ZF83bARCQrA2750D5UuFLqsuj/mgLwYPf2vGsbQrGjq8leNc7qOB7rm6X76GqWUvfkUrjuf1CqutWRlHJLWggKQMlivjTqdh+LspqQNXc0HNdB6ZztZGoGfy5fz2t+30LFFvbRRZVSV0QLQQG5LaIyc2s+R3KWjZhJQ+wjYSqn+WHFfv6X/RnFJA16fgY2r4tvpJTKlRaCAiIivHRHF34q8xhhCRtY/9NLVkfyGKkZWcQs+YrOXmuxdXlJh5FQKp+0EBQgHy8bg4c9weqAq6m37WMm//4naTowXb7N+Wclj2d+Q0LZNtDqPqvjKOX2tBAUMD8fbxrd9zWp3sE0Xf0U3d+dx/wt0dhH4VaX63BcElUWP4HYbAT3/1KvElLKCfR/USHwL1mW4H5fUtd2kAcyvmXot5EM/Golaw/EWx3NrWw5fJIZnzxJM7OFY+3HICWrWB1JKY+ghaCw1OoCbR6gZ8ZMvmoTw/ajidz66TKGfRvJnpgkq9O5LGMM8afSmbv5KK9/8Q1Ds34ioWZPwjsNtTqaUh5D3PEURUREhImMjLQ6xuXLTIMvO8HJQ5y6ZzHj16cybvEeAD6/swXta5a2OKC1Nh1K4Msle4g+mUpCSiYnUzKISUwjPSubYJKYF/AcocHF8B7+D/gHWx1XKbcjImsc88efJV9HBCJSSkTmichOx58hubS5VkTW5XikikhPx7oJIrI3x7qm+cnj8rz94LavISOF4n8M5+GO4cx+vAPlS/ozePwqflkTZXVCS8QmpTHq1w10//gf/t4RQ3Y2VCzpT+vqpRhyVTWev6kec2r8QhmJx7vPN1oElHKyfM1QBowEFhhjxorISMfrs2YDMcYsBJqCvXAAu4C5OZo8ZYyZms8c7iOsDtz0Dkx/ABa9RsVOLzDl/nYM/24NT0xZT+ypNIZ1qGF1ykLz74F47hq/iuT0LIa0C+fRzrUoEeBzdqMVn8OhOdD5JajUwpqgSnmw/PYR9AAmOp5PBHpepP1twJ/GmOR8fq57azYQmt0JS96B7bMpEeDDhCGtuKlReV7/cxvLd8danbBQHIxLZti3kYQU92X2Y1fzQvf65xeBAytg7nNQpxu0e8SaoEp5uPwWgrLGmCOO50eBshdp3w/48Zxlr4rIBhF5T0T88tpQRIaJSKSIRMbExOQjsovo9haUawTThkH8Pny9bbzVpzHhpYvz2E9riTuVbnXCApWYmsHQiZGkZ2bz9eCW1CwTdH6jpGMw5S4oUdlx97Be26BUQbjo/ywRmS8im3J59MjZzth7nfPseRaR8tgnsZ+TY/EooC7QEijFOaeVznn/ccaYCGNMRFhY2MViuz6fALj9W/tP7Kc7IP0UxXy9+bBfM+JPZfD01A0ee69BZlY2D/+4ll0xSXx2Rwtqlgk8v1FWJky9G1JOQN9JEFCy8IMqVURctBAYYzobYxrm8pgORDu+4E9/0R+7wFvdDkwzxpyZussYc8TYpQHfAK3ytztuplR16P0VHN0Evz0AxtCwYgmeubEu87dGM2nFfqsTFohPFu5m0fYYxvRokPeVUrNHwr4lcPN79iMnpVSBye+x9gxgsOP5YGD6Bdr255zTQjmKiGDvX9iUzzzup/b10GUMbPkN/n4TgLvbV6NjnTBen7WNwydSLA7oXBujEvjor530aFqBga2r5t5o1Zew+kt7n0DT/oUbUKkiKL+FYCzQRUR2Ap0drxGRCBH56nQjEakGVAb+Pmf770VkI7ARKA28ks887qndw9CkPyx6DbZMR0R4uUdDso3htVlbrU7nNKkZWYz4eR2hgb6MuaVh7o12/wV/PgO1b4TOLxZmPKWKrHxdPmqMiQU65bI8Ehia4/U+oGIu7a7Lz+d7DBG4+X2I3Q2/DoPAslSu0obhHWvw/vydDGh9nHY13P9ms3fn7WDnsSQmDGlJiWI+5zc4ts3eORxWF3p/qUNLK1VI9DIMV+HjD/1/hOCK8ENfiNnO/dfUoFJIAC/O2ExGlntPeblyTyxfLtnDgNZV6FinzPkNEqLgu17g7fg5+OVyFZFSqkBoIXAlxUvDnb+Cly981xv/lGheuLk+O6KTmLTcfTuOjyWm8vCPa6kWWpznutU7v0FyHEzqBWmJcMevEJJH34FSqkBoIXA1IdVg4BRIiYdJvehS1Ytraofx7rwdHE1ItTrdZcvMyuaRH9dyMjWDz+5oTnG/c85Gpp+CH26H+H32I4FyefQdKKUKjBYCV1ShKfT7AeL3IpNu5ZUbKpCRlc3oGe53UdU783awYk8cr/ZsRN1y54wRlH4Kvr8dDq2xX0Zb7SprQipVxGkhcFXVr4F+38Px7VT+YyBPX1OeOZujmb3pqNXJLtlvaw/x2aLd9G9Vhd4tKp298nQROLAMbh0H9W+xJqRSSguBS6vZGfp+D9GbGbLnMVqXNYyesYnE1IyLb2shYwyfLdrNYz+to1V4KUZ3r392g/RT9g7x00WgcR9rgiqlAC0Erq/29dD3O2zHtjCRF/BOPMwbs7dZnSpPmVnZPDttE2/M3kb3JhX49u5W+PvkuAz0VCxM7A77l8KtX2gRUMoFaCFwB3W6wp3T8E+N4c/Al1m+cjk/rDxgdaqzpGZkMXnVAW54fzE/rjrAg9fW4IO+Tc8uAvH7YPz1EL0Zbp8EjW+3LK9S6j/5nY9AFZZq7eGumQR915vpAWO4b3o8Qf530r1JBcsinUzNYPXeOJbvjmX6+sPEJKbRoEIwn9/Rgq4Ny53d+PBa++mgzFS48zeo2taa0Eqp82ghcCflGyP3zKXYj/34NmYsY6ceJMjveTrWvdjo384Rk5jG6n1xrN4XR+S+eDYfTiDbgK+3jfY1Qhl6dXXa1QjFPnRUDmu/hz8eh8AyMGg6lMnlXgKllGV0zmJ3lJZIxtRh+Oycxa9ZV7Oi3rPc2aEBjSqVOKtZakYWu44lsTsmifhT6SSkZHIqPZNgf29KB/oRFuRH+RIBVAwJOG9CmJT0LHbHJLEjOpE1++NZsSeW3TGnAPD3sdGscggtq4XQpkYozauEnH0K6LTMdJjzrH0AufAOcNs39pvmlFKWyGvOYi0E7io7m5T5r+O/7C32m3I8mj6cjHLN8fexkZ6VTVJqJgfiksk+56/X38dGasb5w1UE+Xnj72v/MjfGEHsqndP/NAL9vO1f+tVDaRVeioYVS+DjdZHupWNb7eMmHd0AbR+yTzPppQegSllJC4Gn2ruE7Gn3QeJRfgkcwB/B/bB5+1LM15saYcWpUy6YWmUDKR3oR7C/N95eNtIyszielE5MYhqHT6QQFZ/M4ROppGX+VyDKBvtRu2wQtcsGUi20ON4X++I/LTsLln8Cf71iHy+o+/tQr3sB7bxS6nJoIfBkKSdg1pOwcQqE1oIbx9rvQShsB1bYJ5Q5vBbq3mwfUTXQA2aTU8pD5FUI9PJRTxBQ0j5Ew8CpYLLhu97wY3/7zGeFIW4vTBkC42+AxGjo/TX0/U6LgFJuQk/aepJaXeydsss/gSXvwPZZULsrXP0EVGppn/fAmQ79C8s+hC3TwcsPrhkJ7R8B3+LO/RylVIHK16khEekDvAjUA1o5JqTJrV1X4APAC/jKGHN6JrNwYDIQCqwB7jTGpF/sc/XU0CVIibdP+bjiU/vzMvWhcV9o1AdKnDdH0KVLPAqbf4NNUyFqNfgFQ8QQaD0cgss7L79SyukKpI9AROoB2cAXwJO5FQIR8QJ2AF2AKGA10N8Ys0VEfgZ+NcZMFpHPgfXGmM8u9rlaCC5DWhJsmAzrf4KoVfZlZRtC1XZQtb39mv4SlcG32PnbZqbBycP2K38OrrL3ARxaAxgo2wia9IXmg8E/+PxtlVIuJ69CkN+pKrc63vxCzVoBu4wxexxtJwM9RGQrcB0wwNFuIvaji4sWAnUZ/AKh5VD7I3Y3bP4V9v0Da7+DVeP+a1e8jP2Ujtjsj5R4SD7+33ovP6jQDK55Bhr2grA6hb8vSqkCURh9BBWBgzleRwGtsZ8OOmGMycyxPM9zFiIyDBgGUKVKlYJJ6ulCa0CHp+yPzHQ4uhHi9sCJfXDiAGSk2Dubs7MgIASCK0BQefscwuUbg7ef1XuglCoAFy0EIjIfKJfLqueMMdOdHyl3xphxwDiwnxoqrM/1WN6+UKmF/aGUKtIuWgiMMfm9IP0QUDnH60qOZbFASRHxdhwVnF6ulFKqEBXGfQSrgVoiEi4ivkA/YIax91IvBG5ztBsMFNoRhlJKKbt8FQIRuVVEooC2wEwRmeNYXkFEZgE4ftt/CJgDbAV+NsZsdrzFM8AIEdmFvc/g6/zkUUopdfl0iAmllCoidIgJpZRSudJCoJRSRZwWAqWUKuK0ECilVBHnlp3FIhID7L/CzUsDxy/ayj14yr54yn6A7our8pR9ye9+VDXGnDc+vFsWgvwQkcjces3dkafsi6fsB+i+uCpP2ZeC2g89NaSUUkWcFgKllCriimIhGHfxJm7DU/bFU/YDdF9clafsS4HsR5HrI1BKKXW2onhEoJRSKgctBEopVcQVyUIgIi+LyAYRWScic0WkgtWZrpSIvCUi2xz7M01ESlqd6UqISB8R2Swi2SLilpf5iUhXEdkuIrtEZKTVea6UiIwXkWMissnqLPkhIpVFZKGIbHH823rU6kxXSkT8RWSViKx37MtLTn3/othHICLBxpiTjuePAPWNMfdbHOuKiMj1wF/GmEwReQPAGPOMxbEum4jUA7KBL4AnjTFuNbysiHgBO4Au2KddXQ30N8ZssTTYFRCRDkAS8K0xpqHVea6UiJQHyhtj/hWRIGAN0NNN/04EKG6MSRIRH+Af4FFjzApnvH+RPCI4XQQcigNuWw2NMXNzzPu8AvtMb27HGLPVGLPd6hz50ArYZYzZY4xJByYDPSzOdEWMMYuBOKtz5Jcx5ogx5l/H80Ts86HkOS+6KzN2SY6XPo6H0763imQhABCRV0XkIDAQeMHqPE5yN/Cn1SGKqIrAwRyvo3DTLx1PJCLVgGbASmuTXDkR8RKRdcAxYJ4xxmn74rGFQETmi8imXB49AIwxzxljKgPfY59BzWVdbF8cbZ4DMrHvj0u6lP1QytlEJBD4BXjsnLMBbsUYk2WMaYr9qL+ViDjttN1FJ693V8aYzpfY9HtgFjC6AOPky8X2RUTuAm4GOhkX7vS5jL8Td3QIqJzjdSXHMmUhx/n0X4DvjTG/Wp3HGYwxJ0RkIdAVcEqHvsceEVyIiNTK8bIHsM2qLPklIl2Bp4FbjDHJVucpwlYDtUQkXER8gX7w/3buEKWCKAzD8PtpEBdw12BzBQabYHMXrsEiCFYRzFcwCYLF4AYsRsFgsRgNgjv4DefKzY4XDpfzPnHC8IU55zvM/AwPnTMNbfGBdQ68VdVF7zz/kWT2OxGYZJs2lLCyfWvUqaF7YIc2pfIBHFfVWp7ekrwDW8DX4tLzOk5AJTkCroAZ8A28VNVB31R/k+QQuAQ2geuqOu8caZIkt8A+7ZfHn8BpVc27hpogyR7wBLzS1jrASVU99ks1TZJd4Ib2bG0Ad1V1trL7j1gEkqSlIV8NSZKWLAJJGpxFIEmDswgkaXAWgSQNHwwPlAAAAA9JREFUziKQpMFZBJI0uB/iFw7kFeeN1QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#predictions\n",
        "x = torch.unsqueeze(torch.linspace(-3, 20, 800), dim=1) \n",
        "y = torch.sin(x)\n",
        "N = neural_network(x, weights,bias)\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(x.data.numpy(),y.data.numpy(), color = \"orange\")\n",
        "ax.plot(x.data.numpy(), N.data.numpy(), 'g-', label ='prediction in T')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "SL-cPkjXv_4f",
        "outputId": "715d16df-bee9-4844-eac7-2f9ce1c4be88"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f8dca5c2890>]"
            ]
          },
          "metadata": {},
          "execution_count": 67
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#second approach , net1 trained out of T"
      ],
      "metadata": {
        "id": "cW2k-olYzQJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net1 = torch.nn.Sequential(\n",
        "        torch.nn.Linear(1, 40),\n",
        "        torch.nn.Sigmoid(),\n",
        "        torch.nn.Linear(40, 1),\n",
        "    )"
      ],
      "metadata": {
        "id": "2vuDbHbuzNnl"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(x):\n",
        "\n",
        "    x.requires_grad = True\n",
        "    N = neural_network(x, weights, bias)\n",
        "    dNx_dx = dN_dx(weights, x)\n",
        "\n",
        "    return  torch.mean( ( dNx_dx - net1(x)*N )  ** 2)"
      ],
      "metadata": {
        "id": "FlsicHAgMFka"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(net1.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "ZZfMJ56ZzbNU"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.unsqueeze(torch.linspace(-3,8, 200), dim=1)  #train net1 in (-4, 4)"
      ],
      "metadata": {
        "id": "dUR59yeQHMMg"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def closure():\n",
        "    \n",
        "    l = loss(x)\n",
        "    #losses.append(l)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    l.backward()\n",
        "    print(f\"loss: {l}\")\n",
        "    return l\n",
        "\n",
        "for i in range(5000):\n",
        "    optimizer.step(closure)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54huTbj3zqWd",
        "outputId": "1f3357a9-2592-41b1-af95-c2401acb3ead"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.0788634791970253\n",
            "loss: 0.0789237916469574\n",
            "loss: 0.07901237905025482\n",
            "loss: 0.0791398212313652\n",
            "loss: 0.07931052893400192\n",
            "loss: 0.07952173054218292\n",
            "loss: 0.07973592728376389\n",
            "loss: 0.079888254404068\n",
            "loss: 0.07987309247255325\n",
            "loss: 0.07964535057544708\n",
            "loss: 0.07925962656736374\n",
            "loss: 0.07890244573354721\n",
            "loss: 0.07874193787574768\n",
            "loss: 0.07881440222263336\n",
            "loss: 0.0790124386548996\n",
            "loss: 0.07917161285877228\n",
            "loss: 0.07917939871549606\n",
            "loss: 0.07903385907411575\n",
            "loss: 0.07884593307971954\n",
            "loss: 0.0787409245967865\n",
            "loss: 0.07876627147197723\n",
            "loss: 0.0788673609495163\n",
            "loss: 0.07894594967365265\n",
            "loss: 0.07893906533718109\n",
            "loss: 0.0788571909070015\n",
            "loss: 0.07876775413751602\n",
            "loss: 0.07873228937387466\n",
            "loss: 0.07876129448413849\n",
            "loss: 0.07881437987089157\n",
            "loss: 0.07884152233600616\n",
            "loss: 0.07882214337587357\n",
            "loss: 0.0787748470902443\n",
            "loss: 0.07873677462339401\n",
            "loss: 0.07873113453388214\n",
            "loss: 0.07875289767980576\n",
            "loss: 0.07877711951732635\n",
            "loss: 0.07878252118825912\n",
            "loss: 0.07876605540513992\n",
            "loss: 0.07874172925949097\n",
            "loss: 0.07872702926397324\n",
            "loss: 0.07872918993234634\n",
            "loss: 0.07874169200658798\n",
            "loss: 0.07875189185142517\n",
            "loss: 0.07875128090381622\n",
            "loss: 0.07874121516942978\n",
            "loss: 0.07872916013002396\n",
            "loss: 0.07872286438941956\n",
            "loss: 0.07872465252876282\n",
            "loss: 0.07873094826936722\n",
            "loss: 0.07873544842004776\n",
            "loss: 0.07873444259166718\n",
            "loss: 0.07872912287712097\n",
            "loss: 0.07872306555509567\n",
            "loss: 0.07871969044208527\n",
            "loss: 0.07872005552053452\n",
            "loss: 0.07872248440980911\n",
            "loss: 0.07872462272644043\n",
            "loss: 0.07872448116540909\n",
            "loss: 0.07872194051742554\n",
            "loss: 0.0787186324596405\n",
            "loss: 0.07871641218662262\n",
            "loss: 0.07871587574481964\n",
            "loss: 0.07871652394533157\n",
            "loss: 0.07871756702661514\n",
            "loss: 0.07871759682893753\n",
            "loss: 0.07871664315462112\n",
            "loss: 0.07871492207050323\n",
            "loss: 0.0787133201956749\n",
            "loss: 0.07871230691671371\n",
            "loss: 0.07871213555335999\n",
            "loss: 0.07871226966381073\n",
            "loss: 0.07871240377426147\n",
            "loss: 0.07871197909116745\n",
            "loss: 0.07871117442846298\n",
            "loss: 0.07871011644601822\n",
            "loss: 0.07870917022228241\n",
            "loss: 0.07870848476886749\n",
            "loss: 0.07870814204216003\n",
            "loss: 0.07870792597532272\n",
            "loss: 0.07870782911777496\n",
            "loss: 0.07870731502771378\n",
            "loss: 0.07870669662952423\n",
            "loss: 0.07870601862668991\n",
            "loss: 0.07870537042617798\n",
            "loss: 0.07870466262102127\n",
            "loss: 0.07870416343212128\n",
            "loss: 0.07870371639728546\n",
            "loss: 0.0787033811211586\n",
            "loss: 0.07870304584503174\n",
            "loss: 0.07870244979858398\n",
            "loss: 0.07870183140039444\n",
            "loss: 0.07870134711265564\n",
            "loss: 0.07870078831911087\n",
            "loss: 0.07870019227266312\n",
            "loss: 0.07869983464479446\n",
            "loss: 0.07869938015937805\n",
            "loss: 0.07869892567396164\n",
            "loss: 0.07869835197925568\n",
            "loss: 0.07869799435138702\n",
            "loss: 0.07869761437177658\n",
            "loss: 0.07869694381952286\n",
            "loss: 0.07869647443294525\n",
            "loss: 0.07869601994752884\n",
            "loss: 0.07869552820920944\n",
            "loss: 0.07869503647089005\n",
            "loss: 0.0786944255232811\n",
            "loss: 0.07869407534599304\n",
            "loss: 0.07869358360767365\n",
            "loss: 0.07869302481412888\n",
            "loss: 0.07869252562522888\n",
            "loss: 0.07869208604097366\n",
            "loss: 0.07869155704975128\n",
            "loss: 0.07869119942188263\n",
            "loss: 0.07869064062833786\n",
            "loss: 0.0786900445818901\n",
            "loss: 0.07868954539299011\n",
            "loss: 0.07868912816047668\n",
            "loss: 0.07868865132331848\n",
            "loss: 0.07868823409080505\n",
            "loss: 0.07868768274784088\n",
            "loss: 0.07868728786706924\n",
            "loss: 0.0786866694688797\n",
            "loss: 0.07868616282939911\n",
            "loss: 0.07868581265211105\n",
            "loss: 0.07868533581495285\n",
            "loss: 0.07868477702140808\n",
            "loss: 0.07868430018424988\n",
            "loss: 0.07868390530347824\n",
            "loss: 0.07868333160877228\n",
            "loss: 0.07868270576000214\n",
            "loss: 0.0786823034286499\n",
            "loss: 0.07868190109729767\n",
            "loss: 0.07868137955665588\n",
            "loss: 0.0786808505654335\n",
            "loss: 0.07868039608001709\n",
            "loss: 0.07867991179227829\n",
            "loss: 0.07867943495512009\n",
            "loss: 0.07867880910634995\n",
            "loss: 0.07867830991744995\n",
            "loss: 0.07867800444364548\n",
            "loss: 0.07867754250764847\n",
            "loss: 0.07867704331874847\n",
            "loss: 0.07867654412984848\n",
            "loss: 0.07867588847875595\n",
            "loss: 0.0786755308508873\n",
            "loss: 0.07867494225502014\n",
            "loss: 0.07867465913295746\n",
            "loss: 0.0786740630865097\n",
            "loss: 0.07867355644702911\n",
            "loss: 0.07867303490638733\n",
            "loss: 0.07867264747619629\n",
            "loss: 0.07867193222045898\n",
            "loss: 0.07867155224084854\n",
            "loss: 0.07867085188627243\n",
            "loss: 0.07867047935724258\n",
            "loss: 0.07867012917995453\n",
            "loss: 0.0786694884300232\n",
            "loss: 0.07866913825273514\n",
            "loss: 0.07866853475570679\n",
            "loss: 0.07866792380809784\n",
            "loss: 0.07866750657558441\n",
            "loss: 0.07866716384887695\n",
            "loss: 0.0786667913198471\n",
            "loss: 0.07866620272397995\n",
            "loss: 0.07866569608449936\n",
            "loss: 0.07866517454385757\n",
            "loss: 0.07866454124450684\n",
            "loss: 0.07866421341896057\n",
            "loss: 0.07866362482309341\n",
            "loss: 0.0786634087562561\n",
            "loss: 0.07866286486387253\n",
            "loss: 0.07866214960813522\n",
            "loss: 0.07866189628839493\n",
            "loss: 0.07866142690181732\n",
            "loss: 0.07866121828556061\n",
            "loss: 0.07866062968969345\n",
            "loss: 0.0786605253815651\n",
            "loss: 0.0786600187420845\n",
            "loss: 0.07866007089614868\n",
            "loss: 0.07866018265485764\n",
            "loss: 0.07866061478853226\n",
            "loss: 0.07866132259368896\n",
            "loss: 0.07866253703832626\n",
            "loss: 0.078664630651474\n",
            "loss: 0.07866807281970978\n",
            "loss: 0.0786736011505127\n",
            "loss: 0.07868240773677826\n",
            "loss: 0.0786956325173378\n",
            "loss: 0.07871611416339874\n",
            "loss: 0.07874772697687149\n",
            "loss: 0.07879531383514404\n",
            "loss: 0.07886684685945511\n",
            "loss: 0.07897040992975235\n",
            "loss: 0.0791163519024849\n",
            "loss: 0.07930438220500946\n",
            "loss: 0.07952254265546799\n",
            "loss: 0.07971641421318054\n",
            "loss: 0.07980844378471375\n",
            "loss: 0.07970073819160461\n",
            "loss: 0.07939262688159943\n",
            "loss: 0.07899744063615799\n",
            "loss: 0.07871192693710327\n",
            "loss: 0.07865579426288605\n",
            "loss: 0.07879742980003357\n",
            "loss: 0.07899563759565353\n",
            "loss: 0.07909785211086273\n",
            "loss: 0.07903619110584259\n",
            "loss: 0.07885918766260147\n",
            "loss: 0.07869521528482437\n",
            "loss: 0.07864466309547424\n",
            "loss: 0.07871021330356598\n",
            "loss: 0.0788121148943901\n",
            "loss: 0.0788595974445343\n",
            "loss: 0.0788186639547348\n",
            "loss: 0.07872633635997772\n",
            "loss: 0.0786537230014801\n",
            "loss: 0.07864515483379364\n",
            "loss: 0.07868830114603043\n",
            "loss: 0.07873588055372238\n",
            "loss: 0.07874563336372375\n",
            "loss: 0.07871334254741669\n",
            "loss: 0.07866624742746353\n",
            "loss: 0.0786387026309967\n",
            "loss: 0.07864450663328171\n",
            "loss: 0.07866965979337692\n",
            "loss: 0.07868869602680206\n",
            "loss: 0.07868620753288269\n",
            "loss: 0.07866573333740234\n",
            "loss: 0.07864321768283844\n",
            "loss: 0.07863417267799377\n",
            "loss: 0.07864057272672653\n",
            "loss: 0.07865311950445175\n",
            "loss: 0.0786602720618248\n",
            "loss: 0.07865636050701141\n",
            "loss: 0.07864490151405334\n",
            "loss: 0.0786346048116684\n",
            "loss: 0.07863088697195053\n",
            "loss: 0.07863418012857437\n",
            "loss: 0.07864010334014893\n",
            "loss: 0.07864303886890411\n",
            "loss: 0.07864084839820862\n",
            "loss: 0.0786350667476654\n",
            "loss: 0.07862987369298935\n",
            "loss: 0.07862747460603714\n",
            "loss: 0.07862862944602966\n",
            "loss: 0.07863123714923859\n",
            "loss: 0.07863263785839081\n",
            "loss: 0.07863201200962067\n",
            "loss: 0.0786292552947998\n",
            "loss: 0.07862604409456253\n",
            "loss: 0.07862427830696106\n",
            "loss: 0.07862389832735062\n",
            "loss: 0.0786249041557312\n",
            "loss: 0.0786256194114685\n",
            "loss: 0.07862541824579239\n",
            "loss: 0.07862431555986404\n",
            "loss: 0.07862266898155212\n",
            "loss: 0.0786210373044014\n",
            "loss: 0.07862032949924469\n",
            "loss: 0.07862009853124619\n",
            "loss: 0.07862024009227753\n",
            "loss: 0.07862038165330887\n",
            "loss: 0.07861989736557007\n",
            "loss: 0.07861898094415665\n",
            "loss: 0.07861790806055069\n",
            "loss: 0.07861705124378204\n",
            "loss: 0.07861649245023727\n",
            "loss: 0.07861613482236862\n",
            "loss: 0.07861598581075668\n",
            "loss: 0.07861589640378952\n",
            "loss: 0.07861524820327759\n",
            "loss: 0.07861476391553879\n",
            "loss: 0.07861390709877014\n",
            "loss: 0.07861320674419403\n",
            "loss: 0.07861264795064926\n",
            "loss: 0.07861214876174927\n",
            "loss: 0.07861176133155823\n",
            "loss: 0.07861144840717316\n",
            "loss: 0.0786110907793045\n",
            "loss: 0.07861055433750153\n",
            "loss: 0.07860994338989258\n",
            "loss: 0.07860938459634781\n",
            "loss: 0.07860883325338364\n",
            "loss: 0.07860830426216125\n",
            "loss: 0.07860805094242096\n",
            "loss: 0.07860732823610306\n",
            "loss: 0.07860707491636276\n",
            "loss: 0.07860647886991501\n",
            "loss: 0.07860613614320755\n",
            "loss: 0.07860557734966278\n",
            "loss: 0.07860495895147324\n",
            "loss: 0.07860453426837921\n",
            "loss: 0.0786038190126419\n",
            "loss: 0.07860343158245087\n",
            "loss: 0.07860308140516281\n",
            "loss: 0.07860258966684341\n",
            "loss: 0.07860208302736282\n",
            "loss: 0.0786016434431076\n",
            "loss: 0.07860114425420761\n",
            "loss: 0.07860066741704941\n",
            "loss: 0.07860010117292404\n",
            "loss: 0.07859964668750763\n",
            "loss: 0.07859926670789719\n",
            "loss: 0.07859879732131958\n",
            "loss: 0.07859806716442108\n",
            "loss: 0.07859786599874496\n",
            "loss: 0.07859716564416885\n",
            "loss: 0.0785968229174614\n",
            "loss: 0.07859626412391663\n",
            "loss: 0.07859580218791962\n",
            "loss: 0.07859531044960022\n",
            "loss: 0.0785948634147644\n",
            "loss: 0.07859428226947784\n",
            "loss: 0.07859394699335098\n",
            "loss: 0.07859328389167786\n",
            "loss: 0.07859277725219727\n",
            "loss: 0.07859235256910324\n",
            "loss: 0.0785917341709137\n",
            "loss: 0.07859135419130325\n",
            "loss: 0.07859080284833908\n",
            "loss: 0.07859040051698685\n",
            "loss: 0.078590027987957\n",
            "loss: 0.07858942449092865\n",
            "loss: 0.07858895510435104\n",
            "loss: 0.07858838140964508\n",
            "loss: 0.07858798652887344\n",
            "loss: 0.07858740538358688\n",
            "loss: 0.07858682423830032\n",
            "loss: 0.07858642935752869\n",
            "loss: 0.07858577370643616\n",
            "loss: 0.07858555763959885\n",
            "loss: 0.07858498394489288\n",
            "loss: 0.07858459651470184\n",
            "loss: 0.07858399301767349\n",
            "loss: 0.0785837173461914\n",
            "loss: 0.07858309894800186\n",
            "loss: 0.07858254760503769\n",
            "loss: 0.07858214527368546\n",
            "loss: 0.0785815641283989\n",
            "loss: 0.07858125120401382\n",
            "loss: 0.07858064025640488\n",
            "loss: 0.07858021557331085\n",
            "loss: 0.07857964932918549\n",
            "loss: 0.07857927680015564\n",
            "loss: 0.07857862859964371\n",
            "loss: 0.07857818901538849\n",
            "loss: 0.0785776898264885\n",
            "loss: 0.07857722043991089\n",
            "loss: 0.07857679575681686\n",
            "loss: 0.07857629656791687\n",
            "loss: 0.0785757526755333\n",
            "loss: 0.07857535034418106\n",
            "loss: 0.07857473939657211\n",
            "loss: 0.07857420295476913\n",
            "loss: 0.0785738155245781\n",
            "loss: 0.0785733237862587\n",
            "loss: 0.0785728394985199\n",
            "loss: 0.07857248932123184\n",
            "loss: 0.07857192307710648\n",
            "loss: 0.07857152074575424\n",
            "loss: 0.07857123762369156\n",
            "loss: 0.07857075333595276\n",
            "loss: 0.07857057452201843\n",
            "loss: 0.07857029139995575\n",
            "loss: 0.07857029139995575\n",
            "loss: 0.07857009768486023\n",
            "loss: 0.07857032865285873\n",
            "loss: 0.07857105135917664\n",
            "loss: 0.0785721093416214\n",
            "loss: 0.07857389003038406\n",
            "loss: 0.07857680320739746\n",
            "loss: 0.0785810723900795\n",
            "loss: 0.07858794927597046\n",
            "loss: 0.07859852910041809\n",
            "loss: 0.07861444354057312\n",
            "loss: 0.07863864302635193\n",
            "loss: 0.07867473363876343\n",
            "loss: 0.07872820645570755\n",
            "loss: 0.07880620658397675\n",
            "loss: 0.07891605794429779\n",
            "loss: 0.07906404137611389\n",
            "loss: 0.07924395799636841\n",
            "loss: 0.0794358178973198\n",
            "loss: 0.07957980036735535\n",
            "loss: 0.0796065479516983\n",
            "loss: 0.07944795489311218\n",
            "loss: 0.0791379064321518\n",
            "loss: 0.0787983164191246\n",
            "loss: 0.07858728617429733\n",
            "loss: 0.07857625931501389\n",
            "loss: 0.0787154957652092\n",
            "loss: 0.07888278365135193\n",
            "loss: 0.07895945757627487\n",
            "loss: 0.0788993239402771\n",
            "loss: 0.0787462443113327\n",
            "loss: 0.0786040797829628\n",
            "loss: 0.07855347543954849\n",
            "loss: 0.07860103249549866\n",
            "loss: 0.07868769019842148\n",
            "loss: 0.07873902469873428\n",
            "loss: 0.07871934771537781\n",
            "loss: 0.07864712178707123\n",
            "loss: 0.07857611030340195\n",
            "loss: 0.07854997366666794\n",
            "loss: 0.07857325673103333\n",
            "loss: 0.07861550152301788\n",
            "loss: 0.07863990217447281\n",
            "loss: 0.07862895727157593\n",
            "loss: 0.07859327644109726\n",
            "loss: 0.07855887711048126\n",
            "loss: 0.0785466656088829\n",
            "loss: 0.07855802774429321\n",
            "loss: 0.07857830822467804\n",
            "loss: 0.07858984172344208\n",
            "loss: 0.0785844624042511\n",
            "loss: 0.07856710255146027\n",
            "loss: 0.07855022698640823\n",
            "loss: 0.0785432904958725\n",
            "loss: 0.07854762673377991\n",
            "loss: 0.07855714112520218\n",
            "loss: 0.07856306433677673\n",
            "loss: 0.07856160402297974\n",
            "loss: 0.07855382561683655\n",
            "loss: 0.07854513078927994\n",
            "loss: 0.07854034751653671\n",
            "loss: 0.07854080200195312\n",
            "loss: 0.0785444900393486\n",
            "loss: 0.078547902405262\n",
            "loss: 0.07854831963777542\n",
            "loss: 0.07854568213224411\n",
            "loss: 0.07854112982749939\n",
            "loss: 0.07853767275810242\n",
            "loss: 0.07853618264198303\n",
            "loss: 0.07853712886571884\n",
            "loss: 0.07853878289461136\n",
            "loss: 0.07853975147008896\n",
            "loss: 0.07853910326957703\n",
            "loss: 0.07853714376688004\n",
            "loss: 0.07853495329618454\n",
            "loss: 0.07853315025568008\n",
            "loss: 0.07853247225284576\n",
            "loss: 0.07853247970342636\n",
            "loss: 0.07853291183710098\n",
            "loss: 0.07853303104639053\n",
            "loss: 0.07853268086910248\n",
            "loss: 0.0785316750407219\n",
            "loss: 0.07853028923273087\n",
            "loss: 0.0785292237997055\n",
            "loss: 0.07852854579687119\n",
            "loss: 0.07852835953235626\n",
            "loss: 0.07852822542190552\n",
            "loss: 0.07852819561958313\n",
            "loss: 0.07852774113416672\n",
            "loss: 0.07852711528539658\n",
            "loss: 0.07852626591920853\n",
            "loss: 0.0785255879163742\n",
            "loss: 0.07852490246295929\n",
            "loss: 0.07852428406476974\n",
            "loss: 0.07852406799793243\n",
            "loss: 0.07852354645729065\n",
            "loss: 0.07852335274219513\n",
            "loss: 0.07852277904748917\n",
            "loss: 0.07852217555046082\n",
            "loss: 0.07852177321910858\n",
            "loss: 0.0785210132598877\n",
            "loss: 0.07852047681808472\n",
            "loss: 0.07851982861757278\n",
            "loss: 0.07851947098970413\n",
            "loss: 0.07851894199848175\n",
            "loss: 0.07851865887641907\n",
            "loss: 0.07851812243461609\n",
            "loss: 0.07851772010326385\n",
            "loss: 0.0785173550248146\n",
            "loss: 0.07851678878068924\n",
            "loss: 0.07851611822843552\n",
            "loss: 0.07851585745811462\n",
            "loss: 0.07851514965295792\n",
            "loss: 0.0785147026181221\n",
            "loss: 0.07851428538560867\n",
            "loss: 0.07851383090019226\n",
            "loss: 0.07851335406303406\n",
            "loss: 0.0785127580165863\n",
            "loss: 0.07851222157478333\n",
            "loss: 0.0785118117928505\n",
            "loss: 0.07851143926382065\n",
            "loss: 0.07851086556911469\n",
            "loss: 0.07851044833660126\n",
            "loss: 0.07850996404886246\n",
            "loss: 0.07850933074951172\n",
            "loss: 0.07850904762744904\n",
            "loss: 0.07850838452577591\n",
            "loss: 0.07850795984268188\n",
            "loss: 0.07850733399391174\n",
            "loss: 0.07850708067417145\n",
            "loss: 0.07850643992424011\n",
            "loss: 0.0785059705376625\n",
            "loss: 0.07850540429353714\n",
            "loss: 0.0785050168633461\n",
            "loss: 0.0785045400261879\n",
            "loss: 0.07850399613380432\n",
            "loss: 0.07850370556116104\n",
            "loss: 0.07850299775600433\n",
            "loss: 0.07850262522697449\n",
            "loss: 0.07850190252065659\n",
            "loss: 0.07850176095962524\n",
            "loss: 0.07850128412246704\n",
            "loss: 0.0785006582736969\n",
            "loss: 0.07850030064582825\n",
            "loss: 0.07849979400634766\n",
            "loss: 0.0784992128610611\n",
            "loss: 0.07849860191345215\n",
            "loss: 0.07849818468093872\n",
            "loss: 0.07849773019552231\n",
            "loss: 0.07849729061126709\n",
            "loss: 0.07849669456481934\n",
            "loss: 0.07849636673927307\n",
            "loss: 0.07849584519863129\n",
            "loss: 0.07849527150392532\n",
            "loss: 0.07849481701850891\n",
            "loss: 0.07849431037902832\n",
            "loss: 0.07849390059709549\n",
            "loss: 0.07849333435297012\n",
            "loss: 0.07849271595478058\n",
            "loss: 0.07849232852458954\n",
            "loss: 0.0784919485449791\n",
            "loss: 0.07849141955375671\n",
            "loss: 0.0784909650683403\n",
            "loss: 0.07849045097827911\n",
            "loss: 0.0784899964928627\n",
            "loss: 0.07848944514989853\n",
            "loss: 0.07848896831274033\n",
            "loss: 0.07848849147558212\n",
            "loss: 0.0784880667924881\n",
            "loss: 0.0784875825047493\n",
            "loss: 0.07848699390888214\n",
            "loss: 0.07848650217056274\n",
            "loss: 0.07848599553108215\n",
            "loss: 0.07848557084798813\n",
            "loss: 0.07848507165908813\n",
            "loss: 0.07848461717367172\n",
            "loss: 0.07848409563302994\n",
            "loss: 0.07848386466503143\n",
            "loss: 0.07848332822322845\n",
            "loss: 0.07848300784826279\n",
            "loss: 0.07848261296749115\n",
            "loss: 0.07848265767097473\n",
            "loss: 0.078482486307621\n",
            "loss: 0.07848259806632996\n",
            "loss: 0.07848294079303741\n",
            "loss: 0.0784839615225792\n",
            "loss: 0.0784856528043747\n",
            "loss: 0.07848843932151794\n",
            "loss: 0.0784929096698761\n",
            "loss: 0.07850003987550735\n",
            "loss: 0.07851146906614304\n",
            "loss: 0.07852903008460999\n",
            "loss: 0.07855699211359024\n",
            "loss: 0.07860007882118225\n",
            "loss: 0.07866689562797546\n",
            "loss: 0.07876621931791306\n",
            "loss: 0.07891134917736053\n",
            "loss: 0.07910789549350739\n",
            "loss: 0.07935125380754471\n",
            "loss: 0.07959243655204773\n",
            "loss: 0.07974714040756226\n",
            "loss: 0.07968881726264954\n",
            "loss: 0.07938183099031448\n",
            "loss: 0.07892902195453644\n",
            "loss: 0.0785694271326065\n",
            "loss: 0.0784747302532196\n",
            "loss: 0.07862934470176697\n",
            "loss: 0.07886335998773575\n",
            "loss: 0.07898331433534622\n",
            "loss: 0.07890323549509048\n",
            "loss: 0.07868912816047668\n",
            "loss: 0.07850715517997742\n",
            "loss: 0.07847394794225693\n",
            "loss: 0.07857237011194229\n",
            "loss: 0.07868847995996475\n",
            "loss: 0.07871509343385696\n",
            "loss: 0.07863593846559525\n",
            "loss: 0.07852239906787872\n",
            "loss: 0.07846475392580032\n",
            "loss: 0.0784907117486\n",
            "loss: 0.07855555415153503\n",
            "loss: 0.07859215140342712\n",
            "loss: 0.07856947183609009\n",
            "loss: 0.07851134985685349\n",
            "loss: 0.0784670040011406\n",
            "loss: 0.07846566289663315\n",
            "loss: 0.07849583029747009\n",
            "loss: 0.07852327823638916\n",
            "loss: 0.0785227045416832\n",
            "loss: 0.07849656045436859\n",
            "loss: 0.0784677192568779\n",
            "loss: 0.07845774292945862\n",
            "loss: 0.07846838235855103\n",
            "loss: 0.07848498970270157\n",
            "loss: 0.07849109172821045\n",
            "loss: 0.07848214358091354\n",
            "loss: 0.0784662589430809\n",
            "loss: 0.07845581322908401\n",
            "loss: 0.07845627516508102\n",
            "loss: 0.07846402376890182\n",
            "loss: 0.07847044616937637\n",
            "loss: 0.07846991717815399\n",
            "loss: 0.07846301048994064\n",
            "loss: 0.07845516502857208\n",
            "loss: 0.07845167070627213\n",
            "loss: 0.07845336943864822\n",
            "loss: 0.07845723628997803\n",
            "loss: 0.07845932990312576\n",
            "loss: 0.0784577876329422\n",
            "loss: 0.07845365256071091\n",
            "loss: 0.07844968885183334\n",
            "loss: 0.0784483253955841\n",
            "loss: 0.07844943553209305\n",
            "loss: 0.07845082879066467\n",
            "loss: 0.07845136523246765\n",
            "loss: 0.07845035940408707\n",
            "loss: 0.0784481018781662\n",
            "loss: 0.07844613492488861\n",
            "loss: 0.07844516634941101\n",
            "loss: 0.07844530045986176\n",
            "loss: 0.0784459188580513\n",
            "loss: 0.07844582200050354\n",
            "loss: 0.07844509929418564\n",
            "loss: 0.07844403386116028\n",
            "loss: 0.07844255119562149\n",
            "loss: 0.0784418135881424\n",
            "loss: 0.07844145596027374\n",
            "loss: 0.07844145596027374\n",
            "loss: 0.078441321849823\n",
            "loss: 0.07844092696905136\n",
            "loss: 0.07844028621912003\n",
            "loss: 0.07843918353319168\n",
            "loss: 0.07843859493732452\n",
            "loss: 0.07843801379203796\n",
            "loss: 0.07843764126300812\n",
            "loss: 0.07843740284442902\n",
            "loss: 0.07843717932701111\n",
            "loss: 0.07843656092882156\n",
            "loss: 0.078435979783535\n",
            "loss: 0.07843546569347382\n",
            "loss: 0.07843467593193054\n",
            "loss: 0.07843424379825592\n",
            "loss: 0.0784337967634201\n",
            "loss: 0.07843335717916489\n",
            "loss: 0.07843316346406937\n",
            "loss: 0.07843263447284698\n",
            "loss: 0.07843202352523804\n",
            "loss: 0.07843153178691864\n",
            "loss: 0.07843103259801865\n",
            "loss: 0.07843045890331268\n",
            "loss: 0.07842984795570374\n",
            "loss: 0.07842957228422165\n",
            "loss: 0.07842905074357986\n",
            "loss: 0.07842863351106644\n",
            "loss: 0.07842817902565002\n",
            "loss: 0.07842748612165451\n",
            "loss: 0.07842700183391571\n",
            "loss: 0.07842662185430527\n",
            "loss: 0.07842600345611572\n",
            "loss: 0.07842561602592468\n",
            "loss: 0.07842505723237991\n",
            "loss: 0.07842467725276947\n",
            "loss: 0.07842408120632172\n",
            "loss: 0.07842370867729187\n",
            "loss: 0.07842323929071426\n",
            "loss: 0.07842275500297546\n",
            "loss: 0.07842231541872025\n",
            "loss: 0.07842177152633667\n",
            "loss: 0.07842130213975906\n",
            "loss: 0.07842076569795609\n",
            "loss: 0.07842020690441132\n",
            "loss: 0.07841987162828445\n",
            "loss: 0.07841954380273819\n",
            "loss: 0.0784190222620964\n",
            "loss: 0.07841846346855164\n",
            "loss: 0.07841800153255463\n",
            "loss: 0.07841747999191284\n",
            "loss: 0.07841707766056061\n",
            "loss: 0.07841651141643524\n",
            "loss: 0.07841599732637405\n",
            "loss: 0.07841560244560242\n",
            "loss: 0.07841505110263824\n",
            "loss: 0.07841451466083527\n",
            "loss: 0.07841406017541885\n",
            "loss: 0.07841359823942184\n",
            "loss: 0.07841312885284424\n",
            "loss: 0.07841271162033081\n",
            "loss: 0.07841213047504425\n",
            "loss: 0.07841170579195023\n",
            "loss: 0.0784112736582756\n",
            "loss: 0.07841060310602188\n",
            "loss: 0.07841014117002487\n",
            "loss: 0.07840953022241592\n",
            "loss: 0.07840932160615921\n",
            "loss: 0.07840876281261444\n",
            "loss: 0.07840833067893982\n",
            "loss: 0.0784078910946846\n",
            "loss: 0.07840734720230103\n",
            "loss: 0.07840685546398163\n",
            "loss: 0.07840628921985626\n",
            "loss: 0.07840590924024582\n",
            "loss: 0.0784054771065712\n",
            "loss: 0.0784049928188324\n",
            "loss: 0.07840445637702942\n",
            "loss: 0.0784040167927742\n",
            "loss: 0.07840333133935928\n",
            "loss: 0.0784030333161354\n",
            "loss: 0.07840254157781601\n",
            "loss: 0.07840203493833542\n",
            "loss: 0.07840153574943542\n",
            "loss: 0.07840096950531006\n",
            "loss: 0.07840065658092499\n",
            "loss: 0.07840010523796082\n",
            "loss: 0.07839956879615784\n",
            "loss: 0.07839922606945038\n",
            "loss: 0.07839860767126083\n",
            "loss: 0.07839816063642502\n",
            "loss: 0.07839762419462204\n",
            "loss: 0.078397236764431\n",
            "loss: 0.0783967599272728\n",
            "loss: 0.07839604467153549\n",
            "loss: 0.07839567214250565\n",
            "loss: 0.07839515805244446\n",
            "loss: 0.07839468121528625\n",
            "loss: 0.07839423418045044\n",
            "loss: 0.07839379459619522\n",
            "loss: 0.07839331030845642\n",
            "loss: 0.07839284837245941\n",
            "loss: 0.07839231193065643\n",
            "loss: 0.07839178293943405\n",
            "loss: 0.07839149981737137\n",
            "loss: 0.078390933573246\n",
            "loss: 0.07839038968086243\n",
            "loss: 0.07838984578847885\n",
            "loss: 0.07838936150074005\n",
            "loss: 0.07838902622461319\n",
            "loss: 0.07838848233222961\n",
            "loss: 0.07838798314332962\n",
            "loss: 0.07838738709688187\n",
            "loss: 0.07838695496320724\n",
            "loss: 0.07838651537895203\n",
            "loss: 0.07838600873947144\n",
            "loss: 0.07838571816682816\n",
            "loss: 0.07838522642850876\n",
            "loss: 0.078384630382061\n",
            "loss: 0.07838426530361176\n",
            "loss: 0.07838401943445206\n",
            "loss: 0.07838363200426102\n",
            "loss: 0.07838349044322968\n",
            "loss: 0.07838357239961624\n",
            "loss: 0.07838387787342072\n",
            "loss: 0.07838454097509384\n",
            "loss: 0.07838587462902069\n",
            "loss: 0.07838799804449081\n",
            "loss: 0.07839194685220718\n",
            "loss: 0.07839822769165039\n",
            "loss: 0.0784088671207428\n",
            "loss: 0.07842577248811722\n",
            "loss: 0.07845330238342285\n",
            "loss: 0.07849682122468948\n",
            "loss: 0.07856600731611252\n",
            "loss: 0.07867270708084106\n",
            "loss: 0.07883297652006149\n",
            "loss: 0.07905731350183487\n",
            "loss: 0.07934438437223434\n",
            "loss: 0.07963831722736359\n",
            "loss: 0.07983364164829254\n",
            "loss: 0.07976283878087997\n",
            "loss: 0.07938019931316376\n",
            "loss: 0.07883384823799133\n",
            "loss: 0.07844091206789017\n",
            "loss: 0.07839904725551605\n",
            "loss: 0.07863809913396835\n",
            "loss: 0.07890703529119492\n",
            "loss: 0.07897009700536728\n",
            "loss: 0.07878358662128448\n",
            "loss: 0.07850981503725052\n",
            "loss: 0.07837075740098953\n",
            "loss: 0.07843813300132751\n",
            "loss: 0.07859712839126587\n",
            "loss: 0.07867751270532608\n",
            "loss: 0.07860608398914337\n",
            "loss: 0.07845929265022278\n",
            "loss: 0.07836969196796417\n",
            "loss: 0.07839588820934296\n",
            "loss: 0.0784820094704628\n",
            "loss: 0.07852888852357864\n",
            "loss: 0.07849214226007462\n",
            "loss: 0.07841292023658752\n",
            "loss: 0.07836489379405975\n",
            "loss: 0.07837976515293121\n",
            "loss: 0.07842601835727692\n",
            "loss: 0.07845014333724976\n",
            "loss: 0.07842931151390076\n",
            "loss: 0.07838660478591919\n",
            "loss: 0.0783611312508583\n",
            "loss: 0.07836923748254776\n",
            "loss: 0.07839386910200119\n",
            "loss: 0.07840652763843536\n",
            "loss: 0.07839532196521759\n",
            "loss: 0.07837244123220444\n",
            "loss: 0.07835812866687775\n",
            "loss: 0.07836147397756577\n",
            "loss: 0.07837428897619247\n",
            "loss: 0.07838156074285507\n",
            "loss: 0.07837634533643723\n",
            "loss: 0.07836414128541946\n",
            "loss: 0.07835562527179718\n",
            "loss: 0.07835577428340912\n",
            "loss: 0.07836198806762695\n",
            "loss: 0.07836639881134033\n",
            "loss: 0.07836482673883438\n",
            "loss: 0.07835868746042252\n",
            "loss: 0.07835312187671661\n",
            "loss: 0.0783519446849823\n",
            "loss: 0.07835421711206436\n",
            "loss: 0.07835711538791656\n",
            "loss: 0.07835701107978821\n",
            "loss: 0.07835400849580765\n",
            "loss: 0.07835055142641068\n",
            "loss: 0.07834868133068085\n",
            "loss: 0.07834913581609726\n",
            "loss: 0.07835057377815247\n",
            "loss: 0.07835111767053604\n",
            "loss: 0.0783499926328659\n",
            "loss: 0.07834796607494354\n",
            "loss: 0.07834608852863312\n",
            "loss: 0.07834546267986298\n",
            "loss: 0.07834567129611969\n",
            "loss: 0.07834622263908386\n",
            "loss: 0.07834586501121521\n",
            "loss: 0.07834500074386597\n",
            "loss: 0.0783436968922615\n",
            "loss: 0.07834253460168839\n",
            "loss: 0.07834228873252869\n",
            "loss: 0.0783422514796257\n",
            "loss: 0.07834210246801376\n",
            "loss: 0.07834172993898392\n",
            "loss: 0.07834083586931229\n",
            "loss: 0.07834003865718842\n",
            "loss: 0.07833938300609589\n",
            "loss: 0.07833895087242126\n",
            "loss: 0.0783386304974556\n",
            "loss: 0.07833842188119888\n",
            "loss: 0.07833800464868546\n",
            "loss: 0.07833731919527054\n",
            "loss: 0.07833678275346756\n",
            "loss: 0.07833616435527802\n",
            "loss: 0.07833555340766907\n",
            "loss: 0.0783352255821228\n",
            "loss: 0.07833489775657654\n",
            "loss: 0.07833442091941833\n",
            "loss: 0.07833395153284073\n",
            "loss: 0.07833333313465118\n",
            "loss: 0.07833276689052582\n",
            "loss: 0.07833239436149597\n",
            "loss: 0.07833199948072433\n",
            "loss: 0.07833151519298553\n",
            "loss: 0.07833099365234375\n",
            "loss: 0.07833045721054077\n",
            "loss: 0.07833018153905869\n",
            "loss: 0.07832957059144974\n",
            "loss: 0.07832905650138855\n",
            "loss: 0.0783286839723587\n",
            "loss: 0.07832802832126617\n",
            "loss: 0.07832777500152588\n",
            "loss: 0.07832714170217514\n",
            "loss: 0.07832670956850052\n",
            "loss: 0.07832631468772888\n",
            "loss: 0.07832584530115128\n",
            "loss: 0.07832526415586472\n",
            "loss: 0.0783248171210289\n",
            "loss: 0.07832441478967667\n",
            "loss: 0.07832394540309906\n",
            "loss: 0.0783233568072319\n",
            "loss: 0.07832301408052444\n",
            "loss: 0.07832248508930206\n",
            "loss: 0.07832211256027222\n",
            "loss: 0.07832162827253342\n",
            "loss: 0.07832098752260208\n",
            "loss: 0.07832060009241104\n",
            "loss: 0.07832007110118866\n",
            "loss: 0.07831955701112747\n",
            "loss: 0.07831910252571106\n",
            "loss: 0.0783187747001648\n",
            "loss: 0.07831816375255585\n",
            "loss: 0.0783175602555275\n",
            "loss: 0.07831728458404541\n",
            "loss: 0.07831671833992004\n",
            "loss: 0.07831630855798721\n",
            "loss: 0.07831596583127975\n",
            "loss: 0.07831541448831558\n",
            "loss: 0.07831484079360962\n",
            "loss: 0.07831443101167679\n",
            "loss: 0.07831404358148575\n",
            "loss: 0.0783134400844574\n",
            "loss: 0.0783129632472992\n",
            "loss: 0.0783124715089798\n",
            "loss: 0.07831211388111115\n",
            "loss: 0.078311488032341\n",
            "loss: 0.07831108570098877\n",
            "loss: 0.07831057161092758\n",
            "loss: 0.07831022143363953\n",
            "loss: 0.0783095732331276\n",
            "loss: 0.0783090591430664\n",
            "loss: 0.07830879092216492\n",
            "loss: 0.07830816507339478\n",
            "loss: 0.07830774039030075\n",
            "loss: 0.07830735296010971\n",
            "loss: 0.07830677181482315\n",
            "loss: 0.07830619812011719\n",
            "loss: 0.07830577343702316\n",
            "loss: 0.07830526679754257\n",
            "loss: 0.07830483466386795\n",
            "loss: 0.07830438762903214\n",
            "loss: 0.07830394804477692\n",
            "loss: 0.07830343395471573\n",
            "loss: 0.07830289006233215\n",
            "loss: 0.07830257713794708\n",
            "loss: 0.07830210030078888\n",
            "loss: 0.07830148935317993\n",
            "loss: 0.07830120623111725\n",
            "loss: 0.07830055803060532\n",
            "loss: 0.0783000960946083\n",
            "loss: 0.07829952985048294\n",
            "loss: 0.07829923927783966\n",
            "loss: 0.07829846441745758\n",
            "loss: 0.0782981812953949\n",
            "loss: 0.07829776406288147\n",
            "loss: 0.07829722762107849\n",
            "loss: 0.07829680293798447\n",
            "loss: 0.07829639315605164\n",
            "loss: 0.07829581201076508\n",
            "loss: 0.07829529047012329\n",
            "loss: 0.07829470187425613\n",
            "loss: 0.0782943069934845\n",
            "loss: 0.07829388231039047\n",
            "loss: 0.07829345762729645\n",
            "loss: 0.07829289883375168\n",
            "loss: 0.0782923474907875\n",
            "loss: 0.0782918632030487\n",
            "loss: 0.07829150557518005\n",
            "loss: 0.07829099148511887\n",
            "loss: 0.07829058915376663\n",
            "loss: 0.07829008251428604\n",
            "loss: 0.07828951627016068\n",
            "loss: 0.0782889798283577\n",
            "loss: 0.0782884955406189\n",
            "loss: 0.07828803360462189\n",
            "loss: 0.07828762382268906\n",
            "loss: 0.07828719913959503\n",
            "loss: 0.07828667014837265\n",
            "loss: 0.07828617841005325\n",
            "loss: 0.07828585803508759\n",
            "loss: 0.078285351395607\n",
            "loss: 0.07828502357006073\n",
            "loss: 0.07828465849161148\n",
            "loss: 0.07828420400619507\n",
            "loss: 0.07828400284051895\n",
            "loss: 0.07828357815742493\n",
            "loss: 0.07828347384929657\n",
            "loss: 0.07828330248594284\n",
            "loss: 0.07828348129987717\n",
            "loss: 0.07828360050916672\n",
            "loss: 0.07828440517187119\n",
            "loss: 0.07828538119792938\n",
            "loss: 0.07828717678785324\n",
            "loss: 0.0782899558544159\n",
            "loss: 0.07829415053129196\n",
            "loss: 0.07830063253641129\n",
            "loss: 0.07831026613712311\n",
            "loss: 0.0783248171210289\n",
            "loss: 0.07834658026695251\n",
            "loss: 0.078378826379776\n",
            "loss: 0.07842617481946945\n",
            "loss: 0.07849425822496414\n",
            "loss: 0.07859012484550476\n",
            "loss: 0.07871752977371216\n",
            "loss: 0.07887719571590424\n",
            "loss: 0.07904908806085587\n",
            "loss: 0.07919739186763763\n",
            "loss: 0.07925126701593399\n",
            "loss: 0.07915987074375153\n",
            "loss: 0.07891096919775009\n",
            "loss: 0.07859721034765244\n",
            "loss: 0.07835126668214798\n",
            "loss: 0.07827012240886688\n",
            "loss: 0.07834942638874054\n",
            "loss: 0.0785018652677536\n",
            "loss: 0.07861863821744919\n",
            "loss: 0.07862519472837448\n",
            "loss: 0.07852377742528915\n",
            "loss: 0.07838056981563568\n",
            "loss: 0.07828225940465927\n",
            "loss: 0.07827325165271759\n",
            "loss: 0.07833390682935715\n",
            "loss: 0.07840539515018463\n",
            "loss: 0.07843293994665146\n",
            "loss: 0.07840094715356827\n",
            "loss: 0.07833459973335266\n",
            "loss: 0.07827872037887573\n",
            "loss: 0.07826321572065353\n",
            "loss: 0.07828648388385773\n",
            "loss: 0.07832248508930206\n",
            "loss: 0.07834181189537048\n",
            "loss: 0.07833216339349747\n",
            "loss: 0.07830219715833664\n",
            "loss: 0.07827229797840118\n",
            "loss: 0.07825956493616104\n",
            "loss: 0.07826648652553558\n",
            "loss: 0.07828294485807419\n",
            "loss: 0.07829510420560837\n",
            "loss: 0.07829403132200241\n",
            "loss: 0.07828179001808167\n",
            "loss: 0.07826658338308334\n",
            "loss: 0.07825721055269241\n",
            "loss: 0.07825690507888794\n",
            "loss: 0.07826323807239532\n",
            "loss: 0.07827005535364151\n",
            "loss: 0.07827232033014297\n",
            "loss: 0.07826866209506989\n",
            "loss: 0.07826176285743713\n",
            "loss: 0.07825528830289841\n",
            "loss: 0.07825225591659546\n",
            "loss: 0.07825328409671783\n",
            "loss: 0.07825631648302078\n",
            "loss: 0.07825866341590881\n",
            "loss: 0.07825864851474762\n",
            "loss: 0.07825639098882675\n",
            "loss: 0.07825307548046112\n",
            "loss: 0.07825016230344772\n",
            "loss: 0.07824864238500595\n",
            "loss: 0.07824885100126266\n",
            "loss: 0.0782497227191925\n",
            "loss: 0.0782504677772522\n",
            "loss: 0.07825037091970444\n",
            "loss: 0.07824930548667908\n",
            "loss: 0.07824758440256119\n",
            "loss: 0.07824593782424927\n",
            "loss: 0.0782448798418045\n",
            "loss: 0.07824444770812988\n",
            "loss: 0.0782443955540657\n",
            "loss: 0.07824453711509705\n",
            "loss: 0.07824447751045227\n",
            "loss: 0.07824401557445526\n",
            "loss: 0.07824316620826721\n",
            "loss: 0.0782422423362732\n",
            "loss: 0.07824146747589111\n",
            "loss: 0.07824060320854187\n",
            "loss: 0.07824020087718964\n",
            "loss: 0.07823989540338516\n",
            "loss: 0.07823967188596725\n",
            "loss: 0.07823937386274338\n",
            "loss: 0.07823892682790756\n",
            "loss: 0.07823824882507324\n",
            "loss: 0.07823745906352997\n",
            "loss: 0.07823709398508072\n",
            "loss: 0.07823637127876282\n",
            "loss: 0.0782359316945076\n",
            "loss: 0.07823535799980164\n",
            "loss: 0.07823500037193298\n",
            "loss: 0.07823466509580612\n",
            "loss: 0.07823414355516434\n",
            "loss: 0.0782337635755539\n",
            "loss: 0.0782332569360733\n",
            "loss: 0.07823269814252853\n",
            "loss: 0.0782323107123375\n",
            "loss: 0.07823169976472855\n",
            "loss: 0.07823117077350616\n",
            "loss: 0.07823070883750916\n",
            "loss: 0.07823025435209274\n",
            "loss: 0.07822979986667633\n",
            "loss: 0.07822929322719574\n",
            "loss: 0.0782289206981659\n",
            "loss: 0.07822845876216888\n",
            "loss: 0.07822804152965546\n",
            "loss: 0.07822738587856293\n",
            "loss: 0.07822690159082413\n",
            "loss: 0.0782264694571495\n",
            "loss: 0.07822588831186295\n",
            "loss: 0.0782255008816719\n",
            "loss: 0.07822496443986893\n",
            "loss: 0.07822442799806595\n",
            "loss: 0.0782240480184555\n",
            "loss: 0.0782235786318779\n",
            "loss: 0.0782230868935585\n",
            "loss: 0.07822255790233612\n",
            "loss: 0.07822200655937195\n",
            "loss: 0.07822158187627792\n",
            "loss: 0.07822111994028091\n",
            "loss: 0.07822068780660629\n",
            "loss: 0.07822031527757645\n",
            "loss: 0.07821992039680481\n",
            "loss: 0.0782192274928093\n",
            "loss: 0.07821860909461975\n",
            "loss: 0.07821822911500931\n",
            "loss: 0.07821786403656006\n",
            "loss: 0.07821714133024216\n",
            "loss: 0.0782168060541153\n",
            "loss: 0.07821635901927948\n",
            "loss: 0.07821597903966904\n",
            "loss: 0.07821546494960785\n",
            "loss: 0.0782148614525795\n",
            "loss: 0.0782143846154213\n",
            "loss: 0.07821396738290787\n",
            "loss: 0.07821344584226608\n",
            "loss: 0.07821301370859146\n",
            "loss: 0.0782124400138855\n",
            "loss: 0.07821211963891983\n",
            "loss: 0.07821162045001984\n",
            "loss: 0.07821114361286163\n",
            "loss: 0.07821059226989746\n",
            "loss: 0.07821004092693329\n",
            "loss: 0.07820948213338852\n",
            "loss: 0.0782090574502945\n",
            "loss: 0.07820864766836166\n",
            "loss: 0.0782080888748169\n",
            "loss: 0.07820765674114227\n",
            "loss: 0.0782071128487587\n",
            "loss: 0.0782066360116005\n",
            "loss: 0.07820617407560349\n",
            "loss: 0.07820576429367065\n",
            "loss: 0.07820522040128708\n",
            "loss: 0.07820475846529007\n",
            "loss: 0.0782042145729065\n",
            "loss: 0.07820377498865128\n",
            "loss: 0.07820331305265427\n",
            "loss: 0.07820285111665726\n",
            "loss: 0.07820237427949905\n",
            "loss: 0.07820189744234085\n",
            "loss: 0.07820139825344086\n",
            "loss: 0.07820090651512146\n",
            "loss: 0.07820042967796326\n",
            "loss: 0.07819996029138565\n",
            "loss: 0.07819946855306625\n",
            "loss: 0.07819905132055283\n",
            "loss: 0.07819872349500656\n",
            "loss: 0.07819832116365433\n",
            "loss: 0.07819785922765732\n",
            "loss: 0.07819752395153046\n",
            "loss: 0.07819733023643494\n",
            "loss: 0.07819721847772598\n",
            "loss: 0.0781974345445633\n",
            "loss: 0.0781978964805603\n",
            "loss: 0.07819878309965134\n",
            "loss: 0.07820067554712296\n",
            "loss: 0.07820402830839157\n",
            "loss: 0.07820926606655121\n",
            "loss: 0.07821819931268692\n",
            "loss: 0.07823283970355988\n",
            "loss: 0.07825670391321182\n",
            "loss: 0.078295037150383\n",
            "loss: 0.07835680991411209\n",
            "loss: 0.07845339924097061\n",
            "loss: 0.07860233634710312\n",
            "loss: 0.07881719619035721\n",
            "loss: 0.07910491526126862\n",
            "loss: 0.07942362129688263\n",
            "loss: 0.0796794518828392\n",
            "loss: 0.07969358563423157\n",
            "loss: 0.07936891913414001\n",
            "loss: 0.07880028337240219\n",
            "loss: 0.07831908762454987\n",
            "loss: 0.07819341123104095\n",
            "loss: 0.07840962707996368\n",
            "loss: 0.07871416211128235\n",
            "loss: 0.07882757484912872\n",
            "loss: 0.07865987718105316\n",
            "loss: 0.078362837433815\n",
            "loss: 0.07818929105997086\n",
            "loss: 0.07824413478374481\n",
            "loss: 0.07841542363166809\n",
            "loss: 0.07851115614175797\n",
            "loss: 0.07843973487615585\n",
            "loss: 0.07828101515769958\n",
            "loss: 0.07818375527858734\n",
            "loss: 0.07821445167064667\n",
            "loss: 0.07830903679132462\n",
            "loss: 0.07835563272237778\n",
            "loss: 0.07830987870693207\n",
            "loss: 0.07822288572788239\n",
            "loss: 0.07817771285772324\n",
            "loss: 0.07820234447717667\n",
            "loss: 0.07825351506471634\n",
            "loss: 0.07827214896678925\n",
            "loss: 0.07824112474918365\n",
            "loss: 0.07819391787052155\n",
            "loss: 0.07817459851503372\n",
            "loss: 0.07819189876317978\n",
            "loss: 0.07821903377771378\n",
            "loss: 0.07822523266077042\n",
            "loss: 0.07820563018321991\n",
            "loss: 0.07818062603473663\n",
            "loss: 0.0781719908118248\n",
            "loss: 0.07818248122930527\n",
            "loss: 0.07819630205631256\n",
            "loss: 0.07819853723049164\n",
            "loss: 0.0781872496008873\n",
            "loss: 0.07817377895116806\n",
            "loss: 0.07816901803016663\n",
            "loss: 0.07817446440458298\n",
            "loss: 0.07818172127008438\n",
            "loss: 0.07818297296762466\n",
            "loss: 0.07817698270082474\n",
            "loss: 0.07816961407661438\n",
            "loss: 0.07816637307405472\n",
            "loss: 0.07816841453313828\n",
            "loss: 0.07817202806472778\n",
            "loss: 0.07817322015762329\n",
            "loss: 0.07817036658525467\n",
            "loss: 0.07816605269908905\n",
            "loss: 0.07816353440284729\n",
            "loss: 0.07816397398710251\n",
            "loss: 0.07816572487354279\n",
            "loss: 0.07816663384437561\n",
            "loss: 0.07816535234451294\n",
            "loss: 0.07816295325756073\n",
            "loss: 0.0781610906124115\n",
            "loss: 0.07816042006015778\n",
            "loss: 0.07816105335950851\n",
            "loss: 0.0781615823507309\n",
            "loss: 0.07816126197576523\n",
            "loss: 0.07816006243228912\n",
            "loss: 0.07815834879875183\n",
            "loss: 0.0781574472784996\n",
            "loss: 0.07815742492675781\n",
            "loss: 0.07815755903720856\n",
            "loss: 0.07815729826688766\n",
            "loss: 0.07815700769424438\n",
            "loss: 0.07815582305192947\n",
            "loss: 0.07815489172935486\n",
            "loss: 0.07815442234277725\n",
            "loss: 0.0781538188457489\n",
            "loss: 0.0781538262963295\n",
            "loss: 0.07815355062484741\n",
            "loss: 0.07815302908420563\n",
            "loss: 0.07815227657556534\n",
            "loss: 0.07815161347389221\n",
            "loss: 0.07815109193325043\n",
            "loss: 0.07815076410770416\n",
            "loss: 0.07815046608448029\n",
            "loss: 0.07814992964267731\n",
            "loss: 0.07814940810203552\n",
            "loss: 0.07814880460500717\n",
            "loss: 0.07814835757017136\n",
            "loss: 0.07814786583185196\n",
            "loss: 0.07814743369817734\n",
            "loss: 0.0781470388174057\n",
            "loss: 0.07814647257328033\n",
            "loss: 0.07814604043960571\n",
            "loss: 0.07814552634954453\n",
            "loss: 0.07814504206180573\n",
            "loss: 0.07814467698335648\n",
            "loss: 0.0781441181898117\n",
            "loss: 0.07814367860555649\n",
            "loss: 0.07814333587884903\n",
            "loss: 0.0781429186463356\n",
            "loss: 0.07814228534698486\n",
            "loss: 0.07814189046621323\n",
            "loss: 0.07814130932092667\n",
            "loss: 0.07814082503318787\n",
            "loss: 0.0781405046582222\n",
            "loss: 0.07813997566699982\n",
            "loss: 0.07813946157693863\n",
            "loss: 0.07813901454210281\n",
            "loss: 0.07813859730958939\n",
            "loss: 0.078138068318367\n",
            "loss: 0.0781375989317894\n",
            "loss: 0.07813730090856552\n",
            "loss: 0.07813668996095657\n",
            "loss: 0.07813619822263718\n",
            "loss: 0.07813572883605957\n",
            "loss: 0.07813534885644913\n",
            "loss: 0.07813485711812973\n",
            "loss: 0.07813426852226257\n",
            "loss: 0.07813381403684616\n",
            "loss: 0.07813331484794617\n",
            "loss: 0.07813303917646408\n",
            "loss: 0.07813261449337006\n",
            "loss: 0.07813204079866409\n",
            "loss: 0.0781315416097641\n",
            "loss: 0.07813112437725067\n",
            "loss: 0.07813059538602829\n",
            "loss: 0.07813022285699844\n",
            "loss: 0.07812971621751785\n",
            "loss: 0.07812918722629547\n",
            "loss: 0.07812871038913727\n",
            "loss: 0.07812824100255966\n",
            "loss: 0.07812779396772385\n",
            "loss: 0.07812731713056564\n",
            "loss: 0.07812688499689102\n",
            "loss: 0.0781262144446373\n",
            "loss: 0.07812582701444626\n",
            "loss: 0.07812533527612686\n",
            "loss: 0.07812496274709702\n",
            "loss: 0.0781245157122612\n",
            "loss: 0.07812400907278061\n",
            "loss: 0.0781235322356224\n",
            "loss: 0.078123077750206\n",
            "loss: 0.07812271267175674\n",
            "loss: 0.078122079372406\n",
            "loss: 0.07812163233757019\n",
            "loss: 0.07812108844518661\n",
            "loss: 0.07812070101499557\n",
            "loss: 0.07812025398015976\n",
            "loss: 0.0781196877360344\n",
            "loss: 0.07811925560235977\n",
            "loss: 0.07811886072158813\n",
            "loss: 0.07811836898326874\n",
            "loss: 0.07811793684959412\n",
            "loss: 0.07811737805604935\n",
            "loss: 0.0781167596578598\n",
            "loss: 0.0781165212392807\n",
            "loss: 0.07811596244573593\n",
            "loss: 0.07811535149812698\n",
            "loss: 0.07811512798070908\n",
            "loss: 0.07811454683542252\n",
            "loss: 0.07811397314071655\n",
            "loss: 0.07811355590820312\n",
            "loss: 0.07811307907104492\n",
            "loss: 0.07811269909143448\n",
            "loss: 0.07811214029788971\n",
            "loss: 0.07811177521944046\n",
            "loss: 0.07811125367879868\n",
            "loss: 0.07811067253351212\n",
            "loss: 0.07811032235622406\n",
            "loss: 0.07810983061790466\n",
            "loss: 0.07810921967029572\n",
            "loss: 0.07810905575752258\n",
            "loss: 0.07810834795236588\n",
            "loss: 0.07810782641172409\n",
            "loss: 0.07810739427804947\n",
            "loss: 0.0781068429350853\n",
            "loss: 0.07810629159212112\n",
            "loss: 0.07810593396425247\n",
            "loss: 0.07810556888580322\n",
            "loss: 0.07810497283935547\n",
            "loss: 0.07810462266206741\n",
            "loss: 0.07810407876968384\n",
            "loss: 0.07810359448194504\n",
            "loss: 0.07810316234827042\n",
            "loss: 0.07810255140066147\n",
            "loss: 0.07810220867395401\n",
            "loss: 0.07810160517692566\n",
            "loss: 0.07810118049383163\n",
            "loss: 0.07810071855783463\n",
            "loss: 0.07810026407241821\n",
            "loss: 0.07809976488351822\n",
            "loss: 0.0780993178486824\n",
            "loss: 0.07809886336326599\n",
            "loss: 0.07809825241565704\n",
            "loss: 0.07809767872095108\n",
            "loss: 0.07809731364250183\n",
            "loss: 0.07809694111347198\n",
            "loss: 0.07809638977050781\n",
            "loss: 0.07809590548276901\n",
            "loss: 0.07809552550315857\n",
            "loss: 0.07809503376483917\n",
            "loss: 0.07809440791606903\n",
            "loss: 0.07809395343065262\n",
            "loss: 0.07809344679117203\n",
            "loss: 0.07809296995401382\n",
            "loss: 0.07809249311685562\n",
            "loss: 0.07809203118085861\n",
            "loss: 0.07809162884950638\n",
            "loss: 0.07809104770421982\n",
            "loss: 0.07809053361415863\n",
            "loss: 0.0780901163816452\n",
            "loss: 0.07808966189622879\n",
            "loss: 0.07808927446603775\n",
            "loss: 0.07808875292539597\n",
            "loss: 0.07808829843997955\n",
            "loss: 0.07808788865804672\n",
            "loss: 0.0780874714255333\n",
            "loss: 0.07808706909418106\n",
            "loss: 0.07808676362037659\n",
            "loss: 0.07808651775121689\n",
            "loss: 0.07808655500411987\n",
            "loss: 0.07808658480644226\n",
            "loss: 0.07808730751276016\n",
            "loss: 0.07808864861726761\n",
            "loss: 0.07809114456176758\n",
            "loss: 0.07809559255838394\n",
            "loss: 0.07810322940349579\n",
            "loss: 0.07811588793992996\n",
            "loss: 0.07813742011785507\n",
            "loss: 0.07817336171865463\n",
            "loss: 0.07823314517736435\n",
            "loss: 0.07833090424537659\n",
            "loss: 0.07848700135946274\n",
            "loss: 0.07872337847948074\n",
            "loss: 0.07905644178390503\n",
            "loss: 0.0794505700469017\n",
            "loss: 0.07979536801576614\n",
            "loss: 0.07985475659370422\n",
            "loss: 0.07947683334350586\n",
            "loss: 0.07877330482006073\n",
            "loss: 0.07819800078868866\n",
            "loss: 0.07810363918542862\n",
            "loss: 0.07842640578746796\n",
            "loss: 0.078785240650177\n",
            "loss: 0.07882289588451385\n",
            "loss: 0.07851513475179672\n",
            "loss: 0.07816490530967712\n",
            "loss: 0.07808411866426468\n",
            "loss: 0.07827085256576538\n",
            "loss: 0.07846404612064362\n",
            "loss: 0.07844455540180206\n",
            "loss: 0.07824483513832092\n",
            "loss: 0.07808426767587662\n",
            "loss: 0.07810469716787338\n",
            "loss: 0.07823318243026733\n",
            "loss: 0.07829894870519638\n",
            "loss: 0.07822899520397186\n",
            "loss: 0.07811054587364197\n",
            "loss: 0.07806864380836487\n",
            "loss: 0.078123539686203\n",
            "loss: 0.07818849384784698\n",
            "loss: 0.07818302512168884\n",
            "loss: 0.07811873406171799\n",
            "loss: 0.07806864380836487\n",
            "loss: 0.07807821780443192\n",
            "loss: 0.07811973989009857\n",
            "loss: 0.07813753187656403\n",
            "loss: 0.07811184972524643\n",
            "loss: 0.07807444781064987\n",
            "loss: 0.07806362956762314\n",
            "loss: 0.07808241248130798\n",
            "loss: 0.0781019851565361\n",
            "loss: 0.07809852808713913\n",
            "loss: 0.07807721942663193\n",
            "loss: 0.07806150615215302\n",
            "loss: 0.07806406170129776\n",
            "loss: 0.07807684689760208\n",
            "loss: 0.07808268815279007\n",
            "loss: 0.07807514816522598\n",
            "loss: 0.07806264609098434\n",
            "loss: 0.07805777341127396\n",
            "loss: 0.07806232571601868\n",
            "loss: 0.07806865870952606\n",
            "loss: 0.07806875556707382\n",
            "loss: 0.0780627653002739\n",
            "loss: 0.07805655896663666\n",
            "loss: 0.07805553078651428\n",
            "loss: 0.0780586525797844\n",
            "loss: 0.07806124538183212\n",
            "loss: 0.07806005328893661\n",
            "loss: 0.07805585861206055\n",
            "loss: 0.07805298268795013\n",
            "loss: 0.07805278897285461\n",
            "loss: 0.07805459946393967\n",
            "loss: 0.07805550843477249\n",
            "loss: 0.07805430889129639\n",
            "loss: 0.07805179804563522\n",
            "loss: 0.07805004715919495\n",
            "loss: 0.07804999500513077\n",
            "loss: 0.0780506432056427\n",
            "loss: 0.07805079966783524\n",
            "loss: 0.07804994285106659\n",
            "loss: 0.07804831862449646\n",
            "loss: 0.07804740965366364\n",
            "loss: 0.07804699242115021\n",
            "loss: 0.07804708927869797\n",
            "loss: 0.07804692536592484\n",
            "loss: 0.0780462995171547\n",
            "loss: 0.07804537564516068\n",
            "loss: 0.07804464548826218\n",
            "loss: 0.0780441090464592\n",
            "loss: 0.07804413139820099\n",
            "loss: 0.07804383337497711\n",
            "loss: 0.07804320007562637\n",
            "loss: 0.0780426487326622\n",
            "loss: 0.07804179191589355\n",
            "loss: 0.0780414342880249\n",
            "loss: 0.07804097980260849\n",
            "loss: 0.07804082334041595\n",
            "loss: 0.07804037630558014\n",
            "loss: 0.07803985476493835\n",
            "loss: 0.07803913950920105\n",
            "loss: 0.07803859561681747\n",
            "loss: 0.07803811877965927\n",
            "loss: 0.0780377984046936\n",
            "loss: 0.07803752273321152\n",
            "loss: 0.07803711295127869\n",
            "loss: 0.07803633064031601\n",
            "loss: 0.07803589105606079\n",
            "loss: 0.07803553342819214\n",
            "loss: 0.07803495228290558\n",
            "loss: 0.07803449779748917\n",
            "loss: 0.07803408801555634\n",
            "loss: 0.07803365588188171\n",
            "loss: 0.07803310453891754\n",
            "loss: 0.07803284376859665\n",
            "loss: 0.0780322477221489\n",
            "loss: 0.07803177833557129\n",
            "loss: 0.07803133130073547\n",
            "loss: 0.0780307948589325\n",
            "loss: 0.07803031802177429\n",
            "loss: 0.07802996784448624\n",
            "loss: 0.07802945375442505\n",
            "loss: 0.07802897691726685\n",
            "loss: 0.07802847772836685\n",
            "loss: 0.07802799344062805\n",
            "loss: 0.07802766561508179\n",
            "loss: 0.0780271589756012\n",
            "loss: 0.07802672684192657\n",
            "loss: 0.0780261754989624\n",
            "loss: 0.07802562415599823\n",
            "loss: 0.07802534848451614\n",
            "loss: 0.07802485674619675\n",
            "loss: 0.07802435010671616\n",
            "loss: 0.0780239924788475\n",
            "loss: 0.07802362740039825\n",
            "loss: 0.0780230313539505\n",
            "loss: 0.07802245765924454\n",
            "loss: 0.07802223414182663\n",
            "loss: 0.07802162319421768\n",
            "loss: 0.07802112400531769\n",
            "loss: 0.0780206173658371\n",
            "loss: 0.07802018523216248\n",
            "loss: 0.07801969349384308\n",
            "loss: 0.0780194029211998\n",
            "loss: 0.07801882922649384\n",
            "loss: 0.0780184417963028\n",
            "loss: 0.07801790535449982\n",
            "loss: 0.07801743596792221\n",
            "loss: 0.07801710814237595\n",
            "loss: 0.07801643013954163\n",
            "loss: 0.07801604270935059\n",
            "loss: 0.07801549136638641\n",
            "loss: 0.07801509648561478\n",
            "loss: 0.0780145674943924\n",
            "loss: 0.07801413536071777\n",
            "loss: 0.07801371812820435\n",
            "loss: 0.07801325619220734\n",
            "loss: 0.0780128762125969\n",
            "loss: 0.0780123621225357\n",
            "loss: 0.0780119001865387\n",
            "loss: 0.0780113935470581\n",
            "loss: 0.0780109241604805\n",
            "loss: 0.07801041007041931\n",
            "loss: 0.07801003754138947\n",
            "loss: 0.07800961285829544\n",
            "loss: 0.07800907641649246\n",
            "loss: 0.07800854742527008\n",
            "loss: 0.0780082494020462\n",
            "loss: 0.07800763100385666\n",
            "loss: 0.07800725102424622\n",
            "loss: 0.07800666987895966\n",
            "loss: 0.07800643146038055\n",
            "loss: 0.07800571620464325\n",
            "loss: 0.07800520956516266\n",
            "loss: 0.07800489664077759\n",
            "loss: 0.07800430804491043\n",
            "loss: 0.07800393551588058\n",
            "loss: 0.0780034065246582\n",
            "loss: 0.07800304889678955\n",
            "loss: 0.07800251245498657\n",
            "loss: 0.0780019462108612\n",
            "loss: 0.07800152897834778\n",
            "loss: 0.0780009776353836\n",
            "loss: 0.07800065726041794\n",
            "loss: 0.07800007611513138\n",
            "loss: 0.07799974828958511\n",
            "loss: 0.07799915969371796\n",
            "loss: 0.07799878716468811\n",
            "loss: 0.07799824327230453\n",
            "loss: 0.07799790054559708\n",
            "loss: 0.07799737900495529\n",
            "loss: 0.07799689471721649\n",
            "loss: 0.07799641788005829\n",
            "loss: 0.0779959112405777\n",
            "loss: 0.07799550890922546\n",
            "loss: 0.07799489796161652\n",
            "loss: 0.07799442857503891\n",
            "loss: 0.07799404114484787\n",
            "loss: 0.07799360156059265\n",
            "loss: 0.07799313217401505\n",
            "loss: 0.07799267023801804\n",
            "loss: 0.07799219340085983\n",
            "loss: 0.07799166440963745\n",
            "loss: 0.07799113541841507\n",
            "loss: 0.07799074798822403\n",
            "loss: 0.07799025624990463\n",
            "loss: 0.07798980176448822\n",
            "loss: 0.07798921316862106\n",
            "loss: 0.07798877358436584\n",
            "loss: 0.07798812538385391\n",
            "loss: 0.07798793911933899\n",
            "loss: 0.07798735797405243\n",
            "loss: 0.07798686623573303\n",
            "loss: 0.07798631489276886\n",
            "loss: 0.077985979616642\n",
            "loss: 0.07798535376787186\n",
            "loss: 0.07798495888710022\n",
            "loss: 0.07798457890748978\n",
            "loss: 0.07798416912555695\n",
            "loss: 0.077983558177948\n",
            "loss: 0.0779830813407898\n",
            "loss: 0.07798255234956741\n",
            "loss: 0.07798215746879578\n",
            "loss: 0.07798180729150772\n",
            "loss: 0.077981136739254\n",
            "loss: 0.0779806524515152\n",
            "loss: 0.07798029482364655\n",
            "loss: 0.07797986268997192\n",
            "loss: 0.07797928154468536\n",
            "loss: 0.07797889411449432\n",
            "loss: 0.07797834277153015\n",
            "loss: 0.0779779925942421\n",
            "loss: 0.07797740399837494\n",
            "loss: 0.07797694951295853\n",
            "loss: 0.0779765248298645\n",
            "loss: 0.07797595858573914\n",
            "loss: 0.07797562330961227\n",
            "loss: 0.07797503471374512\n",
            "loss: 0.0779745802283287\n",
            "loss: 0.07797428220510483\n",
            "loss: 0.0779738575220108\n",
            "loss: 0.07797352969646454\n",
            "loss: 0.07797307521104813\n",
            "loss: 0.07797308266162872\n",
            "loss: 0.07797277718782425\n",
            "loss: 0.07797282189130783\n",
            "loss: 0.0779728814959526\n",
            "loss: 0.0779733881354332\n",
            "loss: 0.07797448337078094\n",
            "loss: 0.07797607779502869\n",
            "loss: 0.07797893136739731\n",
            "loss: 0.0779833272099495\n",
            "loss: 0.07799039036035538\n",
            "loss: 0.07800144702196121\n",
            "loss: 0.07801831513643265\n",
            "loss: 0.07804466038942337\n",
            "loss: 0.07808487862348557\n",
            "loss: 0.07814561575651169\n",
            "loss: 0.07823496311903\n",
            "loss: 0.07836359739303589\n",
            "loss: 0.07853665947914124\n",
            "loss: 0.07875107228755951\n",
            "loss: 0.07896891981363297\n",
            "loss: 0.07912412285804749\n",
            "loss: 0.07910887897014618\n",
            "loss: 0.07887701690196991\n",
            "loss: 0.07848548144102097\n",
            "loss: 0.0781235471367836\n",
            "loss: 0.07796284556388855\n",
            "loss: 0.07803861051797867\n",
            "loss: 0.07823949307203293\n",
            "loss: 0.07839859277009964\n",
            "loss: 0.07840234041213989\n",
            "loss: 0.07825218141078949\n",
            "loss: 0.07806288450956345\n",
            "loss: 0.07796098291873932\n",
            "loss: 0.07799132168292999\n",
            "loss: 0.07809494435787201\n",
            "loss: 0.07817218452692032\n",
            "loss: 0.07816087454557419\n",
            "loss: 0.07807526737451553\n",
            "loss: 0.07798609882593155\n",
            "loss: 0.07795414328575134\n",
            "loss: 0.07798652350902557\n",
            "loss: 0.07804024964570999\n",
            "loss: 0.07806506007909775\n",
            "loss: 0.078042171895504\n",
            "loss: 0.0779932513833046\n",
            "loss: 0.07795663177967072\n",
            "loss: 0.07795412838459015\n",
            "loss: 0.0779777467250824\n",
            "loss: 0.0780014917254448\n",
            "loss: 0.07800443470478058\n",
            "loss: 0.07798593491315842\n",
            "loss: 0.07796129584312439\n",
            "loss: 0.07794839143753052\n",
            "loss: 0.0779522955417633\n",
            "loss: 0.07796541601419449\n",
            "loss: 0.07797457277774811\n",
            "loss: 0.0779724270105362\n",
            "loss: 0.07796129584312439\n",
            "loss: 0.07794958353042603\n",
            "loss: 0.07794472575187683\n",
            "loss: 0.07794768363237381\n",
            "loss: 0.07795379310846329\n",
            "loss: 0.07795750349760056\n",
            "loss: 0.07795548439025879\n",
            "loss: 0.077949658036232\n",
            "loss: 0.07794389128684998\n",
            "loss: 0.07794152945280075\n",
            "loss: 0.07794269919395447\n",
            "loss: 0.07794531434774399\n",
            "loss: 0.07794711738824844\n",
            "loss: 0.0779460221529007\n",
            "loss: 0.07794323563575745\n",
            "loss: 0.0779401957988739\n",
            "loss: 0.07793834805488586\n",
            "loss: 0.07793832570314407\n",
            "loss: 0.07793934643268585\n",
            "loss: 0.07794001698493958\n",
            "loss: 0.07793975621461868\n",
            "loss: 0.07793840765953064\n",
            "loss: 0.0779367983341217\n",
            "loss: 0.07793529331684113\n",
            "loss: 0.07793467491865158\n",
            "loss: 0.077934630215168\n",
            "loss: 0.07793471217155457\n",
            "loss: 0.07793476432561874\n",
            "loss: 0.07793410122394562\n",
            "loss: 0.07793329656124115\n",
            "loss: 0.0779322162270546\n",
            "loss: 0.07793153077363968\n",
            "loss: 0.07793105393648148\n",
            "loss: 0.07793073356151581\n",
            "loss: 0.07793060690164566\n",
            "loss: 0.07793018966913223\n",
            "loss: 0.07792973518371582\n",
            "loss: 0.07792926579713821\n",
            "loss: 0.0779285728931427\n",
            "loss: 0.07792776077985764\n",
            "loss: 0.07792722433805466\n",
            "loss: 0.07792674750089645\n",
            "loss: 0.07792653143405914\n",
            "loss: 0.07792612910270691\n",
            "loss: 0.07792572677135468\n",
            "loss: 0.0779251977801323\n",
            "loss: 0.07792459428310394\n",
            "loss: 0.07792404294013977\n",
            "loss: 0.07792361080646515\n",
            "loss: 0.07792306691408157\n",
            "loss: 0.07792258262634277\n",
            "loss: 0.07792216539382935\n",
            "loss: 0.07792186737060547\n",
            "loss: 0.07792138308286667\n",
            "loss: 0.07792087644338608\n",
            "loss: 0.07792031019926071\n",
            "loss: 0.07792001962661743\n",
            "loss: 0.07791928946971893\n",
            "loss: 0.07791902869939804\n",
            "loss: 0.07791844010353088\n",
            "loss: 0.0779179260134697\n",
            "loss: 0.07791760563850403\n",
            "loss: 0.07791706919670105\n",
            "loss: 0.07791659981012344\n",
            "loss: 0.07791614532470703\n",
            "loss: 0.07791564613580704\n",
            "loss: 0.07791522145271301\n",
            "loss: 0.0779147520661354\n",
            "loss: 0.0779142677783966\n",
            "loss: 0.07791385799646378\n",
            "loss: 0.07791323959827423\n",
            "loss: 0.07791291922330856\n",
            "loss: 0.07791247218847275\n",
            "loss: 0.07791193574666977\n",
            "loss: 0.077911376953125\n",
            "loss: 0.07791105657815933\n",
            "loss: 0.07791059464216232\n",
            "loss: 0.07790998369455338\n",
            "loss: 0.07790961861610413\n",
            "loss: 0.0779091864824295\n",
            "loss: 0.07790856063365936\n",
            "loss: 0.07790827006101608\n",
            "loss: 0.07790772616863251\n",
            "loss: 0.07790727913379669\n",
            "loss: 0.07790688425302505\n",
            "loss: 0.07790639251470566\n",
            "loss: 0.07790576666593552\n",
            "loss: 0.07790530472993851\n",
            "loss: 0.0779050663113594\n",
            "loss: 0.07790457457304001\n",
            "loss: 0.077904112637043\n",
            "loss: 0.07790358364582062\n",
            "loss: 0.07790303975343704\n",
            "loss: 0.07790260016918182\n",
            "loss: 0.07790207117795944\n",
            "loss: 0.077901691198349\n",
            "loss: 0.07790118455886841\n",
            "loss: 0.07790069282054901\n",
            "loss: 0.07790017873048782\n",
            "loss: 0.0778997614979744\n",
            "loss: 0.07789923995733261\n",
            "loss: 0.07789883017539978\n",
            "loss: 0.07789837568998337\n",
            "loss: 0.07789797335863113\n",
            "loss: 0.07789748162031174\n",
            "loss: 0.07789703458547592\n",
            "loss: 0.077896349132061\n",
            "loss: 0.07789599895477295\n",
            "loss: 0.07789552956819534\n",
            "loss: 0.07789509743452072\n",
            "loss: 0.07789449393749237\n",
            "loss: 0.07789407670497894\n",
            "loss: 0.07789363712072372\n",
            "loss: 0.07789310812950134\n",
            "loss: 0.07789269089698792\n",
            "loss: 0.07789207249879837\n",
            "loss: 0.07789187133312225\n",
            "loss: 0.07789131999015808\n",
            "loss: 0.0778907909989357\n",
            "loss: 0.07789036631584167\n",
            "loss: 0.07788996398448944\n",
            "loss: 0.07788929343223572\n",
            "loss: 0.07788872718811035\n",
            "loss: 0.07788826525211334\n",
            "loss: 0.07788792252540588\n",
            "loss: 0.07788744568824768\n",
            "loss: 0.07788701355457306\n",
            "loss: 0.07788640260696411\n",
            "loss: 0.07788600772619247\n",
            "loss: 0.07788541913032532\n",
            "loss: 0.07788508385419846\n",
            "loss: 0.07788442820310593\n",
            "loss: 0.07788417488336563\n",
            "loss: 0.07788369059562683\n",
            "loss: 0.07788325846195221\n",
            "loss: 0.07788275927305222\n",
            "loss: 0.07788234949111938\n",
            "loss: 0.0778818279504776\n",
            "loss: 0.07788144052028656\n",
            "loss: 0.0778811052441597\n",
            "loss: 0.07788081467151642\n",
            "loss: 0.0778806060552597\n",
            "loss: 0.07788056135177612\n",
            "loss: 0.07788047194480896\n",
            "loss: 0.07788094133138657\n",
            "loss: 0.07788219302892685\n",
            "loss: 0.07788392901420593\n",
            "loss: 0.0778869017958641\n",
            "loss: 0.07789206504821777\n",
            "loss: 0.07790062576532364\n",
            "loss: 0.07791431248188019\n",
            "loss: 0.07793684303760529\n",
            "loss: 0.0779726654291153\n",
            "loss: 0.0780298039317131\n",
            "loss: 0.07811927050352097\n",
            "loss: 0.07825659960508347\n",
            "loss: 0.07845520228147507\n",
            "loss: 0.07872390002012253\n",
            "loss: 0.07903027534484863\n",
            "loss: 0.07929474860429764\n",
            "loss: 0.07935313135385513\n",
            "loss: 0.07909549027681351\n",
            "loss: 0.07856906950473785\n",
            "loss: 0.07806562632322311\n",
            "loss: 0.07786951214075089\n",
            "loss: 0.0780242457985878\n",
            "loss: 0.07832285016775131\n",
            "loss: 0.07848870754241943\n",
            "loss: 0.07838665693998337\n",
            "loss: 0.07810987532138824\n",
            "loss: 0.07789531350135803\n",
            "loss: 0.0778917595744133\n",
            "loss: 0.07804248481988907\n",
            "loss: 0.07817032933235168\n",
            "loss: 0.0781492292881012\n",
            "loss: 0.07800932228565216\n",
            "loss: 0.07788456231355667\n",
            "loss: 0.07787417620420456\n",
            "loss: 0.07795462757349014\n",
            "loss: 0.0780249834060669\n",
            "loss: 0.07801295816898346\n",
            "loss: 0.07793599367141724\n",
            "loss: 0.07787048816680908\n",
            "loss: 0.0778677687048912\n",
            "loss: 0.07791273295879364\n",
            "loss: 0.07794862240552902\n",
            "loss: 0.07793857902288437\n",
            "loss: 0.07789599895477295\n",
            "loss: 0.07786199450492859\n",
            "loss: 0.07786287367343903\n",
            "loss: 0.0778871551156044\n",
            "loss: 0.07790546864271164\n",
            "loss: 0.07789880782365799\n",
            "loss: 0.07787549495697021\n",
            "loss: 0.07785732299089432\n",
            "loss: 0.07785765826702118\n",
            "loss: 0.07787057757377625\n",
            "loss: 0.0778803750872612\n",
            "loss: 0.07787705212831497\n",
            "loss: 0.07786455750465393\n",
            "loss: 0.07785424590110779\n",
            "loss: 0.07785335928201675\n",
            "loss: 0.07785972952842712\n",
            "loss: 0.07786493748426437\n",
            "loss: 0.07786408066749573\n",
            "loss: 0.07785780727863312\n",
            "loss: 0.07785144448280334\n",
            "loss: 0.0778498575091362\n",
            "loss: 0.07785239070653915\n",
            "loss: 0.07785532623529434\n",
            "loss: 0.07785575836896896\n",
            "loss: 0.07785254716873169\n",
            "loss: 0.07784906029701233\n",
            "loss: 0.07784681767225266\n",
            "loss: 0.07784736156463623\n",
            "loss: 0.07784893363714218\n",
            "loss: 0.07784963399171829\n",
            "loss: 0.07784853875637054\n",
            "loss: 0.07784625142812729\n",
            "loss: 0.07784440368413925\n",
            "loss: 0.07784396409988403\n",
            "loss: 0.07784441858530045\n",
            "loss: 0.07784461230039597\n",
            "loss: 0.07784440368413925\n",
            "loss: 0.07784325629472733\n",
            "loss: 0.07784198969602585\n",
            "loss: 0.07784103602170944\n",
            "loss: 0.07784075289964676\n",
            "loss: 0.07784067839384079\n",
            "loss: 0.07784068584442139\n",
            "loss: 0.07784020155668259\n",
            "loss: 0.07783930003643036\n",
            "loss: 0.07783851772546768\n",
            "loss: 0.07783780992031097\n",
            "loss: 0.0778375044465065\n",
            "loss: 0.07783732563257217\n",
            "loss: 0.07783696055412292\n",
            "loss: 0.07783640921115875\n",
            "loss: 0.07783570140600204\n",
            "loss: 0.07783517241477966\n",
            "loss: 0.07783466577529907\n",
            "loss: 0.07783427834510803\n",
            "loss: 0.07783375680446625\n",
            "loss: 0.07783345133066177\n",
            "loss: 0.07783301174640656\n",
            "loss: 0.07783255726099014\n",
            "loss: 0.0778319388628006\n",
            "loss: 0.07783135026693344\n",
            "loss: 0.07783088088035583\n",
            "loss: 0.07783053070306778\n",
            "loss: 0.07782997936010361\n",
            "loss: 0.07782966643571854\n",
            "loss: 0.07782921940088272\n",
            "loss: 0.07782865315675735\n",
            "loss: 0.07782828062772751\n",
            "loss: 0.07782766222953796\n",
            "loss: 0.0778273269534111\n",
            "loss: 0.07782681286334991\n",
            "loss: 0.07782645523548126\n",
            "loss: 0.0778258889913559\n",
            "loss: 0.07782557606697083\n",
            "loss: 0.07782506942749023\n",
            "loss: 0.0778244212269783\n",
            "loss: 0.0778239369392395\n",
            "loss: 0.07782354950904846\n",
            "loss: 0.07782323658466339\n",
            "loss: 0.07782259583473206\n",
            "loss: 0.07782227545976639\n",
            "loss: 0.07782182842493057\n",
            "loss: 0.07782134413719177\n",
            "loss: 0.07782077044248581\n",
            "loss: 0.07782034575939178\n",
            "loss: 0.07781995087862015\n",
            "loss: 0.07781944423913956\n",
            "loss: 0.07781911641359329\n",
            "loss: 0.07781854271888733\n",
            "loss: 0.07781816273927689\n",
            "loss: 0.07781770080327988\n",
            "loss: 0.07781718671321869\n",
            "loss: 0.07781675457954407\n",
            "loss: 0.07781632989645004\n",
            "loss: 0.07781579345464706\n",
            "loss: 0.07781542092561722\n",
            "loss: 0.07781486958265305\n",
            "loss: 0.0778145045042038\n",
            "loss: 0.07781412452459335\n",
            "loss: 0.07781355082988739\n",
            "loss: 0.07781297713518143\n",
            "loss: 0.07781253755092621\n",
            "loss: 0.07781209796667099\n",
            "loss: 0.07781175523996353\n",
            "loss: 0.07781121879816055\n",
            "loss: 0.07781077921390533\n",
            "loss: 0.07781018316745758\n",
            "loss: 0.07780978828668594\n",
            "loss: 0.07780943810939789\n",
            "loss: 0.07780890166759491\n",
            "loss: 0.07780846208333969\n",
            "loss: 0.07780791074037552\n",
            "loss: 0.07780749350786209\n",
            "loss: 0.0778069943189621\n",
            "loss: 0.07780667394399643\n",
            "loss: 0.07780620455741882\n",
            "loss: 0.07780561596155167\n",
            "loss: 0.07780523598194122\n",
            "loss: 0.077804796397686\n",
            "loss: 0.07780426740646362\n",
            "loss: 0.07780369371175766\n",
            "loss: 0.07780320197343826\n",
            "loss: 0.07780279964208603\n",
            "loss: 0.07780233025550842\n",
            "loss: 0.0778018981218338\n",
            "loss: 0.07780145108699799\n",
            "loss: 0.07780087739229202\n",
            "loss: 0.07780059427022934\n",
            "loss: 0.07780006527900696\n",
            "loss: 0.0777994841337204\n",
            "loss: 0.07779921591281891\n",
            "loss: 0.07779865711927414\n",
            "loss: 0.07779811322689056\n",
            "loss: 0.07779752463102341\n",
            "loss: 0.07779723405838013\n",
            "loss: 0.07779694348573685\n",
            "loss: 0.07779641449451447\n",
            "loss: 0.0777958482503891\n",
            "loss: 0.07779540121555328\n",
            "loss: 0.0777948647737503\n",
            "loss: 0.0777943879365921\n",
            "loss: 0.07779392600059509\n",
            "loss: 0.07779345661401749\n",
            "loss: 0.07779301702976227\n",
            "loss: 0.07779248058795929\n",
            "loss: 0.07779202610254288\n",
            "loss: 0.07779165357351303\n",
            "loss: 0.0777912586927414\n",
            "loss: 0.07779072970151901\n",
            "loss: 0.07779015600681305\n",
            "loss: 0.07778969407081604\n",
            "loss: 0.07778920233249664\n",
            "loss: 0.077788807451725\n",
            "loss: 0.07778824120759964\n",
            "loss: 0.07778777182102203\n",
            "loss: 0.07778733223676682\n",
            "loss: 0.07778695225715637\n",
            "loss: 0.07778635621070862\n",
            "loss: 0.07778596878051758\n",
            "loss: 0.0777854472398758\n",
            "loss: 0.07778502255678177\n",
            "loss: 0.07778458297252655\n",
            "loss: 0.0777839794754982\n",
            "loss: 0.07778352499008179\n",
            "loss: 0.07778304070234299\n",
            "loss: 0.07778265327215195\n",
            "loss: 0.07778239250183105\n",
            "loss: 0.07778171449899673\n",
            "loss: 0.07778146117925644\n",
            "loss: 0.07778090238571167\n",
            "loss: 0.0777803584933281\n",
            "loss: 0.07778005301952362\n",
            "loss: 0.07777974009513855\n",
            "loss: 0.07777929306030273\n",
            "loss: 0.07777903228998184\n",
            "loss: 0.07777877897024155\n",
            "loss: 0.07777884602546692\n",
            "loss: 0.07777900993824005\n",
            "loss: 0.07777933776378632\n",
            "loss: 0.07778018712997437\n",
            "loss: 0.07778170704841614\n",
            "loss: 0.07778415828943253\n",
            "loss: 0.07778819650411606\n",
            "loss: 0.07779477536678314\n",
            "loss: 0.07780495285987854\n",
            "loss: 0.07782068103551865\n",
            "loss: 0.07784568518400192\n",
            "loss: 0.077884241938591\n",
            "loss: 0.07794322073459625\n",
            "loss: 0.07803171128034592\n",
            "loss: 0.07816158980131149\n",
            "loss: 0.0783395767211914\n",
            "loss: 0.07856686413288116\n",
            "loss: 0.0788067877292633\n",
            "loss: 0.07899132370948792\n",
            "loss: 0.07899995893239975\n",
            "loss: 0.0787687823176384\n",
            "loss: 0.07834836840629578\n",
            "loss: 0.07794895023107529\n",
            "loss: 0.07776958495378494\n",
            "loss: 0.07785435020923615\n",
            "loss: 0.07807654142379761\n",
            "loss: 0.07824532687664032\n",
            "loss: 0.07823549211025238\n",
            "loss: 0.07805893570184708\n",
            "loss: 0.07785490155220032\n",
            "loss: 0.07776391506195068\n",
            "loss: 0.0778188407421112\n",
            "loss: 0.0779375359416008\n",
            "loss: 0.0780051052570343\n",
            "loss: 0.07796761393547058\n",
            "loss: 0.07786215841770172\n",
            "loss: 0.07777609676122665\n",
            "loss: 0.07776637375354767\n",
            "loss: 0.07781881839036942\n",
            "loss: 0.0778728798031807\n",
            "loss: 0.07787907123565674\n",
            "loss: 0.07783540338277817\n",
            "loss: 0.07778117060661316\n",
            "loss: 0.07775765657424927\n",
            "loss: 0.07777359336614609\n",
            "loss: 0.07780500501394272\n",
            "loss: 0.0778202936053276\n",
            "loss: 0.07780735194683075\n",
            "loss: 0.07777879387140274\n",
            "loss: 0.07775775343179703\n",
            "loss: 0.07775705307722092\n",
            "loss: 0.07777107506990433\n",
            "loss: 0.07778409123420715\n",
            "loss: 0.07778476923704147\n",
            "loss: 0.07777287065982819\n",
            "loss: 0.07775860279798508\n",
            "loss: 0.07775168865919113\n",
            "loss: 0.07775498181581497\n",
            "loss: 0.07776270061731339\n",
            "loss: 0.07776714861392975\n",
            "loss: 0.07776470482349396\n",
            "loss: 0.07775750011205673\n",
            "loss: 0.07775094360113144\n",
            "loss: 0.07774854451417923\n",
            "loss: 0.07775084674358368\n",
            "loss: 0.0777541995048523\n",
            "loss: 0.07775551080703735\n",
            "loss: 0.07775353640317917\n",
            "loss: 0.07774976640939713\n",
            "loss: 0.07774659246206284\n",
            "loss: 0.07774543017148972\n",
            "loss: 0.07774626463651657\n",
            "loss: 0.07774752378463745\n",
            "loss: 0.07774821668863297\n",
            "loss: 0.07774679362773895\n",
            "loss: 0.07774502784013748\n",
            "loss: 0.07774321734905243\n",
            "loss: 0.07774234563112259\n",
            "loss: 0.0777423083782196\n",
            "loss: 0.07774270325899124\n",
            "loss: 0.0777428075671196\n",
            "loss: 0.07774224132299423\n",
            "loss: 0.07774131000041962\n",
            "loss: 0.07773985713720322\n",
            "loss: 0.07773901522159576\n",
            "loss: 0.07773881405591965\n",
            "loss: 0.07773849368095398\n",
            "loss: 0.07773841172456741\n",
            "loss: 0.07773825526237488\n",
            "loss: 0.07773751765489578\n",
            "loss: 0.07773682475090027\n",
            "loss: 0.07773596048355103\n",
            "loss: 0.07773543149232864\n",
            "loss: 0.07773497700691223\n",
            "loss: 0.07773473113775253\n",
            "loss: 0.07773444801568985\n",
            "loss: 0.07773413509130478\n",
            "loss: 0.07773353159427643\n",
            "loss: 0.07773293554782867\n",
            "loss: 0.07773226499557495\n",
            "loss: 0.07773181796073914\n",
            "loss: 0.07773131132125854\n",
            "loss: 0.07773097604513168\n",
            "loss: 0.07773042470216751\n",
            "loss: 0.07773005962371826\n",
            "loss: 0.07772962749004364\n",
            "loss: 0.07772906124591827\n",
            "loss: 0.07772865146398544\n",
            "loss: 0.07772817462682724\n",
            "loss: 0.07772768288850784\n",
            "loss: 0.07772725820541382\n",
            "loss: 0.07772672921419144\n",
            "loss: 0.07772621512413025\n",
            "loss: 0.07772595435380936\n",
            "loss: 0.07772551476955414\n",
            "loss: 0.07772491127252579\n",
            "loss: 0.07772441953420639\n",
            "loss: 0.0777239203453064\n",
            "loss: 0.07772351056337357\n",
            "loss: 0.07772301137447357\n",
            "loss: 0.07772257179021835\n",
            "loss: 0.07772216945886612\n",
            "loss: 0.0777217298746109\n",
            "loss: 0.07772111147642136\n",
            "loss: 0.07772068679332733\n",
            "loss: 0.07772032916545868\n",
            "loss: 0.07771982252597809\n",
            "loss: 0.07771936058998108\n",
            "loss: 0.07771876454353333\n",
            "loss: 0.07771839946508408\n",
            "loss: 0.07771798223257065\n",
            "loss: 0.07771745324134827\n",
            "loss: 0.0777169018983841\n",
            "loss: 0.0777166485786438\n",
            "loss: 0.07771614193916321\n",
            "loss: 0.07771561294794083\n",
            "loss: 0.07771532237529755\n",
            "loss: 0.07771466672420502\n",
            "loss: 0.07771426439285278\n",
            "loss: 0.07771376520395279\n",
            "loss: 0.07771341502666473\n",
            "loss: 0.07771284133195877\n",
            "loss: 0.07771242409944534\n",
            "loss: 0.07771185040473938\n",
            "loss: 0.07771160453557968\n",
            "loss: 0.07771109789609909\n",
            "loss: 0.07771056890487671\n",
            "loss: 0.07771001011133194\n",
            "loss: 0.07770971208810806\n",
            "loss: 0.07770918309688568\n",
            "loss: 0.07770862430334091\n",
            "loss: 0.0777081549167633\n",
            "loss: 0.0777076780796051\n",
            "loss: 0.07770717144012451\n",
            "loss: 0.07770676910877228\n",
            "loss: 0.07770639657974243\n",
            "loss: 0.07770594209432602\n",
            "loss: 0.07770538330078125\n",
            "loss: 0.07770487666130066\n",
            "loss: 0.07770445942878723\n",
            "loss: 0.07770389318466187\n",
            "loss: 0.077703557908535\n",
            "loss: 0.07770295441150665\n",
            "loss: 0.0777026116847992\n",
            "loss: 0.07770216464996338\n",
            "loss: 0.07770176976919174\n",
            "loss: 0.07770125567913055\n",
            "loss: 0.07770080119371414\n",
            "loss: 0.07770028710365295\n",
            "loss: 0.07769978046417236\n",
            "loss: 0.07769928127527237\n",
            "loss: 0.0776989683508873\n",
            "loss: 0.07769840210676193\n",
            "loss: 0.07769808918237686\n",
            "loss: 0.07769736647605896\n",
            "loss: 0.07769694924354553\n",
            "loss: 0.07769646495580673\n",
            "loss: 0.07769602537155151\n",
            "loss: 0.07769564539194107\n",
            "loss: 0.07769512385129929\n",
            "loss: 0.07769476622343063\n",
            "loss: 0.07769423723220825\n",
            "loss: 0.07769378274679184\n",
            "loss: 0.07769346982240677\n",
            "loss: 0.07769288867712021\n",
            "loss: 0.07769247889518738\n",
            "loss: 0.07769212126731873\n",
            "loss: 0.07769174128770828\n",
            "loss: 0.07769130170345306\n",
            "loss: 0.07769087702035904\n",
            "loss: 0.07769064605236053\n",
            "loss: 0.07769028842449188\n",
            "loss: 0.07769003510475159\n",
            "loss: 0.07768987864255905\n",
            "loss: 0.07768991589546204\n",
            "loss: 0.0776902362704277\n",
            "loss: 0.07769062370061874\n",
            "loss: 0.07769162207841873\n",
            "loss: 0.07769326120615005\n",
            "loss: 0.07769586890935898\n",
            "loss: 0.07769960910081863\n",
            "loss: 0.077705517411232\n",
            "loss: 0.07771441340446472\n",
            "loss: 0.07772817462682724\n",
            "loss: 0.07774852216243744\n",
            "loss: 0.07777906954288483\n",
            "loss: 0.0778237134218216\n",
            "loss: 0.07788915187120438\n",
            "loss: 0.07798158377408981\n",
            "loss: 0.07810801267623901\n",
            "loss: 0.07826678454875946\n",
            "loss: 0.07844772189855576\n",
            "loss: 0.07860829681158066\n",
            "loss: 0.07869021594524384\n",
            "loss: 0.07861792296171188\n",
            "loss: 0.07838290929794312\n",
            "loss: 0.07805623859167099\n",
            "loss: 0.07778403162956238\n",
            "loss: 0.07767748087644577\n",
            "loss: 0.07774601131677628\n",
            "loss: 0.07790399342775345\n",
            "loss: 0.07803276181221008\n",
            "loss: 0.07805001735687256\n",
            "loss: 0.07794792205095291\n",
            "loss: 0.07779776304960251\n",
            "loss: 0.07769189029932022\n",
            "loss: 0.07768034189939499\n",
            "loss: 0.07774406671524048\n",
            "loss: 0.07781969755887985\n",
            "loss: 0.0778491199016571\n",
            "loss: 0.07781385630369186\n",
            "loss: 0.07774294912815094\n",
            "loss: 0.07768483459949493\n",
            "loss: 0.07767150551080704\n",
            "loss: 0.07769845426082611\n",
            "loss: 0.0777362734079361\n",
            "loss: 0.07775377482175827\n",
            "loss: 0.07774006575345993\n",
            "loss: 0.07770656794309616\n",
            "loss: 0.07767680287361145\n",
            "loss: 0.07766697555780411\n",
            "loss: 0.07767751067876816\n",
            "loss: 0.07769550383090973\n",
            "loss: 0.07770596444606781\n",
            "loss: 0.07770183682441711\n",
            "loss: 0.07768692076206207\n",
            "loss: 0.07767141610383987\n",
            "loss: 0.07766390591859818\n",
            "loss: 0.07766647636890411\n",
            "loss: 0.07767441123723984\n",
            "loss: 0.07768062502145767\n",
            "loss: 0.07768112421035767\n",
            "loss: 0.07767526805400848\n",
            "loss: 0.07766745239496231\n",
            "loss: 0.07766173779964447\n",
            "loss: 0.077660471200943\n",
            "loss: 0.07766300439834595\n",
            "loss: 0.07766631245613098\n",
            "loss: 0.07766822725534439\n",
            "loss: 0.07766684144735336\n",
            "loss: 0.07766339182853699\n",
            "loss: 0.0776597410440445\n",
            "loss: 0.07765723019838333\n",
            "loss: 0.07765671610832214\n",
            "loss: 0.07765784114599228\n",
            "loss: 0.07765914499759674\n",
            "loss: 0.07765941321849823\n",
            "loss: 0.07765847444534302\n",
            "loss: 0.07765690982341766\n",
            "loss: 0.0776548981666565\n",
            "loss: 0.0776534155011177\n",
            "loss: 0.07765309512615204\n",
            "loss: 0.07765302807092667\n",
            "loss: 0.0776534304022789\n",
            "loss: 0.0776534155011177\n",
            "loss: 0.07765282690525055\n",
            "loss: 0.0776519849896431\n",
            "loss: 0.07765081524848938\n",
            "loss: 0.07764999568462372\n",
            "loss: 0.07764926552772522\n",
            "loss: 0.07764878123998642\n",
            "loss: 0.07764875143766403\n",
            "loss: 0.07764864712953568\n",
            "loss: 0.07764828205108643\n",
            "loss: 0.07764768600463867\n",
            "loss: 0.07764711230993271\n",
            "loss: 0.0776463970541954\n",
            "loss: 0.0776456892490387\n",
            "loss: 0.07764516770839691\n",
            "loss: 0.07764481753110886\n",
            "loss: 0.07764438539743423\n",
            "loss: 0.07764394581317902\n",
            "loss: 0.07764366269111633\n",
            "loss: 0.07764330506324768\n",
            "loss: 0.07764267176389694\n",
            "loss: 0.07764221727848053\n",
            "loss: 0.07764169573783875\n",
            "loss: 0.07764113694429398\n",
            "loss: 0.07764056324958801\n",
            "loss: 0.07763999700546265\n",
            "loss: 0.07763949036598206\n",
            "loss: 0.07763922959566116\n",
            "loss: 0.07763877511024475\n",
            "loss: 0.07763839513063431\n",
            "loss: 0.07763795554637909\n",
            "loss: 0.07763742655515671\n",
            "loss: 0.07763706892728806\n",
            "loss: 0.0776364877820015\n",
            "loss: 0.07763604074716568\n",
            "loss: 0.07763556391000748\n",
            "loss: 0.07763514667749405\n",
            "loss: 0.07763466238975525\n",
            "loss: 0.07763415575027466\n",
            "loss: 0.07763369381427765\n",
            "loss: 0.07763326913118362\n",
            "loss: 0.07763267308473587\n",
            "loss: 0.07763226330280304\n",
            "loss: 0.07763192802667618\n",
            "loss: 0.07763130962848663\n",
            "loss: 0.0776308923959732\n",
            "loss: 0.077630415558815\n",
            "loss: 0.0776299312710762\n",
            "loss: 0.07762953639030457\n",
            "loss: 0.07762900739908218\n",
            "loss: 0.07762864977121353\n",
            "loss: 0.07762815803289413\n",
            "loss: 0.07762766629457474\n",
            "loss: 0.07762716710567474\n",
            "loss: 0.07762663066387177\n",
            "loss: 0.07762626558542252\n",
            "loss: 0.07762569934129715\n",
            "loss: 0.07762530446052551\n",
            "loss: 0.07762490957975388\n",
            "loss: 0.07762446254491806\n",
            "loss: 0.0776238739490509\n",
            "loss: 0.07762344181537628\n",
            "loss: 0.07762310653924942\n",
            "loss: 0.07762247323989868\n",
            "loss: 0.07762211561203003\n",
            "loss: 0.07762157171964645\n",
            "loss: 0.07762112468481064\n",
            "loss: 0.07762063294649124\n",
            "loss: 0.07762019336223602\n",
            "loss: 0.07761961221694946\n",
            "loss: 0.0776192769408226\n",
            "loss: 0.07761876285076141\n",
            "loss: 0.07761839777231216\n",
            "loss: 0.07761777937412262\n",
            "loss: 0.0776173397898674\n",
            "loss: 0.07761695981025696\n",
            "loss: 0.07761656492948532\n",
            "loss: 0.07761603593826294\n",
            "loss: 0.07761562615633011\n",
            "loss: 0.07761504501104355\n",
            "loss: 0.07761452347040176\n",
            "loss: 0.07761402428150177\n",
            "loss: 0.07761352509260178\n",
            "loss: 0.07761333137750626\n",
            "loss: 0.07761271297931671\n",
            "loss: 0.07761218398809433\n",
            "loss: 0.07761182636022568\n",
            "loss: 0.07761126756668091\n",
            "loss: 0.07761092483997345\n",
            "loss: 0.07761051505804062\n",
            "loss: 0.07761012762784958\n",
            "loss: 0.07760967314243317\n",
            "loss: 0.07760926336050034\n",
            "loss: 0.0776088759303093\n",
            "loss: 0.07760867476463318\n",
            "loss: 0.07760834693908691\n",
            "loss: 0.07760823518037796\n",
            "loss: 0.07760817557573318\n",
            "loss: 0.07760841399431229\n",
            "loss: 0.07760903239250183\n",
            "loss: 0.07761004567146301\n",
            "loss: 0.0776117816567421\n",
            "loss: 0.07761483639478683\n",
            "loss: 0.07761981338262558\n",
            "loss: 0.07762788981199265\n",
            "loss: 0.07764030247926712\n",
            "loss: 0.07765983790159225\n",
            "loss: 0.07769069075584412\n",
            "loss: 0.07773834466934204\n",
            "loss: 0.07781152427196503\n",
            "loss: 0.07792031764984131\n",
            "loss: 0.07807693630456924\n",
            "loss: 0.07828493416309357\n",
            "loss: 0.0785340815782547\n",
            "loss: 0.07876475900411606\n",
            "loss: 0.07888474315404892\n",
            "loss: 0.07877098768949509\n",
            "loss: 0.07841718941926956\n",
            "loss: 0.07796284556388855\n",
            "loss: 0.07765057682991028\n",
            "loss: 0.07761742174625397\n",
            "loss: 0.07780638337135315\n",
            "loss: 0.07803113013505936\n",
            "loss: 0.0781107023358345\n",
            "loss: 0.07799156755208969\n",
            "loss: 0.0777696818113327\n",
            "loss: 0.0776127278804779\n",
            "loss: 0.07761295139789581\n",
            "loss: 0.07772567123174667\n",
            "loss: 0.07782973349094391\n",
            "loss: 0.07783201336860657\n",
            "loss: 0.07773788273334503\n",
            "loss: 0.07762996852397919\n",
            "loss: 0.07759025692939758\n",
            "loss: 0.07762998342514038\n",
            "loss: 0.07769408077001572\n",
            "loss: 0.0777193009853363\n",
            "loss: 0.07768573611974716\n",
            "loss: 0.07762607932090759\n",
            "loss: 0.07758964598178864\n",
            "loss: 0.07759702950716019\n",
            "loss: 0.07763008028268814\n",
            "loss: 0.07765281945466995\n",
            "loss: 0.07764575630426407\n",
            "loss: 0.07761657238006592\n",
            "loss: 0.07759058475494385\n",
            "loss: 0.0775853842496872\n",
            "loss: 0.07759897410869598\n",
            "loss: 0.07761480659246445\n",
            "loss: 0.07761763781309128\n",
            "loss: 0.07760624587535858\n",
            "loss: 0.07759030163288116\n",
            "loss: 0.07758177816867828\n",
            "loss: 0.0775846540927887\n",
            "loss: 0.07759302109479904\n",
            "loss: 0.07759848982095718\n",
            "loss: 0.07759619504213333\n",
            "loss: 0.07758817076683044\n",
            "loss: 0.07758097350597382\n",
            "loss: 0.07757843285799026\n",
            "loss: 0.07758118212223053\n",
            "loss: 0.07758509367704391\n",
            "loss: 0.07758648693561554\n",
            "loss: 0.07758413255214691\n",
            "loss: 0.07757987082004547\n",
            "loss: 0.0775764212012291\n",
            "loss: 0.07757557183504105\n",
            "loss: 0.07757695019245148\n",
            "loss: 0.07757843285799026\n",
            "loss: 0.07757869362831116\n",
            "loss: 0.07757717370986938\n",
            "loss: 0.07757487893104553\n",
            "loss: 0.07757293432950974\n",
            "loss: 0.07757233828306198\n",
            "loss: 0.07757265865802765\n",
            "loss: 0.0775732547044754\n",
            "loss: 0.07757321000099182\n",
            "loss: 0.07757247239351273\n",
            "loss: 0.07757113128900528\n",
            "loss: 0.07756981253623962\n",
            "loss: 0.07756920158863068\n",
            "loss: 0.07756878435611725\n",
            "loss: 0.07756888121366501\n",
            "loss: 0.07756879925727844\n",
            "loss: 0.07756839692592621\n",
            "loss: 0.077567458152771\n",
            "loss: 0.07756652683019638\n",
            "loss: 0.07756602764129639\n",
            "loss: 0.07756545394659042\n",
            "loss: 0.07756519317626953\n",
            "loss: 0.0775650292634964\n",
            "loss: 0.07756469398736954\n",
            "loss: 0.07756424695253372\n",
            "loss: 0.07756343483924866\n",
            "loss: 0.07756295055150986\n",
            "loss: 0.07756244391202927\n",
            "loss: 0.07756173610687256\n",
            "loss: 0.07756143808364868\n",
            "loss: 0.0775611475110054\n",
            "loss: 0.07756068557500839\n",
            "loss: 0.07756030559539795\n",
            "loss: 0.07755967974662781\n",
            "loss: 0.07755916565656662\n",
            "loss: 0.07755881547927856\n",
            "loss: 0.0775582417845726\n",
            "loss: 0.0775577649474144\n",
            "loss: 0.07755746692419052\n",
            "loss: 0.0775570198893547\n",
            "loss: 0.0775565356016159\n",
            "loss: 0.07755599170923233\n",
            "loss: 0.0775555670261383\n",
            "loss: 0.07755505293607712\n",
            "loss: 0.07755456119775772\n",
            "loss: 0.07755410671234131\n",
            "loss: 0.07755371183156967\n",
            "loss: 0.07755322754383087\n",
            "loss: 0.07755285501480103\n",
            "loss: 0.07755234837532043\n",
            "loss: 0.07755181938409805\n",
            "loss: 0.07755150645971298\n",
            "loss: 0.0775509849190712\n",
            "loss: 0.07755047082901001\n",
            "loss: 0.07755003869533539\n",
            "loss: 0.07754955440759659\n",
            "loss: 0.07754925638437271\n",
            "loss: 0.07754868268966675\n",
            "loss: 0.07754812389612198\n",
            "loss: 0.07754764705896378\n",
            "loss: 0.07754721492528915\n",
            "loss: 0.07754675298929214\n",
            "loss: 0.07754640281200409\n",
            "loss: 0.07754585146903992\n",
            "loss: 0.07754535973072052\n",
            "loss: 0.0775449275970459\n",
            "loss: 0.0775444284081459\n",
            "loss: 0.0775439441204071\n",
            "loss: 0.07754351943731308\n",
            "loss: 0.07754309475421906\n",
            "loss: 0.07754269987344742\n",
            "loss: 0.07754231989383698\n",
            "loss: 0.07754180580377579\n",
            "loss: 0.07754134386777878\n",
            "loss: 0.07754099369049072\n",
            "loss: 0.07754043489694595\n",
            "loss: 0.07753994315862656\n",
            "loss: 0.07753945142030716\n",
            "loss: 0.07753915339708328\n",
            "loss: 0.07753860205411911\n",
            "loss: 0.0775381326675415\n",
            "loss: 0.07753754407167435\n",
            "loss: 0.07753720134496689\n",
            "loss: 0.07753676921129227\n",
            "loss: 0.07753616571426392\n",
            "loss: 0.07753579318523407\n",
            "loss: 0.07753534615039825\n",
            "loss: 0.07753482460975647\n",
            "loss: 0.07753458619117737\n",
            "loss: 0.07753396779298782\n",
            "loss: 0.07753351330757141\n",
            "loss: 0.07753296196460724\n",
            "loss: 0.0775325745344162\n",
            "loss: 0.07753214985132217\n",
            "loss: 0.07753163576126099\n",
            "loss: 0.07753127813339233\n",
            "loss: 0.07753075659275055\n",
            "loss: 0.0775303989648819\n",
            "loss: 0.07752984017133713\n",
            "loss: 0.07752940058708191\n",
            "loss: 0.07752886414527893\n",
            "loss: 0.07752837985754013\n",
            "loss: 0.07752806693315506\n",
            "loss: 0.07752742618322372\n",
            "loss: 0.07752707600593567\n",
            "loss: 0.07752668112516403\n",
            "loss: 0.07752607017755508\n",
            "loss: 0.07752574235200882\n",
            "loss: 0.07752526551485062\n",
            "loss: 0.07752475142478943\n",
            "loss: 0.07752423733472824\n",
            "loss: 0.07752383500337601\n",
            "loss: 0.07752330601215363\n",
            "loss: 0.07752298563718796\n",
            "loss: 0.07752255350351334\n",
            "loss: 0.07752200216054916\n",
            "loss: 0.07752152532339096\n",
            "loss: 0.0775211751461029\n",
            "loss: 0.07752054184675217\n",
            "loss: 0.0775199681520462\n",
            "loss: 0.0775197222828865\n",
            "loss: 0.07751929759979248\n",
            "loss: 0.07751878350973129\n",
            "loss: 0.07751832902431488\n",
            "loss: 0.0775180459022522\n",
            "loss: 0.07751744240522385\n",
            "loss: 0.0775170549750328\n",
            "loss: 0.07751674950122833\n",
            "loss: 0.07751653343439102\n",
            "loss: 0.07751616835594177\n",
            "loss: 0.07751616090536118\n",
            "loss: 0.07751618325710297\n",
            "loss: 0.07751638442277908\n",
            "loss: 0.07751699537038803\n",
            "loss: 0.07751831412315369\n",
            "loss: 0.07752010226249695\n",
            "loss: 0.07752355933189392\n",
            "loss: 0.07752865552902222\n",
            "loss: 0.07753691077232361\n",
            "loss: 0.07754974067211151\n",
            "loss: 0.07756991684436798\n",
            "loss: 0.07760079205036163\n",
            "loss: 0.07764869928359985\n",
            "loss: 0.0777205303311348\n",
            "loss: 0.07782703638076782\n",
            "loss: 0.07797762751579285\n",
            "loss: 0.07817891240119934\n",
            "loss: 0.07841305434703827\n",
            "loss: 0.07863501459360123\n",
            "loss: 0.07874156534671783\n",
            "loss: 0.07864127308130264\n",
            "loss: 0.07830450683832169\n",
            "loss: 0.07787437736988068\n",
            "loss: 0.07756656408309937\n",
            "loss: 0.07751795649528503\n",
            "loss: 0.07768532633781433\n",
            "loss: 0.07790050655603409\n",
            "loss: 0.07799547165632248\n",
            "loss: 0.07790146768093109\n",
            "loss: 0.07769778370857239\n",
            "loss: 0.07753359526395798\n",
            "loss: 0.07750974595546722\n",
            "loss: 0.07760348916053772\n",
            "loss: 0.07770996540784836\n",
            "loss: 0.07773473858833313\n",
            "loss: 0.07766174525022507\n",
            "loss: 0.07755695283412933\n",
            "loss: 0.07749979197978973\n",
            "loss: 0.07751934230327606\n",
            "loss: 0.07757840305566788\n",
            "loss: 0.07761683315038681\n",
            "loss: 0.07760211825370789\n",
            "loss: 0.07755061239004135\n",
            "loss: 0.07750527560710907\n",
            "loss: 0.07749678939580917\n",
            "loss: 0.07752081006765366\n",
            "loss: 0.07754892110824585\n",
            "loss: 0.0775548666715622\n",
            "loss: 0.0775352269411087\n",
            "loss: 0.07750765234231949\n",
            "loss: 0.07749280333518982\n",
            "loss: 0.07749742269515991\n",
            "loss: 0.07751253247261047\n",
            "loss: 0.07752251625061035\n",
            "loss: 0.07751909643411636\n",
            "loss: 0.07750590890645981\n",
            "loss: 0.07749316841363907\n",
            "loss: 0.07748930156230927\n",
            "loss: 0.07749392837285995\n",
            "loss: 0.07750124484300613\n",
            "loss: 0.07750408351421356\n",
            "loss: 0.07750051468610764\n",
            "loss: 0.07749331742525101\n",
            "loss: 0.0774875208735466\n",
            "loss: 0.0774862989783287\n",
            "loss: 0.07748892158269882\n",
            "loss: 0.07749203592538834\n",
            "loss: 0.07749264687299728\n",
            "loss: 0.07749027013778687\n",
            "loss: 0.07748661190271378\n",
            "loss: 0.07748371362686157\n",
            "loss: 0.07748313993215561\n",
            "loss: 0.0774838775396347\n",
            "loss: 0.0774851068854332\n",
            "loss: 0.07748528569936752\n",
            "loss: 0.07748422026634216\n",
            "loss: 0.07748221606016159\n",
            "loss: 0.07748053222894669\n",
            "loss: 0.07747985422611237\n",
            "loss: 0.07747970521450043\n",
            "loss: 0.07748032361268997\n",
            "loss: 0.0774802714586258\n",
            "loss: 0.07747966796159744\n",
            "loss: 0.07747865468263626\n",
            "loss: 0.07747755944728851\n",
            "loss: 0.07747668772935867\n",
            "loss: 0.07747635245323181\n",
            "loss: 0.07747609913349152\n",
            "loss: 0.07747585326433182\n",
            "loss: 0.07747574895620346\n",
            "loss: 0.07747510075569153\n",
            "loss: 0.07747438549995422\n",
            "loss: 0.07747368514537811\n",
            "loss: 0.07747315615415573\n",
            "loss: 0.07747270911931992\n",
            "loss: 0.07747229188680649\n",
            "loss: 0.07747194170951843\n",
            "loss: 0.07747156918048859\n",
            "loss: 0.07747115194797516\n",
            "loss: 0.077470563352108\n",
            "loss: 0.07746990770101547\n",
            "loss: 0.07746944576501846\n",
            "loss: 0.07746894657611847\n",
            "loss: 0.07746853679418564\n",
            "loss: 0.07746812701225281\n",
            "loss: 0.07746780663728714\n",
            "loss: 0.07746747136116028\n",
            "loss: 0.0774669423699379\n",
            "loss: 0.07746641337871552\n",
            "loss: 0.07746589928865433\n",
            "loss: 0.07746550440788269\n",
            "loss: 0.07746504247188568\n",
            "loss: 0.07746446132659912\n",
            "loss: 0.07746412605047226\n",
            "loss: 0.07746358215808868\n",
            "loss: 0.07746312022209167\n",
            "loss: 0.07746261358261108\n",
            "loss: 0.07746221870183945\n",
            "loss: 0.077461838722229\n",
            "loss: 0.07746138423681259\n",
            "loss: 0.07746084034442902\n",
            "loss: 0.07746033370494843\n",
            "loss: 0.07745993137359619\n",
            "loss: 0.07745955139398575\n",
            "loss: 0.07745900005102158\n",
            "loss: 0.07745860517024994\n",
            "loss: 0.07745813578367233\n",
            "loss: 0.07745774835348129\n",
            "loss: 0.07745712995529175\n",
            "loss: 0.07745672762393951\n",
            "loss: 0.07745636254549026\n",
            "loss: 0.07745583355426788\n",
            "loss: 0.07745539397001266\n",
            "loss: 0.07745486497879028\n",
            "loss: 0.07745440304279327\n",
            "loss: 0.07745400071144104\n",
            "loss: 0.07745359092950821\n",
            "loss: 0.07745315879583359\n",
            "loss: 0.07745269685983658\n",
            "loss: 0.07745227217674255\n",
            "loss: 0.07745163142681122\n",
            "loss: 0.07745125889778137\n",
            "loss: 0.07745087146759033\n",
            "loss: 0.07745049148797989\n",
            "loss: 0.07744976133108139\n",
            "loss: 0.07744938880205154\n",
            "loss: 0.07744910567998886\n",
            "loss: 0.07744850963354111\n",
            "loss: 0.07744807749986649\n",
            "loss: 0.07744762301445007\n",
            "loss: 0.07744718343019485\n",
            "loss: 0.07744677364826202\n",
            "loss: 0.07744629681110382\n",
            "loss: 0.07744572311639786\n",
            "loss: 0.07744546234607697\n",
            "loss: 0.07744493335485458\n",
            "loss: 0.0774444118142128\n",
            "loss: 0.07744402438402176\n",
            "loss: 0.07744340598583221\n",
            "loss: 0.07744316756725311\n",
            "loss: 0.07744266092777252\n",
            "loss: 0.07744217664003372\n",
            "loss: 0.07744156569242477\n",
            "loss: 0.07744120806455612\n",
            "loss: 0.07744066417217255\n",
            "loss: 0.0774403065443039\n",
            "loss: 0.07743965834379196\n",
            "loss: 0.07743939012289047\n",
            "loss: 0.07743880897760391\n",
            "loss: 0.07743845134973526\n",
            "loss: 0.07743805646896362\n",
            "loss: 0.07743746042251587\n",
            "loss: 0.07743703573942184\n",
            "loss: 0.07743659615516663\n",
            "loss: 0.07743615657091141\n",
            "loss: 0.07743563503026962\n",
            "loss: 0.07743527740240097\n",
            "loss: 0.07743469625711441\n",
            "loss: 0.07743426412343979\n",
            "loss: 0.07743389904499054\n",
            "loss: 0.07743340730667114\n",
            "loss: 0.07743292301893234\n",
            "loss: 0.07743243128061295\n",
            "loss: 0.07743193954229355\n",
            "loss: 0.07743140310049057\n",
            "loss: 0.07743097096681595\n",
            "loss: 0.07743056118488312\n",
            "loss: 0.07743007689714432\n",
            "loss: 0.07742957770824432\n",
            "loss: 0.07742911577224731\n",
            "loss: 0.07742860168218613\n",
            "loss: 0.07742820680141449\n",
            "loss: 0.07742778211832047\n",
            "loss: 0.07742736488580704\n",
            "loss: 0.07742689549922943\n",
            "loss: 0.07742638885974884\n",
            "loss: 0.07742596417665482\n",
            "loss: 0.07742545753717422\n",
            "loss: 0.0774250328540802\n",
            "loss: 0.07742477208375931\n",
            "loss: 0.0774243101477623\n",
            "loss: 0.07742404192686081\n",
            "loss: 0.07742378115653992\n",
            "loss: 0.07742366194725037\n",
            "loss: 0.07742380350828171\n",
            "loss: 0.07742410898208618\n",
            "loss: 0.07742495089769363\n",
            "loss: 0.07742657512426376\n",
            "loss: 0.07742929458618164\n",
            "loss: 0.07743373513221741\n",
            "loss: 0.07744140177965164\n",
            "loss: 0.07745403051376343\n",
            "loss: 0.07747402787208557\n",
            "loss: 0.07750668376684189\n",
            "loss: 0.07755874842405319\n",
            "loss: 0.07764091342687607\n",
            "loss: 0.07776826620101929\n",
            "loss: 0.07795561105012894\n",
            "loss: 0.07821498066186905\n",
            "loss: 0.07852423936128616\n",
            "loss: 0.07881581038236618\n",
            "loss: 0.07893106341362\n",
            "loss: 0.0787382423877716\n",
            "loss: 0.07823595404624939\n",
            "loss: 0.07769168168306351\n",
            "loss: 0.07742013782262802\n",
            "loss: 0.07752343267202377\n",
            "loss: 0.07782408595085144\n",
            "loss: 0.07803336530923843\n",
            "loss: 0.077974833548069\n",
            "loss: 0.07770469039678574\n",
            "loss: 0.07746024429798126\n",
            "loss: 0.07742302864789963\n",
            "loss: 0.07756567746400833\n",
            "loss: 0.07771080732345581\n",
            "loss: 0.07771125435829163\n",
            "loss: 0.07757644355297089\n",
            "loss: 0.07743866741657257\n",
            "loss: 0.07741313427686691\n",
            "loss: 0.07749025523662567\n",
            "loss: 0.07756819576025009\n",
            "loss: 0.07756482064723969\n",
            "loss: 0.07748885452747345\n",
            "loss: 0.07741755247116089\n",
            "loss: 0.07740980386734009\n",
            "loss: 0.07745429873466492\n",
            "loss: 0.07749351859092712\n",
            "loss: 0.07748626917600632\n",
            "loss: 0.0774432048201561\n",
            "loss: 0.0774068534374237\n",
            "loss: 0.07740605622529984\n",
            "loss: 0.07743105292320251\n",
            "loss: 0.07745040208101273\n",
            "loss: 0.07744422554969788\n",
            "loss: 0.07742022722959518\n",
            "loss: 0.0774015486240387\n",
            "loss: 0.07740180939435959\n",
            "loss: 0.07741513103246689\n",
            "loss: 0.07742515951395035\n",
            "loss: 0.07742123305797577\n",
            "loss: 0.07740816473960876\n",
            "loss: 0.0773978903889656\n",
            "loss: 0.07739757746458054\n",
            "loss: 0.07740437239408493\n",
            "loss: 0.07740969955921173\n",
            "loss: 0.07740813493728638\n",
            "loss: 0.0774013102054596\n",
            "loss: 0.07739512622356415\n",
            "loss: 0.07739394158124924\n",
            "loss: 0.0773969441652298\n",
            "loss: 0.07739990204572678\n",
            "loss: 0.07739982008934021\n",
            "loss: 0.07739642262458801\n",
            "loss: 0.07739268988370895\n",
            "loss: 0.07739098370075226\n",
            "loss: 0.07739172130823135\n",
            "loss: 0.07739332318305969\n",
            "loss: 0.07739362120628357\n",
            "loss: 0.07739239931106567\n",
            "loss: 0.07739008218050003\n",
            "loss: 0.07738842070102692\n",
            "loss: 0.07738814502954483\n",
            "loss: 0.0773887112736702\n",
            "loss: 0.07738890498876572\n",
            "loss: 0.07738858461380005\n",
            "loss: 0.07738742232322693\n",
            "loss: 0.07738599926233292\n",
            "loss: 0.07738523930311203\n",
            "loss: 0.07738502323627472\n",
            "loss: 0.07738516479730606\n",
            "loss: 0.07738517969846725\n",
            "loss: 0.07738453894853592\n",
            "loss: 0.0773836299777031\n",
            "loss: 0.07738286256790161\n",
            "loss: 0.07738210260868073\n",
            "loss: 0.07738199084997177\n",
            "loss: 0.07738158851861954\n",
            "loss: 0.07738140970468521\n",
            "loss: 0.07738087326288223\n",
            "loss: 0.0773802176117897\n",
            "loss: 0.07737953215837479\n",
            "loss: 0.07737912982702255\n",
            "loss: 0.0773787572979927\n",
            "loss: 0.07737839967012405\n",
            "loss: 0.07737793773412704\n",
            "loss: 0.07737751305103302\n",
            "loss: 0.07737698405981064\n",
            "loss: 0.07737640291452408\n",
            "loss: 0.07737607508897781\n",
            "loss: 0.07737541198730469\n",
            "loss: 0.0773751512169838\n",
            "loss: 0.07737472653388977\n",
            "loss: 0.0773741602897644\n",
            "loss: 0.07737379521131516\n",
            "loss: 0.07737327367067337\n",
            "loss: 0.07737284898757935\n",
            "loss: 0.07737231999635696\n",
            "loss: 0.0773719847202301\n",
            "loss: 0.0773715078830719\n",
            "loss: 0.07737115770578384\n",
            "loss: 0.07737071812152863\n",
            "loss: 0.07737015932798386\n",
            "loss: 0.07736977934837341\n",
            "loss: 0.07736925035715103\n",
            "loss: 0.0773688554763794\n",
            "loss: 0.07736831158399582\n",
            "loss: 0.07736792415380478\n",
            "loss: 0.07736743986606598\n",
            "loss: 0.07736697047948837\n",
            "loss: 0.07736668735742569\n",
            "loss: 0.07736602425575256\n",
            "loss: 0.07736576348543167\n",
            "loss: 0.07736518979072571\n",
            "loss: 0.07736476510763168\n",
            "loss: 0.07736440002918243\n",
            "loss: 0.07736393064260483\n",
            "loss: 0.07736342400312424\n",
            "loss: 0.07736288011074066\n",
            "loss: 0.0773625299334526\n",
            "loss: 0.0773620679974556\n",
            "loss: 0.0773615762591362\n",
            "loss: 0.07736125588417053\n",
            "loss: 0.07736072689294815\n",
            "loss: 0.07736019790172577\n",
            "loss: 0.0773598924279213\n",
            "loss: 0.07735934853553772\n",
            "loss: 0.07735900580883026\n",
            "loss: 0.0773584395647049\n",
            "loss: 0.07735802233219147\n",
            "loss: 0.07735765725374222\n",
            "loss: 0.07735707610845566\n",
            "loss: 0.07735651731491089\n",
            "loss: 0.07735631614923477\n",
            "loss: 0.0773557499051094\n",
            "loss: 0.07735540717840195\n",
            "loss: 0.0773548036813736\n",
            "loss: 0.07735446095466614\n",
            "loss: 0.07735389471054077\n",
            "loss: 0.07735350728034973\n",
            "loss: 0.07735316455364227\n",
            "loss: 0.07735246419906616\n",
            "loss: 0.07735227793455124\n",
            "loss: 0.0773516595363617\n",
            "loss: 0.07735127210617065\n",
            "loss: 0.07735079526901245\n",
            "loss: 0.07735016942024231\n",
            "loss: 0.07734982669353485\n",
            "loss: 0.07734932750463486\n",
            "loss: 0.07734888792037964\n",
            "loss: 0.07734842598438263\n",
            "loss: 0.07734794914722443\n",
            "loss: 0.07734749466180801\n",
            "loss: 0.07734716683626175\n",
            "loss: 0.07734662294387817\n",
            "loss: 0.07734623551368713\n",
            "loss: 0.07734578847885132\n",
            "loss: 0.0773453563451767\n",
            "loss: 0.07734489440917969\n",
            "loss: 0.07734431326389313\n",
            "loss: 0.07734406739473343\n",
            "loss: 0.07734348624944687\n",
            "loss: 0.0773429200053215\n",
            "loss: 0.0773426741361618\n",
            "loss: 0.07734208554029465\n",
            "loss: 0.07734165340662003\n",
            "loss: 0.07734125852584839\n",
            "loss: 0.07734071463346481\n",
            "loss: 0.07734028249979019\n",
            "loss: 0.07733996212482452\n",
            "loss: 0.07733940333127975\n",
            "loss: 0.0773390382528305\n",
            "loss: 0.07733848690986633\n",
            "loss: 0.07733796536922455\n",
            "loss: 0.0773375928401947\n",
            "loss: 0.07733695209026337\n",
            "loss: 0.07733671367168427\n",
            "loss: 0.07733618468046188\n",
            "loss: 0.07733568549156189\n",
            "loss: 0.07733532786369324\n",
            "loss: 0.07733478397130966\n",
            "loss: 0.0773344337940216\n",
            "loss: 0.07733393460512161\n",
            "loss: 0.07733340561389923\n",
            "loss: 0.0773329958319664\n",
            "loss: 0.07733239978551865\n",
            "loss: 0.07733213156461716\n",
            "loss: 0.07733166962862015\n",
            "loss: 0.07733112573623657\n",
            "loss: 0.07733059674501419\n",
            "loss: 0.07733022421598434\n",
            "loss: 0.07732969522476196\n",
            "loss: 0.07732943445444107\n",
            "loss: 0.0773288756608963\n",
            "loss: 0.07732835412025452\n",
            "loss: 0.07732784003019333\n",
            "loss: 0.07732750475406647\n",
            "loss: 0.07732708007097244\n",
            "loss: 0.07732658088207245\n",
            "loss: 0.07732617110013962\n",
            "loss: 0.07732559740543365\n",
            "loss: 0.07732530683279037\n",
            "loss: 0.07732482254505157\n",
            "loss: 0.07732443511486053\n",
            "loss: 0.07732394337654114\n",
            "loss: 0.07732362300157547\n",
            "loss: 0.07732332497835159\n",
            "loss: 0.07732319831848145\n",
            "loss: 0.07732301205396652\n",
            "loss: 0.07732298225164413\n",
            "loss: 0.07732332497835159\n",
            "loss: 0.07732392102479935\n",
            "loss: 0.07732509821653366\n",
            "loss: 0.0773271769285202\n",
            "loss: 0.07733063399791718\n",
            "loss: 0.07733602076768875\n",
            "loss: 0.07734478265047073\n",
            "loss: 0.07735881209373474\n",
            "loss: 0.07738088816404343\n",
            "loss: 0.07741530984640121\n",
            "loss: 0.07746944576501846\n",
            "loss: 0.0775517150759697\n",
            "loss: 0.07767492532730103\n",
            "loss: 0.07784977555274963\n",
            "loss: 0.0780818834900856\n",
            "loss: 0.07834413647651672\n",
            "loss: 0.07857396453619003\n",
            "loss: 0.0786413624882698\n",
            "loss: 0.07845377922058105\n",
            "loss: 0.07802514731884003\n",
            "loss: 0.07756809890270233\n",
            "loss: 0.07732421904802322\n",
            "loss: 0.07738128304481506\n",
            "loss: 0.07761871814727783\n",
            "loss: 0.07781897485256195\n",
            "loss: 0.0778253972530365\n",
            "loss: 0.07763740420341492\n",
            "loss: 0.07741031050682068\n",
            "loss: 0.07730871438980103\n",
            "loss: 0.07737329602241516\n",
            "loss: 0.077506422996521\n",
            "loss: 0.07757487893104553\n",
            "loss: 0.07752250880002975\n",
            "loss: 0.07740102708339691\n",
            "loss: 0.07731465995311737\n",
            "loss: 0.07731962949037552\n",
            "loss: 0.07738588005304337\n",
            "loss: 0.07743890583515167\n",
            "loss: 0.07742916792631149\n",
            "loss: 0.07737034559249878\n",
            "loss: 0.07731546461582184\n",
            "loss: 0.07730495184659958\n",
            "loss: 0.07733456045389175\n",
            "loss: 0.07736743241548538\n",
            "loss: 0.07737135142087936\n",
            "loss: 0.07734477519989014\n",
            "loss: 0.07731268554925919\n",
            "loss: 0.07729992270469666\n",
            "loss: 0.07731069624423981\n",
            "loss: 0.07732923328876495\n",
            "loss: 0.07733627408742905\n",
            "loss: 0.07732651382684708\n",
            "loss: 0.07730904966592789\n",
            "loss: 0.07729806751012802\n",
            "loss: 0.07729953527450562\n",
            "loss: 0.07730834931135178\n",
            "loss: 0.0773150622844696\n",
            "loss: 0.07731327414512634\n",
            "loss: 0.07730512320995331\n",
            "loss: 0.07729700952768326\n",
            "loss: 0.07729437202215195\n",
            "loss: 0.07729721814393997\n",
            "loss: 0.07730161398649216\n",
            "loss: 0.07730288803577423\n",
            "loss: 0.07730032503604889\n",
            "loss: 0.07729556411504745\n",
            "loss: 0.07729218900203705\n",
            "loss: 0.07729160040616989\n",
            "loss: 0.07729318737983704\n",
            "loss: 0.07729484885931015\n",
            "loss: 0.07729480415582657\n",
            "loss: 0.07729292660951614\n",
            "loss: 0.07729034870862961\n",
            "loss: 0.0772886723279953\n",
            "loss: 0.07728848606348038\n",
            "loss: 0.07728902995586395\n",
            "loss: 0.07728958874940872\n",
            "loss: 0.07728927582502365\n",
            "loss: 0.07728805392980576\n",
            "loss: 0.07728659361600876\n",
            "loss: 0.07728544622659683\n",
            "loss: 0.07728508859872818\n",
            "loss: 0.07728521525859833\n",
            "loss: 0.07728524506092072\n",
            "loss: 0.07728499174118042\n",
            "loss: 0.07728417217731476\n",
            "loss: 0.07728323340415955\n",
            "loss: 0.07728245854377747\n",
            "loss: 0.07728182524442673\n",
            "loss: 0.07728166878223419\n",
            "loss: 0.07728138566017151\n",
            "loss: 0.07728105783462524\n",
            "loss: 0.07728066295385361\n",
            "loss: 0.07728013396263123\n",
            "loss: 0.07727934420108795\n",
            "loss: 0.07727870345115662\n",
            "loss: 0.07727843523025513\n",
            "loss: 0.07727799564599991\n",
            "loss: 0.07727774977684021\n",
            "loss: 0.07727726548910141\n",
            "loss: 0.07727684080600739\n",
            "loss: 0.07727627456188202\n",
            "loss: 0.07727574557065964\n",
            "loss: 0.07727521657943726\n",
            "loss: 0.07727482169866562\n",
            "loss: 0.0772743970155716\n",
            "loss: 0.07727405428886414\n",
            "loss: 0.07727361470460892\n",
            "loss: 0.07727300375699997\n",
            "loss: 0.0772726759314537\n",
            "loss: 0.07727208733558655\n",
            "loss: 0.07727175951004028\n",
            "loss: 0.07727119326591492\n",
            "loss: 0.07727086544036865\n",
            "loss: 0.07727034389972687\n",
            "loss: 0.0772700086236\n",
            "loss: 0.07726937532424927\n",
            "loss: 0.07726893573999405\n",
            "loss: 0.07726868987083435\n",
            "loss: 0.07726813107728958\n",
            "loss: 0.077267587184906\n",
            "loss: 0.07726728171110153\n",
            "loss: 0.07726673036813736\n",
            "loss: 0.07726644724607468\n",
            "loss: 0.07726570218801498\n",
            "loss: 0.07726551592350006\n",
            "loss: 0.07726497948169708\n",
            "loss: 0.07726448774337769\n",
            "loss: 0.0772642269730568\n",
            "loss: 0.07726354897022247\n",
            "loss: 0.07726319134235382\n",
            "loss: 0.07726268470287323\n",
            "loss: 0.07726214081048965\n",
            "loss: 0.07726182043552399\n",
            "loss: 0.07726135849952698\n",
            "loss: 0.07726100832223892\n",
            "loss: 0.07726047188043594\n",
            "loss: 0.0772600993514061\n",
            "loss: 0.0772596150636673\n",
            "loss: 0.07725907117128372\n",
            "loss: 0.0772586464881897\n",
            "loss: 0.07725820690393448\n",
            "loss: 0.07725778967142105\n",
            "loss: 0.0772572010755539\n",
            "loss: 0.07725679129362106\n",
            "loss: 0.07725635170936584\n",
            "loss: 0.07725592702627182\n",
            "loss: 0.07725542038679123\n",
            "loss: 0.07725505530834198\n",
            "loss: 0.07725469768047333\n",
            "loss: 0.07725412398576736\n",
            "loss: 0.07725366950035095\n",
            "loss: 0.07725323736667633\n",
            "loss: 0.07725285738706589\n",
            "loss: 0.07725244015455246\n",
            "loss: 0.07725178450345993\n",
            "loss: 0.07725149393081665\n",
            "loss: 0.0772508755326271\n",
            "loss: 0.07725057750940323\n",
            "loss: 0.07725001126527786\n",
            "loss: 0.0772496908903122\n",
            "loss: 0.07724922895431519\n",
            "loss: 0.07724874466657639\n",
            "loss: 0.0772482305765152\n",
            "loss: 0.07724778354167938\n",
            "loss: 0.07724729925394058\n",
            "loss: 0.07724685221910477\n",
            "loss: 0.07724650204181671\n",
            "loss: 0.07724585384130478\n",
            "loss: 0.07724551856517792\n",
            "loss: 0.0772450789809227\n",
            "loss: 0.07724460959434509\n",
            "loss: 0.07724417746067047\n",
            "loss: 0.07724376022815704\n",
            "loss: 0.07724330574274063\n",
            "loss: 0.07724258303642273\n",
            "loss: 0.07724234461784363\n",
            "loss: 0.07724189013242722\n",
            "loss: 0.07724134624004364\n",
            "loss: 0.07724089175462723\n",
            "loss: 0.07724038511514664\n",
            "loss: 0.07723993062973022\n",
            "loss: 0.0772397592663765\n",
            "loss: 0.07723912596702576\n",
            "loss: 0.07723876088857651\n",
            "loss: 0.0772383064031601\n",
            "loss: 0.07723797857761383\n",
            "loss: 0.07723748683929443\n",
            "loss: 0.0772368460893631\n",
            "loss: 0.07723677903413773\n",
            "loss: 0.07723609358072281\n",
            "loss: 0.07723577320575714\n",
            "loss: 0.07723529636859894\n",
            "loss: 0.07723512500524521\n",
            "loss: 0.07723488658666611\n",
            "loss: 0.077234648168087\n",
            "loss: 0.07723440229892731\n",
            "loss: 0.07723446190357208\n",
            "loss: 0.07723457366228104\n",
            "loss: 0.07723495364189148\n",
            "loss: 0.07723572105169296\n",
            "loss: 0.07723688334226608\n",
            "loss: 0.07723859697580338\n",
            "loss: 0.07724146544933319\n",
            "loss: 0.0772455632686615\n",
            "loss: 0.07725214213132858\n",
            "loss: 0.0772615447640419\n",
            "loss: 0.0772758275270462\n",
            "loss: 0.07729646563529968\n",
            "loss: 0.07732704281806946\n",
            "loss: 0.07737097889184952\n",
            "loss: 0.07743397355079651\n",
            "loss: 0.07752113044261932\n",
            "loss: 0.07763762027025223\n",
            "loss: 0.07778092473745346\n",
            "loss: 0.07794003188610077\n",
            "loss: 0.07807652652263641\n",
            "loss: 0.07814290374517441\n",
            "loss: 0.07807710766792297\n",
            "loss: 0.07787322998046875\n",
            "loss: 0.0775877982378006\n",
            "loss: 0.07733986526727676\n",
            "loss: 0.07722509652376175\n",
            "loss: 0.07726217061281204\n",
            "loss: 0.0773904025554657\n",
            "loss: 0.07751460373401642\n",
            "loss: 0.07755901664495468\n",
            "loss: 0.0774991437792778\n",
            "loss: 0.0773763656616211\n",
            "loss: 0.07726345211267471\n",
            "loss: 0.07721790671348572\n",
            "loss: 0.07724648714065552\n",
            "loss: 0.07731121778488159\n",
            "loss: 0.0773606076836586\n",
            "loss: 0.07736185193061829\n",
            "loss: 0.07731794565916061\n",
            "loss: 0.07725896686315536\n",
            "loss: 0.07722022384405136\n",
            "loss: 0.07721778750419617\n",
            "loss: 0.07724268734455109\n",
            "loss: 0.07727155834436417\n",
            "loss: 0.07728312164545059\n",
            "loss: 0.07727131247520447\n",
            "loss: 0.07724517583847046\n",
            "loss: 0.07722094655036926\n",
            "loss: 0.07721114903688431\n",
            "loss: 0.07721689343452454\n",
            "loss: 0.07723023742437363\n",
            "loss: 0.07724065333604813\n",
            "loss: 0.07724114507436752\n",
            "loss: 0.07723212987184525\n",
            "loss: 0.07721961289644241\n",
            "loss: 0.07721009850502014\n",
            "loss: 0.07720785588026047\n",
            "loss: 0.07721159607172012\n",
            "loss: 0.07721735537052155\n",
            "loss: 0.07722089439630508\n",
            "loss: 0.07722011208534241\n",
            "loss: 0.07721555978059769\n",
            "loss: 0.07720959186553955\n",
            "loss: 0.07720545679330826\n",
            "loss: 0.07720412313938141\n",
            "loss: 0.0772053599357605\n",
            "loss: 0.07720758765935898\n",
            "loss: 0.07720895111560822\n",
            "loss: 0.07720862329006195\n",
            "loss: 0.07720676064491272\n",
            "loss: 0.07720418274402618\n",
            "loss: 0.07720188796520233\n",
            "loss: 0.07720039784908295\n",
            "loss: 0.07720038294792175\n",
            "loss: 0.07720080763101578\n",
            "loss: 0.0772014856338501\n",
            "loss: 0.07720152288675308\n",
            "loss: 0.0772007629275322\n",
            "loss: 0.07719960063695908\n",
            "loss: 0.07719837129116058\n",
            "loss: 0.07719726115465164\n",
            "loss: 0.07719648629426956\n",
            "loss: 0.07719618082046509\n",
            "loss: 0.07719608396291733\n",
            "loss: 0.07719593495130539\n",
            "loss: 0.07719579339027405\n",
            "loss: 0.07719522714614868\n",
            "loss: 0.07719466090202332\n",
            "loss: 0.07719383388757706\n",
            "loss: 0.07719297707080841\n",
            "loss: 0.07719250023365021\n",
            "loss: 0.07719192653894424\n",
            "loss: 0.077191561460495\n",
            "loss: 0.0771912932395935\n",
            "loss: 0.07719078660011292\n",
            "loss: 0.07719036936759949\n",
            "loss: 0.07718999683856964\n",
            "loss: 0.07718972861766815\n",
            "loss: 0.07718911021947861\n",
            "loss: 0.07718852162361145\n",
            "loss: 0.07718804478645325\n",
            "loss: 0.07718749344348907\n",
            "loss: 0.0771869644522667\n",
            "loss: 0.07718657702207565\n",
            "loss: 0.07718610018491745\n",
            "loss: 0.07718582451343536\n",
            "loss: 0.07718529552221298\n",
            "loss: 0.07718493789434433\n",
            "loss: 0.07718463987112045\n",
            "loss: 0.07718399912118912\n",
            "loss: 0.07718350738286972\n",
            "loss: 0.07718299329280853\n",
            "loss: 0.07718256115913391\n",
            "loss: 0.07718212902545929\n",
            "loss: 0.0771816074848175\n",
            "loss: 0.07718111574649811\n",
            "loss: 0.0771806463599205\n",
            "loss: 0.07718034088611603\n",
            "loss: 0.07717979699373245\n",
            "loss: 0.0771794468164444\n",
            "loss: 0.07717892527580261\n",
            "loss: 0.07717850804328918\n",
            "loss: 0.07717813551425934\n",
            "loss: 0.07717757672071457\n",
            "loss: 0.07717712968587875\n",
            "loss: 0.07717658579349518\n",
            "loss: 0.07717640697956085\n",
            "loss: 0.07717569917440414\n",
            "loss: 0.0771753340959549\n",
            "loss: 0.07717487961053848\n",
            "loss: 0.0771743431687355\n",
            "loss: 0.07717392593622208\n",
            "loss: 0.07717356085777283\n",
            "loss: 0.07717319577932358\n",
            "loss: 0.0771724060177803\n",
            "loss: 0.07717215269804001\n",
            "loss: 0.07717173546552658\n",
            "loss: 0.07717123627662659\n",
            "loss: 0.07717078179121017\n",
            "loss: 0.07717027515172958\n",
            "loss: 0.07716988772153854\n",
            "loss: 0.07716942578554153\n",
            "loss: 0.0771687775850296\n",
            "loss: 0.07716868072748184\n",
            "loss: 0.07716792821884155\n",
            "loss: 0.07716753333806992\n",
            "loss: 0.07716729491949081\n",
            "loss: 0.07716652750968933\n",
            "loss: 0.07716623693704605\n",
            "loss: 0.07716584950685501\n",
            "loss: 0.07716537266969681\n",
            "loss: 0.07716474682092667\n",
            "loss: 0.07716432213783264\n",
            "loss: 0.07716403156518936\n",
            "loss: 0.07716356962919235\n",
            "loss: 0.07716314494609833\n",
            "loss: 0.0771624743938446\n",
            "loss: 0.07716227322816849\n",
            "loss: 0.07716164737939835\n",
            "loss: 0.0771612823009491\n",
            "loss: 0.07716076076030731\n",
            "loss: 0.07716028392314911\n",
            "loss: 0.07715999335050583\n",
            "loss: 0.07715953886508942\n",
            "loss: 0.07715900242328644\n",
            "loss: 0.07715877890586853\n",
            "loss: 0.07715840637683868\n",
            "loss: 0.07715801149606705\n",
            "loss: 0.07715793699026108\n",
            "loss: 0.07715769857168198\n",
            "loss: 0.0771576538681984\n",
            "loss: 0.07715809345245361\n",
            "loss: 0.07715857774019241\n",
            "loss: 0.07715979218482971\n",
            "loss: 0.07716218382120132\n",
            "loss: 0.07716558128595352\n",
            "loss: 0.07717140763998032\n",
            "loss: 0.0771804004907608\n",
            "loss: 0.07719521969556808\n",
            "loss: 0.07721807807683945\n",
            "loss: 0.07725464552640915\n",
            "loss: 0.07731153070926666\n",
            "loss: 0.07739927619695663\n",
            "loss: 0.0775294080376625\n",
            "loss: 0.07771533727645874\n",
            "loss: 0.07795622944831848\n",
            "loss: 0.0782294049859047\n",
            "loss: 0.07844851166009903\n",
            "loss: 0.07849825918674469\n",
            "loss: 0.07826397567987442\n",
            "loss: 0.07780903577804565\n",
            "loss: 0.07735545933246613\n",
            "loss: 0.07714945077896118\n",
            "loss: 0.07724861055612564\n",
            "loss: 0.07750161737203598\n",
            "loss: 0.07768454402685165\n",
            "loss: 0.07765026390552521\n",
            "loss: 0.07743590325117111\n",
            "loss: 0.07721435278654099\n",
            "loss: 0.07714460790157318\n",
            "loss: 0.07723736017942429\n",
            "loss: 0.07737156748771667\n",
            "loss: 0.07741669565439224\n",
            "loss: 0.07733730971813202\n",
            "loss: 0.0772109404206276\n",
            "loss: 0.07714233547449112\n",
            "loss: 0.07716920226812363\n",
            "loss: 0.07724214345216751\n",
            "loss: 0.07728216797113419\n",
            "loss: 0.07725437730550766\n",
            "loss: 0.07718802243471146\n",
            "loss: 0.0771419033408165\n",
            "loss: 0.07714658230543137\n",
            "loss: 0.07718369364738464\n",
            "loss: 0.07721071690320969\n",
            "loss: 0.07720273733139038\n",
            "loss: 0.07716940343379974\n",
            "loss: 0.07714048027992249\n",
            "loss: 0.07713711261749268\n",
            "loss: 0.077154241502285\n",
            "loss: 0.07717113196849823\n",
            "loss: 0.07717163115739822\n",
            "loss: 0.07715606689453125\n",
            "loss: 0.07713855803012848\n",
            "loss: 0.07713243365287781\n",
            "loss: 0.07713872194290161\n",
            "loss: 0.0771486759185791\n",
            "loss: 0.07715212553739548\n",
            "loss: 0.07714615762233734\n",
            "loss: 0.07713625580072403\n",
            "loss: 0.07712998241186142\n",
            "loss: 0.07713067531585693\n",
            "loss: 0.0771356076002121\n",
            "loss: 0.07713903486728668\n",
            "loss: 0.07713804394006729\n",
            "loss: 0.07713334262371063\n",
            "loss: 0.07712844759225845\n",
            "loss: 0.07712669670581818\n",
            "loss: 0.07712807506322861\n",
            "loss: 0.07713015377521515\n",
            "loss: 0.077130988240242\n",
            "loss: 0.07712952792644501\n",
            "loss: 0.07712669670581818\n",
            "loss: 0.0771244466304779\n",
            "loss: 0.07712379097938538\n",
            "loss: 0.07712433487176895\n",
            "loss: 0.07712530344724655\n",
            "loss: 0.07712531834840775\n",
            "loss: 0.07712407410144806\n",
            "loss: 0.07712244242429733\n",
            "loss: 0.07712125033140182\n",
            "loss: 0.07712061703205109\n",
            "loss: 0.07712065428495407\n",
            "loss: 0.07712067663669586\n",
            "loss: 0.07712066918611526\n",
            "loss: 0.0771198570728302\n",
            "loss: 0.0771188735961914\n",
            "loss: 0.07711819559335709\n",
            "loss: 0.0771174356341362\n",
            "loss: 0.07711724936962128\n",
            "loss: 0.07711711525917053\n",
            "loss: 0.07711689174175262\n",
            "loss: 0.07711631804704666\n",
            "loss: 0.07711576670408249\n",
            "loss: 0.07711494714021683\n",
            "loss: 0.077114537358284\n",
            "loss: 0.07711387425661087\n",
            "loss: 0.07711365818977356\n",
            "loss: 0.07711341977119446\n",
            "loss: 0.07711302489042282\n",
            "loss: 0.07711248099803925\n",
            "loss: 0.0771118775010109\n",
            "loss: 0.07711140811443329\n",
            "loss: 0.07711075991392136\n",
            "loss: 0.07711036503314972\n",
            "loss: 0.07711011916399002\n",
            "loss: 0.0771096795797348\n",
            "loss: 0.0771094262599945\n",
            "loss: 0.07710869610309601\n",
            "loss: 0.07710817456245422\n",
            "loss: 0.07710786908864975\n",
            "loss: 0.0771074891090393\n",
            "loss: 0.07710698246955872\n",
            "loss: 0.07710641622543335\n",
            "loss: 0.07710594683885574\n",
            "loss: 0.07710561156272888\n",
            "loss: 0.0771050900220871\n",
            "loss: 0.07710480690002441\n",
            "loss: 0.0771041214466095\n",
            "loss: 0.07710375636816025\n",
            "loss: 0.07710336148738861\n",
            "loss: 0.07710304111242294\n",
            "loss: 0.07710261642932892\n",
            "loss: 0.07710213214159012\n",
            "loss: 0.07710163295269012\n",
            "loss: 0.07710106670856476\n",
            "loss: 0.07710064202547073\n",
            "loss: 0.07710038870573044\n",
            "loss: 0.07709980010986328\n",
            "loss: 0.0770995020866394\n",
            "loss: 0.07709889113903046\n",
            "loss: 0.07709857076406479\n",
            "loss: 0.07709810882806778\n",
            "loss: 0.07709763199090958\n",
            "loss: 0.0770973190665245\n",
            "loss: 0.07709669321775436\n",
            "loss: 0.07709629833698273\n",
            "loss: 0.07709577679634094\n",
            "loss: 0.07709532231092453\n",
            "loss: 0.07709497213363647\n",
            "loss: 0.07709452509880066\n",
            "loss: 0.07709400355815887\n",
            "loss: 0.07709347456693649\n",
            "loss: 0.07709306478500366\n",
            "loss: 0.07709266990423203\n",
            "loss: 0.07709220051765442\n",
            "loss: 0.0770917534828186\n",
            "loss: 0.07709138095378876\n",
            "loss: 0.07709099352359772\n",
            "loss: 0.07709061354398727\n",
            "loss: 0.07709009200334549\n",
            "loss: 0.07708963751792908\n",
            "loss: 0.07708913087844849\n",
            "loss: 0.07708865404129028\n",
            "loss: 0.07708839327096939\n",
            "loss: 0.07708771526813507\n",
            "loss: 0.0770873948931694\n",
            "loss: 0.07708686590194702\n",
            "loss: 0.07708629220724106\n",
            "loss: 0.0770859643816948\n",
            "loss: 0.07708552479743958\n",
            "loss: 0.0770852118730545\n",
            "loss: 0.07708464562892914\n",
            "loss: 0.0770842507481575\n",
            "loss: 0.07708381861448288\n",
            "loss: 0.07708326727151871\n",
            "loss: 0.07708287239074707\n",
            "loss: 0.07708236575126648\n",
            "loss: 0.07708194851875305\n",
            "loss: 0.0770815834403038\n",
            "loss: 0.07708095014095306\n",
            "loss: 0.07708068937063217\n",
            "loss: 0.07708027213811874\n",
            "loss: 0.0770796462893486\n",
            "loss: 0.07707918435335159\n",
            "loss: 0.07707884162664413\n",
            "loss: 0.07707834988832474\n",
            "loss: 0.0770779550075531\n",
            "loss: 0.07707729190587997\n",
            "loss: 0.07707687467336655\n",
            "loss: 0.07707645744085312\n",
            "loss: 0.07707615196704865\n",
            "loss: 0.07707562297582626\n",
            "loss: 0.07707519829273224\n",
            "loss: 0.07707475870847702\n",
            "loss: 0.07707417756319046\n",
            "loss: 0.07707371562719345\n",
            "loss: 0.07707341015338898\n",
            "loss: 0.07707288861274719\n",
            "loss: 0.07707234472036362\n",
            "loss: 0.07707203924655914\n",
            "loss: 0.0770714208483696\n",
            "loss: 0.07707114517688751\n",
            "loss: 0.07707065343856812\n",
            "loss: 0.07707014679908752\n",
            "loss: 0.0770697072148323\n",
            "loss: 0.07706938683986664\n",
            "loss: 0.07706885039806366\n",
            "loss: 0.07706838101148605\n",
            "loss: 0.07706796377897263\n",
            "loss: 0.0770675390958786\n",
            "loss: 0.07706709951162338\n",
            "loss: 0.07706674188375473\n",
            "loss: 0.07706630975008011\n",
            "loss: 0.07706591486930847\n",
            "loss: 0.07706546038389206\n",
            "loss: 0.0770651176571846\n",
            "loss: 0.07706494629383087\n",
            "loss: 0.07706484943628311\n",
            "loss: 0.07706480473279953\n",
            "loss: 0.07706501334905624\n",
            "loss: 0.07706540822982788\n",
            "loss: 0.07706642150878906\n",
            "loss: 0.0770680233836174\n",
            "loss: 0.07707095146179199\n",
            "loss: 0.07707572728395462\n",
            "loss: 0.07708270847797394\n",
            "loss: 0.07709445804357529\n",
            "loss: 0.077112577855587\n",
            "loss: 0.0771409347653389\n",
            "loss: 0.07718481868505478\n",
            "loss: 0.07725200802087784\n",
            "loss: 0.0773523822426796\n",
            "loss: 0.07749895006418228\n",
            "loss: 0.07769718766212463\n",
            "loss: 0.07794196903705597\n",
            "loss: 0.0781833827495575\n",
            "loss: 0.0783371850848198\n",
            "loss: 0.07827576994895935\n",
            "loss: 0.07796506583690643\n",
            "loss: 0.077509805560112\n",
            "loss: 0.07715052366256714\n",
            "loss: 0.07705852389335632\n",
            "loss: 0.07721560448408127\n",
            "loss: 0.07745037227869034\n",
            "loss: 0.07756892591714859\n",
            "loss: 0.07748620212078094\n",
            "loss: 0.07727041095495224\n",
            "loss: 0.07708921283483505\n",
            "loss: 0.07705805450677872\n",
            "loss: 0.077158123254776\n",
            "loss: 0.07727363705635071\n",
            "loss: 0.07729829847812653\n",
            "loss: 0.0772174522280693\n",
            "loss: 0.07710439711809158\n",
            "loss: 0.07704808562994003\n",
            "loss: 0.0770755410194397\n",
            "loss: 0.07714078575372696\n",
            "loss: 0.07717683911323547\n",
            "loss: 0.0771530345082283\n",
            "loss: 0.07709446549415588\n",
            "loss: 0.07705048471689224\n",
            "loss: 0.07704974710941315\n",
            "loss: 0.07708010822534561\n",
            "loss: 0.07710716128349304\n",
            "loss: 0.07710614055395126\n",
            "loss: 0.0770796611905098\n",
            "loss: 0.07705142349004745\n",
            "loss: 0.07704196125268936\n",
            "loss: 0.07705289870500565\n",
            "loss: 0.07706944644451141\n",
            "loss: 0.07707550376653671\n",
            "loss: 0.07706623524427414\n",
            "loss: 0.07705021649599075\n",
            "loss: 0.07703986763954163\n",
            "loss: 0.07704085111618042\n",
            "loss: 0.07704875618219376\n",
            "loss: 0.07705537974834442\n",
            "loss: 0.07705437391996384\n",
            "loss: 0.07704711705446243\n",
            "loss: 0.0770394504070282\n",
            "loss: 0.07703618705272675\n",
            "loss: 0.0770379900932312\n",
            "loss: 0.07704190164804459\n",
            "loss: 0.07704399526119232\n",
            "loss: 0.07704229652881622\n",
            "loss: 0.07703809440135956\n",
            "loss: 0.07703429460525513\n",
            "loss: 0.07703307271003723\n",
            "loss: 0.07703405618667603\n",
            "loss: 0.07703584432601929\n",
            "loss: 0.07703657448291779\n",
            "loss: 0.07703526318073273\n",
            "loss: 0.0770329087972641\n",
            "loss: 0.0770309567451477\n",
            "loss: 0.0770299956202507\n",
            "loss: 0.07703033089637756\n",
            "loss: 0.07703075557947159\n",
            "loss: 0.07703099399805069\n",
            "loss: 0.07703041285276413\n",
            "loss: 0.07702908664941788\n",
            "loss: 0.07702772319316864\n",
            "loss: 0.07702687382698059\n",
            "loss: 0.07702662795782089\n",
            "loss: 0.07702680677175522\n",
            "loss: 0.0770268663764\n",
            "loss: 0.07702633738517761\n",
            "loss: 0.07702561467885971\n",
            "loss: 0.07702475786209106\n",
            "loss: 0.07702401280403137\n",
            "loss: 0.07702327519655228\n",
            "loss: 0.07702313363552094\n",
            "loss: 0.07702291756868362\n",
            "loss: 0.07702269405126572\n",
            "loss: 0.0770222544670105\n",
            "loss: 0.07702162861824036\n",
            "loss: 0.0770210325717926\n",
            "loss: 0.07702041417360306\n",
            "loss: 0.07701991498470306\n",
            "loss: 0.07701956480741501\n",
            "loss: 0.07701920717954636\n",
            "loss: 0.07701889425516129\n",
            "loss: 0.07701840251684189\n",
            "loss: 0.07701796293258667\n",
            "loss: 0.07701738178730011\n",
            "loss: 0.07701685279607773\n",
            "loss: 0.0770164355635643\n",
            "loss: 0.07701605558395386\n",
            "loss: 0.07701551169157028\n",
            "loss: 0.07701531052589417\n",
            "loss: 0.07701479643583298\n",
            "loss: 0.07701438665390015\n",
            "loss: 0.07701379060745239\n",
            "loss: 0.07701342552900314\n",
            "loss: 0.07701290398836136\n",
            "loss: 0.07701252400875092\n",
            "loss: 0.07701202481985092\n",
            "loss: 0.0770115852355957\n",
            "loss: 0.07701118290424347\n",
            "loss: 0.07701070606708527\n",
            "loss: 0.07701027393341064\n",
            "loss: 0.07700987905263901\n",
            "loss: 0.07700949907302856\n",
            "loss: 0.0770089402794838\n",
            "loss: 0.077008455991745\n",
            "loss: 0.07700808346271515\n",
            "loss: 0.07700757682323456\n",
            "loss: 0.07700708508491516\n",
            "loss: 0.07700664550065994\n",
            "loss: 0.07700631022453308\n",
            "loss: 0.07700585573911667\n",
            "loss: 0.07700543105602264\n",
            "loss: 0.07700496166944504\n",
            "loss: 0.07700454443693161\n",
            "loss: 0.07700414955615997\n",
            "loss: 0.07700356096029282\n",
            "loss: 0.07700318098068237\n",
            "loss: 0.07700275629758835\n",
            "loss: 0.07700230181217194\n",
            "loss: 0.07700183987617493\n",
            "loss: 0.07700131088495255\n",
            "loss: 0.07700084894895554\n",
            "loss: 0.07700048387050629\n",
            "loss: 0.07700008153915405\n",
            "loss: 0.0769997164607048\n",
            "loss: 0.07699911296367645\n",
            "loss: 0.0769987627863884\n",
            "loss: 0.07699824124574661\n",
            "loss: 0.07699772715568542\n",
            "loss: 0.07699739933013916\n",
            "loss: 0.07699690759181976\n",
            "loss: 0.07699644565582275\n",
            "loss: 0.07699596881866455\n",
            "loss: 0.07699558138847351\n",
            "loss: 0.07699519395828247\n",
            "loss: 0.07699474692344666\n",
            "loss: 0.07699437439441681\n",
            "loss: 0.07699373364448547\n",
            "loss: 0.07699321955442429\n",
            "loss: 0.07699285447597504\n",
            "loss: 0.07699237763881683\n",
            "loss: 0.07699201256036758\n",
            "loss: 0.07699165493249893\n",
            "loss: 0.07699094712734222\n",
            "loss: 0.07699059695005417\n",
            "loss: 0.07699009776115417\n",
            "loss: 0.07698981463909149\n",
            "loss: 0.0769893154501915\n",
            "loss: 0.07698895782232285\n",
            "loss: 0.07698830962181091\n",
            "loss: 0.07698792219161987\n",
            "loss: 0.07698756456375122\n",
            "loss: 0.07698693871498108\n",
            "loss: 0.0769866481423378\n",
            "loss: 0.07698621600866318\n",
            "loss: 0.07698572427034378\n",
            "loss: 0.07698529213666916\n",
            "loss: 0.07698475569486618\n",
            "loss: 0.07698435336351395\n",
            "loss: 0.07698390632867813\n",
            "loss: 0.07698342204093933\n",
            "loss: 0.07698309421539307\n",
            "loss: 0.07698247581720352\n",
            "loss: 0.07698207348585129\n",
            "loss: 0.07698167115449905\n",
            "loss: 0.07698114216327667\n",
            "loss: 0.07698056846857071\n",
            "loss: 0.07698030769824982\n",
            "loss: 0.07697971910238266\n",
            "loss: 0.07697940617799759\n",
            "loss: 0.07697893679141998\n",
            "loss: 0.07697854191064835\n",
            "loss: 0.07697793841362\n",
            "loss: 0.07697758823633194\n",
            "loss: 0.07697725296020508\n",
            "loss: 0.07697676122188568\n",
            "loss: 0.07697615027427673\n",
            "loss: 0.07697565108537674\n",
            "loss: 0.07697539031505585\n",
            "loss: 0.07697484642267227\n",
            "loss: 0.07697450369596481\n",
            "loss: 0.07697401195764542\n",
            "loss: 0.07697360217571259\n",
            "loss: 0.07697312533855438\n",
            "loss: 0.07697267830371857\n",
            "loss: 0.07697239518165588\n",
            "loss: 0.07697191089391708\n",
            "loss: 0.07697157561779022\n",
            "loss: 0.07697121798992157\n",
            "loss: 0.07697119563817978\n",
            "loss: 0.07697110623121262\n",
            "loss: 0.07697144150733948\n",
            "loss: 0.07697205245494843\n",
            "loss: 0.07697327435016632\n",
            "loss: 0.07697582989931107\n",
            "loss: 0.07698005437850952\n",
            "loss: 0.07698726654052734\n",
            "loss: 0.07699894160032272\n",
            "loss: 0.07701879739761353\n",
            "loss: 0.07705137133598328\n",
            "loss: 0.07710505276918411\n",
            "loss: 0.0771917849779129\n",
            "loss: 0.07732841372489929\n",
            "loss: 0.07753647118806839\n",
            "loss: 0.07782730460166931\n",
            "loss: 0.0781879872083664\n",
            "loss: 0.07851899415254593\n",
            "loss: 0.07865425944328308\n",
            "loss: 0.078396737575531\n",
            "loss: 0.07780072838068008\n",
            "loss: 0.07719513773918152\n",
            "loss: 0.07696128636598587\n",
            "loss: 0.07715988904237747\n",
            "loss: 0.07751698791980743\n",
            "loss: 0.07768379151821136\n",
            "loss: 0.07750756293535233\n",
            "loss: 0.0771648958325386\n",
            "loss: 0.07696385681629181\n",
            "loss: 0.07703740894794464\n",
            "loss: 0.07724042981863022\n",
            "loss: 0.07733388990163803\n",
            "loss: 0.07722652703523636\n",
            "loss: 0.07703889161348343\n",
            "loss: 0.07695531100034714\n",
            "loss: 0.07702422142028809\n",
            "loss: 0.0771334171295166\n",
            "loss: 0.07715316116809845\n",
            "loss: 0.07706784456968307\n",
            "loss: 0.0769733414053917\n",
            "loss: 0.07695847749710083\n",
            "loss: 0.07701486349105835\n",
            "loss: 0.07706379145383835\n",
            "loss: 0.07704959064722061\n",
            "loss: 0.07699212431907654\n",
            "loss: 0.07695243507623672\n",
            "loss: 0.07696250081062317\n",
            "loss: 0.07699772715568542\n",
            "loss: 0.07701331377029419\n",
            "loss: 0.07699331641197205\n",
            "loss: 0.0769609808921814\n",
            "loss: 0.07694807648658752\n",
            "loss: 0.0769609734416008\n",
            "loss: 0.07697908580303192\n",
            "loss: 0.07698079198598862\n",
            "loss: 0.07696522772312164\n",
            "loss: 0.0769491121172905\n",
            "loss: 0.07694646716117859\n",
            "loss: 0.07695554941892624\n",
            "loss: 0.0769636407494545\n",
            "loss: 0.07696173340082169\n",
            "loss: 0.07695203274488449\n",
            "loss: 0.07694411277770996\n",
            "loss: 0.07694414258003235\n",
            "loss: 0.07694932073354721\n",
            "loss: 0.07695295661687851\n",
            "loss: 0.07695064693689346\n",
            "loss: 0.07694505900144577\n",
            "loss: 0.07694111764431\n",
            "loss: 0.07694129645824432\n",
            "loss: 0.0769437849521637\n",
            "loss: 0.0769454836845398\n",
            "loss: 0.07694396376609802\n",
            "loss: 0.07694075256586075\n",
            "loss: 0.07693841308355331\n",
            "loss: 0.0769382044672966\n",
            "loss: 0.07693945616483688\n",
            "loss: 0.076940156519413\n",
            "loss: 0.0769391879439354\n",
            "loss: 0.0769372284412384\n",
            "loss: 0.07693574577569962\n",
            "loss: 0.07693549245595932\n",
            "loss: 0.07693563401699066\n",
            "loss: 0.0769360288977623\n",
            "loss: 0.07693533599376678\n",
            "loss: 0.07693438231945038\n",
            "loss: 0.07693322002887726\n",
            "loss: 0.07693257182836533\n",
            "loss: 0.07693254947662354\n",
            "loss: 0.07693251967430115\n",
            "loss: 0.0769324079155922\n",
            "loss: 0.07693147659301758\n",
            "loss: 0.07693056017160416\n",
            "loss: 0.07693003863096237\n",
            "loss: 0.07692961394786835\n",
            "loss: 0.07692945748567581\n",
            "loss: 0.07692917436361313\n",
            "loss: 0.07692884653806686\n",
            "loss: 0.07692806422710419\n",
            "loss: 0.0769275352358818\n",
            "loss: 0.07692697644233704\n",
            "loss: 0.07692675292491913\n",
            "loss: 0.07692626118659973\n",
            "loss: 0.07692589610815048\n",
            "loss: 0.07692548632621765\n",
            "loss: 0.07692501693964005\n",
            "loss: 0.07692442834377289\n",
            "loss: 0.07692400366067886\n",
            "loss: 0.07692358642816544\n",
            "loss: 0.07692331820726395\n",
            "loss: 0.07692299038171768\n",
            "loss: 0.07692241668701172\n",
            "loss: 0.0769219845533371\n",
            "loss: 0.07692139595746994\n",
            "loss: 0.07692096382379532\n",
            "loss: 0.07692060619592667\n",
            "loss: 0.07692030072212219\n",
            "loss: 0.07691983878612518\n",
            "loss: 0.07691934704780579\n",
            "loss: 0.07691875845193863\n",
            "loss: 0.07691847532987595\n",
            "loss: 0.07691793143749237\n",
            "loss: 0.07691753655672073\n",
            "loss: 0.07691709697246552\n",
            "loss: 0.07691673934459686\n",
            "loss: 0.07691634446382523\n",
            "loss: 0.07691574096679688\n",
            "loss: 0.07691537588834763\n",
            "loss: 0.07691491395235062\n",
            "loss: 0.07691452652215958\n",
            "loss: 0.07691405713558197\n",
            "loss: 0.07691358774900436\n",
            "loss: 0.07691328227519989\n",
            "loss: 0.07691266387701035\n",
            "loss: 0.07691243290901184\n",
            "loss: 0.07691191136837006\n",
            "loss: 0.07691130042076111\n",
            "loss: 0.07691103219985962\n",
            "loss: 0.07691044360399246\n",
            "loss: 0.07691006362438202\n",
            "loss: 0.0769096314907074\n",
            "loss: 0.076909139752388\n",
            "loss: 0.07690879702568054\n",
            "loss: 0.07690824568271637\n",
            "loss: 0.07690785080194473\n",
            "loss: 0.07690747082233429\n",
            "loss: 0.07690701633691788\n",
            "loss: 0.07690645009279251\n",
            "loss: 0.07690630108118057\n",
            "loss: 0.07690584659576416\n",
            "loss: 0.0769052654504776\n",
            "loss: 0.07690490782260895\n",
            "loss: 0.07690459489822388\n",
            "loss: 0.07690407335758209\n",
            "loss: 0.07690345495939255\n",
            "loss: 0.0769030824303627\n",
            "loss: 0.07690271735191345\n",
            "loss: 0.07690221071243286\n",
            "loss: 0.07690173387527466\n",
            "loss: 0.07690127193927765\n",
            "loss: 0.07690096646547318\n",
            "loss: 0.07690045982599258\n",
            "loss: 0.07690000534057617\n",
            "loss: 0.07689955085515976\n",
            "loss: 0.0768992230296135\n",
            "loss: 0.0768987238407135\n",
            "loss: 0.07689831405878067\n",
            "loss: 0.07689785212278366\n",
            "loss: 0.07689749449491501\n",
            "loss: 0.07689692825078964\n",
            "loss: 0.07689650356769562\n",
            "loss: 0.07689617574214935\n",
            "loss: 0.07689554244279861\n",
            "loss: 0.07689512521028519\n",
            "loss: 0.07689469307661057\n",
            "loss: 0.07689439505338669\n",
            "loss: 0.0768938958644867\n",
            "loss: 0.07689332962036133\n",
            "loss: 0.0768929123878479\n",
            "loss: 0.07689247280359268\n",
            "loss: 0.07689198851585388\n",
            "loss: 0.07689167559146881\n",
            "loss: 0.07689114660024643\n",
            "loss: 0.07689065486192703\n",
            "loss: 0.0768902450799942\n",
            "loss: 0.07688981294631958\n",
            "loss: 0.07688940316438675\n",
            "loss: 0.07688900828361511\n",
            "loss: 0.07688841223716736\n",
            "loss: 0.07688818126916885\n",
            "loss: 0.0768875777721405\n",
            "loss: 0.0768871083855629\n",
            "loss: 0.0768868699669838\n",
            "loss: 0.07688632607460022\n",
            "loss: 0.0768859013915062\n",
            "loss: 0.07688538730144501\n",
            "loss: 0.076884925365448\n",
            "loss: 0.07688459753990173\n",
            "loss: 0.07688415795564651\n",
            "loss: 0.07688356935977936\n",
            "loss: 0.07688327133655548\n",
            "loss: 0.0768827274441719\n",
            "loss: 0.07688223570585251\n",
            "loss: 0.07688187062740326\n",
            "loss: 0.07688125222921371\n",
            "loss: 0.07688097655773163\n",
            "loss: 0.07688049972057343\n",
            "loss: 0.07688014209270477\n",
            "loss: 0.07687964290380478\n",
            "loss: 0.07687928527593613\n",
            "loss: 0.07687878608703613\n",
            "loss: 0.07687839865684509\n",
            "loss: 0.07687792181968689\n",
            "loss: 0.07687732577323914\n",
            "loss: 0.07687688618898392\n",
            "loss: 0.07687658071517944\n",
            "loss: 0.07687603682279587\n",
            "loss: 0.07687560468912125\n",
            "loss: 0.07687519490718842\n",
            "loss: 0.07687471061944962\n",
            "loss: 0.07687435299158096\n",
            "loss: 0.07687386125326157\n",
            "loss: 0.07687336951494217\n",
            "loss: 0.07687301933765411\n",
            "loss: 0.0768725648522377\n",
            "loss: 0.07687213271856308\n",
            "loss: 0.0768716111779213\n",
            "loss: 0.07687130570411682\n",
            "loss: 0.07687099277973175\n",
            "loss: 0.07687068730592728\n",
            "loss: 0.07687030732631683\n",
            "loss: 0.07687003910541534\n",
            "loss: 0.07686961442232132\n",
            "loss: 0.07686956226825714\n",
            "loss: 0.07686956226825714\n",
            "loss: 0.07686977088451385\n",
            "loss: 0.07687021046876907\n",
            "loss: 0.07687117159366608\n",
            "loss: 0.07687250524759293\n",
            "loss: 0.07687503844499588\n",
            "loss: 0.0768786072731018\n",
            "loss: 0.07688413560390472\n",
            "loss: 0.07689288258552551\n",
            "loss: 0.07690633833408356\n",
            "loss: 0.07692651450634003\n",
            "loss: 0.07695703953504562\n",
            "loss: 0.07700271904468536\n",
            "loss: 0.0770692527294159\n",
            "loss: 0.0771653950214386\n",
            "loss: 0.07729684561491013\n",
            "loss: 0.0774669423699379\n",
            "loss: 0.07765966653823853\n",
            "loss: 0.0778387188911438\n",
            "loss: 0.07792560756206512\n",
            "loss: 0.07785388827323914\n",
            "loss: 0.07759436964988708\n",
            "loss: 0.07724318653345108\n",
            "loss: 0.07695667445659637\n",
            "loss: 0.0768575370311737\n",
            "loss: 0.07694706320762634\n",
            "loss: 0.077121801674366\n",
            "loss: 0.07725058495998383\n",
            "loss: 0.07724669575691223\n",
            "loss: 0.07711954414844513\n",
            "loss: 0.07695701718330383\n",
            "loss: 0.07686148583889008\n",
            "loss: 0.07687395811080933\n",
            "loss: 0.07695603370666504\n",
            "loss: 0.07703056186437607\n",
            "loss: 0.07703983038663864\n",
            "loss: 0.07698225229978561\n",
            "loss: 0.07690248638391495\n",
            "loss: 0.07685476541519165\n",
            "loss: 0.07686107605695724\n",
            "loss: 0.07690193504095078\n",
            "loss: 0.07693780958652496\n",
            "loss: 0.0769408643245697\n",
            "loss: 0.07691090553998947\n",
            "loss: 0.07687190175056458\n",
            "loss: 0.07684960216283798\n",
            "loss: 0.07685357332229614\n",
            "loss: 0.07687351107597351\n",
            "loss: 0.07689040154218674\n",
            "loss: 0.0768912136554718\n",
            "loss: 0.07687649130821228\n",
            "loss: 0.0768572986125946\n",
            "loss: 0.07684604823589325\n",
            "loss: 0.07684721797704697\n",
            "loss: 0.07685618102550507\n",
            "loss: 0.0768645852804184\n",
            "loss: 0.07686568051576614\n",
            "loss: 0.07685931771993637\n",
            "loss: 0.0768498107790947\n",
            "loss: 0.07684323936700821\n",
            "loss: 0.07684227079153061\n",
            "loss: 0.07684555649757385\n",
            "loss: 0.07684976607561111\n",
            "loss: 0.07685133814811707\n",
            "loss: 0.07684934139251709\n",
            "loss: 0.07684475928544998\n",
            "loss: 0.0768408328294754\n",
            "loss: 0.07683875411748886\n",
            "loss: 0.07683909684419632\n",
            "loss: 0.07684067636728287\n",
            "loss: 0.07684197276830673\n",
            "loss: 0.07684195786714554\n",
            "loss: 0.0768403634428978\n",
            "loss: 0.07683803886175156\n",
            "loss: 0.07683610171079636\n",
            "loss: 0.0768352821469307\n",
            "loss: 0.07683537155389786\n",
            "loss: 0.07683569192886353\n",
            "loss: 0.0768360123038292\n",
            "loss: 0.07683579623699188\n",
            "loss: 0.07683473825454712\n",
            "loss: 0.07683366537094116\n",
            "loss: 0.07683250308036804\n",
            "loss: 0.0768318623304367\n",
            "loss: 0.07683148235082626\n",
            "loss: 0.0768314003944397\n",
            "loss: 0.07683141529560089\n",
            "loss: 0.07683095335960388\n",
            "loss: 0.07683054357767105\n",
            "loss: 0.07682976126670837\n",
            "loss: 0.07682908326387405\n",
            "loss: 0.07682821899652481\n",
            "loss: 0.07682784646749496\n",
            "loss: 0.07682742923498154\n",
            "loss: 0.07682719081640244\n",
            "loss: 0.07682681083679199\n",
            "loss: 0.07682641595602036\n",
            "loss: 0.07682599872350693\n",
            "loss: 0.07682545483112335\n",
            "loss: 0.07682496309280396\n",
            "loss: 0.07682439684867859\n",
            "loss: 0.07682400196790695\n",
            "loss: 0.07682348787784576\n",
            "loss: 0.07682308554649353\n",
            "loss: 0.07682265341281891\n",
            "loss: 0.07682222872972488\n",
            "loss: 0.0768219605088234\n",
            "loss: 0.07682150602340698\n",
            "loss: 0.0768209844827652\n",
            "loss: 0.07682055234909058\n",
            "loss: 0.07681990414857864\n",
            "loss: 0.0768195167183876\n",
            "loss: 0.07681918144226074\n",
            "loss: 0.07681868225336075\n",
            "loss: 0.07681815326213837\n",
            "loss: 0.07681763172149658\n",
            "loss: 0.07681740075349808\n",
            "loss: 0.07681691646575928\n",
            "loss: 0.07681649923324585\n",
            "loss: 0.07681607455015182\n",
            "loss: 0.07681544870138168\n",
            "loss: 0.0768151506781578\n",
            "loss: 0.07681480050086975\n",
            "loss: 0.07681415230035782\n",
            "loss: 0.07681385427713394\n",
            "loss: 0.07681331038475037\n",
            "loss: 0.07681281119585037\n",
            "loss: 0.07681241631507874\n",
            "loss: 0.07681212574243546\n",
            "loss: 0.07681159675121307\n",
            "loss: 0.07681124657392502\n",
            "loss: 0.07681068032979965\n",
            "loss: 0.0768103152513504\n",
            "loss: 0.07680979371070862\n",
            "loss: 0.07680942863225937\n",
            "loss: 0.07680895924568176\n",
            "loss: 0.07680842280387878\n",
            "loss: 0.07680802047252655\n",
            "loss: 0.07680760324001312\n",
            "loss: 0.07680711895227432\n",
            "loss: 0.07680677622556686\n",
            "loss: 0.07680623233318329\n",
            "loss: 0.07680583745241165\n",
            "loss: 0.07680536806583405\n",
            "loss: 0.07680501788854599\n",
            "loss: 0.07680445909500122\n",
            "loss: 0.07680400460958481\n",
            "loss: 0.07680369168519974\n",
            "loss: 0.07680301368236542\n",
            "loss: 0.07680270820856094\n",
            "loss: 0.07680226117372513\n",
            "loss: 0.07680194824934006\n",
            "loss: 0.07680140435695648\n",
            "loss: 0.07680100202560425\n",
            "loss: 0.07680052518844604\n",
            "loss: 0.07679997384548187\n",
            "loss: 0.07679962366819382\n",
            "loss: 0.07679915428161621\n",
            "loss: 0.07679875195026398\n",
            "loss: 0.07679834961891174\n",
            "loss: 0.07679787278175354\n",
            "loss: 0.07679755985736847\n",
            "loss: 0.07679694145917892\n",
            "loss: 0.07679649442434311\n",
            "loss: 0.07679601013660431\n",
            "loss: 0.07679560780525208\n",
            "loss: 0.07679521292448044\n",
            "loss: 0.07679485529661179\n",
            "loss: 0.07679452002048492\n",
            "loss: 0.07679402083158493\n",
            "loss: 0.07679375261068344\n",
            "loss: 0.07679334282875061\n",
            "loss: 0.07679305970668793\n",
            "loss: 0.07679271697998047\n",
            "loss: 0.07679247111082077\n",
            "loss: 0.07679247111082077\n",
            "loss: 0.07679226994514465\n",
            "loss: 0.07679254561662674\n",
            "loss: 0.07679295539855957\n",
            "loss: 0.07679387181997299\n",
            "loss: 0.07679532468318939\n",
            "loss: 0.07679728418588638\n",
            "loss: 0.07680059224367142\n",
            "loss: 0.07680591195821762\n",
            "loss: 0.07681355625391006\n",
            "loss: 0.07682520151138306\n",
            "loss: 0.07684266567230225\n",
            "loss: 0.07686872780323029\n",
            "loss: 0.07690746337175369\n",
            "loss: 0.07696409523487091\n",
            "loss: 0.0770457535982132\n",
            "loss: 0.07715792953968048\n",
            "loss: 0.07730579376220703\n",
            "loss: 0.07747910171747208\n",
            "loss: 0.07765421271324158\n",
            "loss: 0.07777063548564911\n",
            "loss: 0.07776525616645813\n",
            "loss: 0.07758691906929016\n",
            "loss: 0.07728342711925507\n",
            "loss: 0.07697510719299316\n",
            "loss: 0.07679863274097443\n",
            "loss: 0.0768052265048027\n",
            "loss: 0.07694089412689209\n",
            "loss: 0.07709375768899918\n",
            "loss: 0.0771595686674118\n",
            "loss: 0.07710158079862595\n",
            "loss: 0.07696029543876648\n",
            "loss: 0.07682766765356064\n",
            "loss: 0.07677658647298813\n",
            "loss: 0.07681510597467422\n",
            "loss: 0.0768936350941658\n",
            "loss: 0.0769466757774353\n",
            "loss: 0.07693745940923691\n",
            "loss: 0.07687665522098541\n",
            "loss: 0.0768083855509758\n",
            "loss: 0.07677450776100159\n",
            "loss: 0.07678642868995667\n",
            "loss: 0.07682358473539352\n",
            "loss: 0.07685253024101257\n",
            "loss: 0.0768529400229454\n",
            "loss: 0.07682584971189499\n",
            "loss: 0.076791912317276\n",
            "loss: 0.076772041618824\n",
            "loss: 0.07677412778139114\n",
            "loss: 0.07679097354412079\n",
            "loss: 0.07680633664131165\n",
            "loss: 0.07680939882993698\n",
            "loss: 0.0767984539270401\n",
            "loss: 0.07678187638521194\n",
            "loss: 0.07676983624696732\n",
            "loss: 0.07676772773265839\n",
            "loss: 0.07677391171455383\n",
            "loss: 0.07678178697824478\n",
            "loss: 0.07678533345460892\n",
            "loss: 0.07678227871656418\n",
            "loss: 0.07677489519119263\n",
            "loss: 0.0767676904797554\n",
            "loss: 0.07676418125629425\n",
            "loss: 0.07676514238119125\n",
            "loss: 0.0767684355378151\n",
            "loss: 0.07677125185728073\n",
            "loss: 0.07677168399095535\n",
            "loss: 0.07676907628774643\n",
            "loss: 0.0767652615904808\n",
            "loss: 0.07676207274198532\n",
            "loss: 0.0767606571316719\n",
            "loss: 0.07676102966070175\n",
            "loss: 0.07676227390766144\n",
            "loss: 0.07676317542791367\n",
            "loss: 0.07676293700933456\n",
            "loss: 0.07676169276237488\n",
            "loss: 0.07675991207361221\n",
            "loss: 0.07675817608833313\n",
            "loss: 0.07675720006227493\n",
            "loss: 0.07675688713788986\n",
            "loss: 0.07675717771053314\n",
            "loss: 0.07675735652446747\n",
            "loss: 0.07675723731517792\n",
            "loss: 0.07675668597221375\n",
            "loss: 0.0767558217048645\n",
            "loss: 0.07675476372241974\n",
            "loss: 0.07675390690565109\n",
            "loss: 0.07675330340862274\n",
            "loss: 0.0767529085278511\n",
            "loss: 0.07675284892320633\n",
            "loss: 0.07675261795520782\n",
            "loss: 0.07675227522850037\n",
            "loss: 0.07675184309482574\n",
            "loss: 0.07675127685070038\n",
            "loss: 0.07675062119960785\n",
            "loss: 0.07674999535083771\n",
            "loss: 0.07674934715032578\n",
            "loss: 0.07674889266490936\n",
            "loss: 0.07674867659807205\n",
            "loss: 0.0767483338713646\n",
            "loss: 0.07674800604581833\n",
            "loss: 0.07674749195575714\n",
            "loss: 0.07674714177846909\n",
            "loss: 0.07674645632505417\n",
            "loss: 0.07674594968557358\n",
            "loss: 0.07674559950828552\n",
            "loss: 0.07674500346183777\n",
            "loss: 0.07674451172351837\n",
            "loss: 0.07674387842416763\n",
            "loss: 0.07674369215965271\n",
            "loss: 0.07674320787191391\n",
            "loss: 0.0767429918050766\n",
            "loss: 0.07674243301153183\n",
            "loss: 0.07674216479063034\n",
            "loss: 0.07674163579940796\n",
            "loss: 0.07674112915992737\n",
            "loss: 0.07674066722393036\n",
            "loss: 0.07674013823270798\n",
            "loss: 0.07673975825309753\n",
            "loss: 0.07673913240432739\n",
            "loss: 0.07673882693052292\n",
            "loss: 0.07673833519220352\n",
            "loss: 0.07673804461956024\n",
            "loss: 0.07673735916614532\n",
            "loss: 0.07673723995685577\n",
            "loss: 0.07673678547143936\n",
            "loss: 0.07673630118370056\n",
            "loss: 0.07673565298318863\n",
            "loss: 0.07673538476228714\n",
            "loss: 0.07673493027687073\n",
            "loss: 0.07673437148332596\n",
            "loss: 0.07673400640487671\n",
            "loss: 0.07673348486423492\n",
            "loss: 0.07673313468694687\n",
            "loss: 0.07673268020153046\n",
            "loss: 0.07673228532075882\n",
            "loss: 0.07673166692256927\n",
            "loss: 0.07673142105340958\n",
            "loss: 0.076730877161026\n",
            "loss: 0.0767304077744484\n",
            "loss: 0.07673004269599915\n",
            "loss: 0.07672952860593796\n",
            "loss: 0.07672911137342453\n",
            "loss: 0.07672882080078125\n",
            "loss: 0.07672825455665588\n",
            "loss: 0.07672791182994843\n",
            "loss: 0.07672739773988724\n",
            "loss: 0.07672706991434097\n",
            "loss: 0.0767265185713768\n",
            "loss: 0.0767260417342186\n",
            "loss: 0.07672559469938278\n",
            "loss: 0.07672515511512756\n",
            "loss: 0.07672469317913055\n",
            "loss: 0.07672417908906937\n",
            "loss: 0.07672382146120071\n",
            "loss: 0.0767233669757843\n",
            "loss: 0.07672283798456192\n",
            "loss: 0.07672251015901566\n",
            "loss: 0.07672213762998581\n",
            "loss: 0.07672151923179626\n",
            "loss: 0.0767212063074112\n",
            "loss: 0.07672068476676941\n",
            "loss: 0.07672032713890076\n",
            "loss: 0.07671982049942017\n",
            "loss: 0.07671944797039032\n",
            "loss: 0.07671887427568436\n",
            "loss: 0.07671869546175003\n",
            "loss: 0.07671816647052765\n",
            "loss: 0.07671795785427094\n",
            "loss: 0.07671738415956497\n",
            "loss: 0.07671719044446945\n",
            "loss: 0.07671687006950378\n",
            "loss: 0.07671673595905304\n",
            "loss: 0.07671644538640976\n",
            "loss: 0.07671654224395752\n",
            "loss: 0.07671679556369781\n",
            "loss: 0.07671733945608139\n",
            "loss: 0.07671825587749481\n",
            "loss: 0.0767199918627739\n",
            "loss: 0.07672257721424103\n",
            "loss: 0.07672661542892456\n",
            "loss: 0.07673300802707672\n",
            "loss: 0.07674320042133331\n",
            "loss: 0.0767582505941391\n",
            "loss: 0.07678157091140747\n",
            "loss: 0.07681680470705032\n",
            "loss: 0.07686987519264221\n",
            "loss: 0.07694772630929947\n",
            "loss: 0.07705974578857422\n",
            "loss: 0.07721154391765594\n",
            "loss: 0.07740339636802673\n",
            "loss: 0.07760889828205109\n",
            "loss: 0.07777882367372513\n",
            "loss: 0.07781800627708435\n",
            "loss: 0.07766702026128769\n",
            "loss: 0.0773356556892395\n",
            "loss: 0.07696867734193802\n",
            "loss: 0.07673744112253189\n",
            "loss: 0.07672673463821411\n",
            "loss: 0.07688169181346893\n",
            "loss: 0.07706166058778763\n",
            "loss: 0.07713695615530014\n",
            "loss: 0.07705787569284439\n",
            "loss: 0.0768866166472435\n",
            "loss: 0.07674040645360947\n",
            "loss: 0.07670392096042633\n",
            "loss: 0.07677040249109268\n",
            "loss: 0.07686468958854675\n",
            "loss: 0.07690627127885818\n",
            "loss: 0.07686620950698853\n",
            "loss: 0.07678044587373734\n",
            "loss: 0.076712466776371\n",
            "loss: 0.07670189440250397\n",
            "loss: 0.0767398402094841\n",
            "loss: 0.07678456604480743\n",
            "loss: 0.07679776847362518\n",
            "loss: 0.07677122950553894\n",
            "loss: 0.07672824710607529\n",
            "loss: 0.07669907063245773\n",
            "loss: 0.07669933140277863\n",
            "loss: 0.07672034949064255\n",
            "loss: 0.07674037665128708\n",
            "loss: 0.07674320042133331\n",
            "loss: 0.07672779262065887\n",
            "loss: 0.0767064318060875\n",
            "loss: 0.07669378072023392\n",
            "loss: 0.07669509202241898\n",
            "loss: 0.07670547813177109\n",
            "loss: 0.07671460509300232\n",
            "loss: 0.0767153799533844\n",
            "loss: 0.07670735567808151\n",
            "loss: 0.07669693976640701\n",
            "loss: 0.07669034600257874\n",
            "loss: 0.07669050991535187\n",
            "loss: 0.07669511437416077\n",
            "loss: 0.0766996219754219\n",
            "loss: 0.07670017331838608\n",
            "loss: 0.07669680565595627\n",
            "loss: 0.07669147849082947\n",
            "loss: 0.07668759673833847\n",
            "loss: 0.07668649405241013\n",
            "loss: 0.0766880065202713\n",
            "loss: 0.07669016718864441\n",
            "loss: 0.07669086754322052\n",
            "loss: 0.07668983191251755\n",
            "loss: 0.07668736577033997\n",
            "loss: 0.07668484002351761\n",
            "loss: 0.07668346166610718\n",
            "loss: 0.07668320834636688\n",
            "loss: 0.07668399810791016\n",
            "loss: 0.07668440043926239\n",
            "loss: 0.07668417692184448\n",
            "loss: 0.07668337970972061\n",
            "loss: 0.07668191939592361\n",
            "loss: 0.07668078690767288\n",
            "loss: 0.07667998969554901\n",
            "loss: 0.07667969912290573\n",
            "loss: 0.07667975127696991\n",
            "loss: 0.07667963206768036\n",
            "loss: 0.07667939364910126\n",
            "loss: 0.07667896151542664\n",
            "loss: 0.07667788863182068\n",
            "loss: 0.07667719572782516\n",
            "loss: 0.07667659968137741\n",
            "loss: 0.07667595148086548\n",
            "loss: 0.07667569071054459\n",
            "loss: 0.07667568325996399\n",
            "loss: 0.07667524367570877\n",
            "loss: 0.07667477428913116\n",
            "loss: 0.07667412608861923\n",
            "loss: 0.07667360454797745\n",
            "loss: 0.07667308300733566\n",
            "loss: 0.07667248696088791\n",
            "loss: 0.07667206972837448\n",
            "loss: 0.07667192071676254\n",
            "loss: 0.07667149603366852\n",
            "loss: 0.07667098939418793\n",
            "loss: 0.07667078822851181\n",
            "loss: 0.0766701027750969\n",
            "loss: 0.07666966319084167\n",
            "loss: 0.07666917145252228\n",
            "loss: 0.07666870206594467\n",
            "loss: 0.07666835933923721\n",
            "loss: 0.07666781544685364\n",
            "loss: 0.07666759938001633\n",
            "loss: 0.07666700333356857\n",
            "loss: 0.07666663825511932\n",
            "loss: 0.07666613161563873\n",
            "loss: 0.07666561007499695\n",
            "loss: 0.07666530460119247\n",
            "loss: 0.07666481286287308\n",
            "loss: 0.07666429877281189\n",
            "loss: 0.07666391879320145\n",
            "loss: 0.07666347175836563\n",
            "loss: 0.07666315138339996\n",
            "loss: 0.07666265219449997\n",
            "loss: 0.07666216790676117\n",
            "loss: 0.07666172832250595\n",
            "loss: 0.07666131108999252\n",
            "loss: 0.07666096836328506\n",
            "loss: 0.07666046917438507\n",
            "loss: 0.07665997743606567\n",
            "loss: 0.07665955275297165\n",
            "loss: 0.07665899395942688\n",
            "loss: 0.0766586884856224\n",
            "loss: 0.07665827125310898\n",
            "loss: 0.07665786147117615\n",
            "loss: 0.07665741443634033\n",
            "loss: 0.07665695250034332\n",
            "loss: 0.07665643095970154\n",
            "loss: 0.0766560286283493\n",
            "loss: 0.07665565609931946\n",
            "loss: 0.07665511965751648\n",
            "loss: 0.07665470987558365\n",
            "loss: 0.07665416598320007\n",
            "loss: 0.0766538605093956\n",
            "loss: 0.07665329426527023\n",
            "loss: 0.07665291428565979\n",
            "loss: 0.07665252685546875\n",
            "loss: 0.07665198296308517\n",
            "loss: 0.07665154337882996\n",
            "loss: 0.07665123790502548\n",
            "loss: 0.07665061205625534\n",
            "loss: 0.07665025442838669\n",
            "loss: 0.07664987444877625\n",
            "loss: 0.07664921879768372\n",
            "loss: 0.07664897292852402\n",
            "loss: 0.07664849609136581\n",
            "loss: 0.07664825767278671\n",
            "loss: 0.07664752751588821\n",
            "loss: 0.07664725929498672\n",
            "loss: 0.07664678245782852\n",
            "loss: 0.07664647698402405\n",
            "loss: 0.07664591819047928\n",
            "loss: 0.07664551585912704\n",
            "loss: 0.07664505392313004\n",
            "loss: 0.076644666492939\n",
            "loss: 0.07664444297552109\n",
            "loss: 0.07664376497268677\n",
            "loss: 0.07664342224597931\n",
            "loss: 0.07664290815591812\n",
            "loss: 0.0766424685716629\n",
            "loss: 0.07664218544960022\n",
            "loss: 0.07664159685373306\n",
            "loss: 0.07664138823747635\n",
            "loss: 0.07664091885089874\n",
            "loss: 0.07664045691490173\n",
            "loss: 0.07664008438587189\n",
            "loss: 0.07663966715335846\n",
            "loss: 0.07663938403129578\n",
            "loss: 0.07663920521736145\n",
            "loss: 0.07663906365633011\n",
            "loss: 0.07663875073194504\n",
            "loss: 0.07663889229297638\n",
            "loss: 0.07663910835981369\n",
            "loss: 0.07663953304290771\n",
            "loss: 0.07664041221141815\n",
            "loss: 0.07664176821708679\n",
            "loss: 0.0766439363360405\n",
            "loss: 0.07664738595485687\n",
            "loss: 0.07665260136127472\n",
            "loss: 0.07666020095348358\n",
            "loss: 0.07667159289121628\n",
            "loss: 0.07668858021497726\n",
            "loss: 0.07671348750591278\n",
            "loss: 0.07675056159496307\n",
            "loss: 0.07680398970842361\n",
            "loss: 0.07687985152006149\n",
            "loss: 0.07698345929384232\n",
            "loss: 0.07711886614561081\n",
            "loss: 0.07727719098329544\n",
            "loss: 0.07743896543979645\n",
            "loss: 0.07755196839570999\n",
            "loss: 0.0775609239935875\n",
            "loss: 0.07741570472717285\n",
            "loss: 0.07714841514825821\n",
            "loss: 0.07685578614473343\n",
            "loss: 0.07666397839784622\n",
            "loss: 0.07663460820913315\n",
            "loss: 0.07673746347427368\n",
            "loss: 0.07688117772340775\n",
            "loss: 0.07696884125471115\n",
            "loss: 0.07695074379444122\n",
            "loss: 0.07684073597192764\n",
            "loss: 0.07670976221561432\n",
            "loss: 0.07663099467754364\n",
            "loss: 0.07663479447364807\n",
            "loss: 0.07669572532176971\n",
            "loss: 0.07675915956497192\n",
            "loss: 0.07677963376045227\n",
            "loss: 0.07674607634544373\n",
            "loss: 0.07668444514274597\n",
            "loss: 0.07663380354642868\n",
            "loss: 0.07662028819322586\n",
            "loss: 0.07664123177528381\n",
            "loss: 0.07667404413223267\n",
            "loss: 0.07669281959533691\n",
            "loss: 0.07668580859899521\n",
            "loss: 0.0766596645116806\n",
            "loss: 0.07663150876760483\n",
            "loss: 0.07661715149879456\n",
            "loss: 0.0766206756234169\n",
            "loss: 0.07663488388061523\n",
            "loss: 0.07664727419614792\n",
            "loss: 0.07664953172206879\n",
            "loss: 0.07664074748754501\n",
            "loss: 0.0766272023320198\n",
            "loss: 0.07661639153957367\n",
            "loss: 0.07661333680152893\n",
            "loss: 0.07661731541156769\n",
            "loss: 0.07662355154752731\n",
            "loss: 0.0766277089715004\n",
            "loss: 0.07662678509950638\n",
            "loss: 0.07662156969308853\n",
            "loss: 0.07661528885364532\n",
            "loss: 0.07661087810993195\n",
            "loss: 0.07660999149084091\n",
            "loss: 0.07661178708076477\n",
            "loss: 0.07661443948745728\n",
            "loss: 0.07661577314138412\n",
            "loss: 0.07661505043506622\n",
            "loss: 0.07661265134811401\n",
            "loss: 0.07660967856645584\n",
            "loss: 0.07660730183124542\n",
            "loss: 0.07660631835460663\n",
            "loss: 0.07660682499408722\n",
            "loss: 0.07660740613937378\n",
            "loss: 0.07660800218582153\n",
            "loss: 0.076607845723629\n",
            "loss: 0.07660695910453796\n",
            "loss: 0.07660529017448425\n",
            "loss: 0.0766039788722992\n",
            "loss: 0.07660289108753204\n",
            "loss: 0.07660254091024399\n",
            "loss: 0.07660259306430817\n",
            "loss: 0.07660263776779175\n",
            "loss: 0.07660257071256638\n",
            "loss: 0.07660189270973206\n",
            "loss: 0.07660141587257385\n",
            "loss: 0.07660070061683655\n",
            "loss: 0.07659970968961716\n",
            "loss: 0.07659921795129776\n",
            "loss: 0.07659874111413956\n",
            "loss: 0.0765983983874321\n",
            "loss: 0.07659802585840225\n",
            "loss: 0.07659760117530823\n",
            "loss: 0.07659737765789032\n",
            "loss: 0.07659712433815002\n",
            "loss: 0.07659659534692764\n",
            "loss: 0.0765959769487381\n",
            "loss: 0.07659529149532318\n",
            "loss: 0.07659479975700378\n",
            "loss: 0.07659436017274857\n",
            "loss: 0.07659383118152618\n",
            "loss: 0.07659363746643066\n",
            "loss: 0.0765930712223053\n",
            "loss: 0.076592817902565\n",
            "loss: 0.07659227401018143\n",
            "loss: 0.07659180462360382\n",
            "loss: 0.07659129798412323\n",
            "loss: 0.07659086585044861\n",
            "loss: 0.07659053057432175\n",
            "loss: 0.07659008353948593\n",
            "loss: 0.07658953964710236\n",
            "loss: 0.0765891894698143\n",
            "loss: 0.07658863067626953\n",
            "loss: 0.07658826559782028\n",
            "loss: 0.07658762484788895\n",
            "loss: 0.07658736407756805\n",
            "loss: 0.07658690214157104\n",
            "loss: 0.07658646255731583\n",
            "loss: 0.07658619433641434\n",
            "loss: 0.07658557593822479\n",
            "loss: 0.07658515870571136\n",
            "loss: 0.07658467441797256\n",
            "loss: 0.07658429443836212\n",
            "loss: 0.0765838548541069\n",
            "loss: 0.07658346742391586\n",
            "loss: 0.07658298313617706\n",
            "loss: 0.07658259570598602\n",
            "loss: 0.07658210396766663\n",
            "loss: 0.07658179849386215\n",
            "loss: 0.07658112049102783\n",
            "loss: 0.07658085972070694\n",
            "loss: 0.07658031582832336\n",
            "loss: 0.07657983899116516\n",
            "loss: 0.07657948136329651\n",
            "loss: 0.0765790119767189\n",
            "loss: 0.07657849043607712\n",
            "loss: 0.07657825201749802\n",
            "loss: 0.0765775740146637\n",
            "loss: 0.07657735794782639\n",
            "loss: 0.07657693326473236\n",
            "loss: 0.07657629251480103\n",
            "loss: 0.07657603919506073\n",
            "loss: 0.07657556980848312\n",
            "loss: 0.07657509297132492\n",
            "loss: 0.0765746459364891\n",
            "loss: 0.07657425850629807\n",
            "loss: 0.07657382637262344\n",
            "loss: 0.07657334953546524\n",
            "loss: 0.07657287269830704\n",
            "loss: 0.07657238841056824\n",
            "loss: 0.07657201588153839\n",
            "loss: 0.07657160609960556\n",
            "loss: 0.07657115161418915\n",
            "loss: 0.0765705406665802\n",
            "loss: 0.07657023519277573\n",
            "loss: 0.07656966894865036\n",
            "loss: 0.07656944543123245\n",
            "loss: 0.0765688493847847\n",
            "loss: 0.07656837999820709\n",
            "loss: 0.07656802237033844\n",
            "loss: 0.07656750828027725\n",
            "loss: 0.07656709849834442\n",
            "loss: 0.07656672596931458\n",
            "loss: 0.0765661969780922\n",
            "loss: 0.07656578719615936\n",
            "loss: 0.07656537741422653\n",
            "loss: 0.07656502723693848\n",
            "loss: 0.0765644907951355\n",
            "loss: 0.07656415551900864\n",
            "loss: 0.07656380534172058\n",
            "loss: 0.0765635073184967\n",
            "loss: 0.07656300812959671\n",
            "loss: 0.07656262069940567\n",
            "loss: 0.07656219601631165\n",
            "loss: 0.07656222581863403\n",
            "loss: 0.07656200230121613\n",
            "loss: 0.0765620693564415\n",
            "loss: 0.07656270265579224\n",
            "loss: 0.07656353712081909\n",
            "loss: 0.07656531780958176\n",
            "loss: 0.07656845450401306\n",
            "loss: 0.07657355070114136\n",
            "loss: 0.07658184319734573\n",
            "loss: 0.07659556716680527\n",
            "loss: 0.07661786675453186\n",
            "loss: 0.07665382325649261\n",
            "loss: 0.07671137899160385\n",
            "loss: 0.07680171728134155\n",
            "loss: 0.07694078236818314\n",
            "loss: 0.07714329659938812\n",
            "loss: 0.07741855829954147\n",
            "loss: 0.07773476094007492\n",
            "loss: 0.07800961285829544\n",
            "loss: 0.0780719444155693\n",
            "loss: 0.0778060257434845\n",
            "loss: 0.07726160436868668\n",
            "loss: 0.07674570381641388\n",
            "loss: 0.07655200362205505\n",
            "loss: 0.0767199918627739\n",
            "loss: 0.07702748477458954\n",
            "loss: 0.07718772441148758\n",
            "loss: 0.07706861197948456\n",
            "loss: 0.07677958160638809\n",
            "loss: 0.0765705406665802\n",
            "loss: 0.07658378779888153\n",
            "loss: 0.07674555480480194\n",
            "loss: 0.07686819136142731\n",
            "loss: 0.07683011889457703\n",
            "loss: 0.07667920738458633\n",
            "loss: 0.07656006515026093\n",
            "loss: 0.07656439393758774\n",
            "loss: 0.07665327191352844\n",
            "loss: 0.07671702653169632\n",
            "loss: 0.07669156789779663\n",
            "loss: 0.07660781592130661\n",
            "loss: 0.0765482485294342\n",
            "loss: 0.07655729353427887\n",
            "loss: 0.07660697400569916\n",
            "loss: 0.07663723826408386\n",
            "loss: 0.07661760598421097\n",
            "loss: 0.0765712708234787\n",
            "loss: 0.07654234766960144\n",
            "loss: 0.07655095309019089\n",
            "loss: 0.07657790929079056\n",
            "loss: 0.07659158855676651\n",
            "loss: 0.07657875865697861\n",
            "loss: 0.07655324786901474\n",
            "loss: 0.07653901726007462\n",
            "loss: 0.07654455304145813\n",
            "loss: 0.07655912637710571\n",
            "loss: 0.07656564563512802\n",
            "loss: 0.07655830681324005\n",
            "loss: 0.07654446363449097\n",
            "loss: 0.07653642445802689\n",
            "loss: 0.0765388086438179\n",
            "loss: 0.07654630392789841\n",
            "loss: 0.07655006647109985\n",
            "loss: 0.07654654234647751\n",
            "loss: 0.07653899490833282\n",
            "loss: 0.0765342265367508\n",
            "loss: 0.07653454691171646\n",
            "loss: 0.07653798162937164\n",
            "loss: 0.07654053717851639\n",
            "loss: 0.07653918862342834\n",
            "loss: 0.07653507590293884\n",
            "loss: 0.07653173059225082\n",
            "loss: 0.07653100788593292\n",
            "loss: 0.07653248310089111\n",
            "loss: 0.0765339657664299\n",
            "loss: 0.07653365284204483\n",
            "loss: 0.07653182744979858\n",
            "loss: 0.07652945816516876\n",
            "loss: 0.07652831822633743\n",
            "loss: 0.07652849704027176\n",
            "loss: 0.07652909308671951\n",
            "loss: 0.07652933895587921\n",
            "loss: 0.0765283852815628\n",
            "loss: 0.07652700692415237\n",
            "loss: 0.07652594894170761\n",
            "loss: 0.0765254944562912\n",
            "loss: 0.07652547955513\n",
            "loss: 0.07652536779642105\n",
            "loss: 0.07652519643306732\n",
            "loss: 0.07652438431978226\n",
            "loss: 0.07652351260185242\n",
            "loss: 0.0765228196978569\n",
            "loss: 0.07652246952056885\n",
            "loss: 0.07652213424444199\n",
            "loss: 0.07652214169502258\n",
            "loss: 0.07652152329683304\n",
            "loss: 0.07652117311954498\n",
            "loss: 0.07652030885219574\n",
            "loss: 0.07652004808187485\n",
            "loss: 0.07651955634355545\n",
            "loss: 0.07651916891336441\n",
            "loss: 0.07651874423027039\n",
            "loss: 0.07651842385530472\n",
            "loss: 0.07651801407337189\n",
            "loss: 0.07651735097169876\n",
            "loss: 0.07651697844266891\n",
            "loss: 0.07651644945144653\n",
            "loss: 0.07651615887880325\n",
            "loss: 0.07651586085557938\n",
            "loss: 0.07651537656784058\n",
            "loss: 0.07651486247777939\n",
            "loss: 0.07651444524526596\n",
            "loss: 0.07651406526565552\n",
            "loss: 0.0765136256814003\n",
            "loss: 0.07651321589946747\n",
            "loss: 0.07651275396347046\n",
            "loss: 0.0765124037861824\n",
            "loss: 0.07651183009147644\n",
            "loss: 0.07651140540838242\n",
            "loss: 0.07651083171367645\n",
            "loss: 0.07651050388813019\n",
            "loss: 0.07651013135910034\n",
            "loss: 0.07650985568761826\n",
            "loss: 0.07650929689407349\n",
            "loss: 0.07650879770517349\n",
            "loss: 0.07650842517614365\n",
            "loss: 0.0765080451965332\n",
            "loss: 0.07650765031576157\n",
            "loss: 0.07650726288557053\n",
            "loss: 0.07650670409202576\n",
            "loss: 0.07650630176067352\n",
            "loss: 0.07650591433048248\n",
            "loss: 0.07650533318519592\n",
            "loss: 0.07650497555732727\n",
            "loss: 0.07650470733642578\n",
            "loss: 0.07650423794984818\n",
            "loss: 0.0765036791563034\n",
            "loss: 0.07650334388017654\n",
            "loss: 0.07650294154882431\n",
            "loss: 0.07650243490934372\n",
            "loss: 0.07650190591812134\n",
            "loss: 0.0765015110373497\n",
            "loss: 0.07650105655193329\n",
            "loss: 0.07650072872638702\n",
            "loss: 0.07650034874677658\n",
            "loss: 0.076499804854393\n",
            "loss: 0.07649948447942734\n",
            "loss: 0.07649889588356018\n",
            "loss: 0.07649858295917511\n",
            "loss: 0.07649814337491989\n",
            "loss: 0.07649767398834229\n",
            "loss: 0.07649727165699005\n",
            "loss: 0.07649683207273483\n",
            "loss: 0.07649626582860947\n",
            "loss: 0.07649587839841843\n",
            "loss: 0.07649548351764679\n",
            "loss: 0.07649502158164978\n",
            "loss: 0.07649469375610352\n",
            "loss: 0.07649410516023636\n",
            "loss: 0.07649383693933487\n",
            "loss: 0.07649347931146622\n",
            "loss: 0.0764930471777916\n",
            "loss: 0.07649242132902145\n",
            "loss: 0.07649204134941101\n",
            "loss: 0.07649162411689758\n",
            "loss: 0.07649116218090057\n",
            "loss: 0.07649076730012894\n",
            "loss: 0.07649022340774536\n",
            "loss: 0.0764898955821991\n",
            "loss: 0.07648936659097672\n",
            "loss: 0.07648900896310806\n",
            "loss: 0.07648847252130508\n",
            "loss: 0.0764881893992424\n",
            "loss: 0.07648758590221405\n",
            "loss: 0.07648732513189316\n",
            "loss: 0.07648684084415436\n",
            "loss: 0.07648637145757675\n",
            "loss: 0.07648593932390213\n",
            "loss: 0.07648559659719467\n",
            "loss: 0.07648514956235886\n",
            "loss: 0.07648467272520065\n",
            "loss: 0.07648419588804245\n",
            "loss: 0.07648380100727081\n",
            "loss: 0.0764833316206932\n",
            "loss: 0.07648289948701859\n",
            "loss: 0.07648251950740814\n",
            "loss: 0.0764821320772171\n",
            "loss: 0.07648169994354248\n",
            "loss: 0.07648113369941711\n",
            "loss: 0.07648079097270966\n",
            "loss: 0.07648030668497086\n",
            "loss: 0.07647991180419922\n",
            "loss: 0.07647950947284698\n",
            "loss: 0.07647904008626938\n",
            "loss: 0.07647860050201416\n",
            "loss: 0.07647813111543655\n",
            "loss: 0.0764775350689888\n",
            "loss: 0.07647719979286194\n",
            "loss: 0.07647671550512314\n",
            "loss: 0.07647648453712463\n",
            "loss: 0.07647603750228882\n",
            "loss: 0.07647556811571121\n",
            "loss: 0.07647507637739182\n",
            "loss: 0.07647468894720078\n",
            "loss: 0.07647427916526794\n",
            "loss: 0.0764736533164978\n",
            "loss: 0.07647336274385452\n",
            "loss: 0.07647295296192169\n",
            "loss: 0.07647266983985901\n",
            "loss: 0.0764721930027008\n",
            "loss: 0.07647169381380081\n",
            "loss: 0.0764712542295456\n",
            "loss: 0.07647094130516052\n",
            "loss: 0.07647044956684113\n",
            "loss: 0.07647011429071426\n",
            "loss: 0.07646975666284561\n",
            "loss: 0.0764692947268486\n",
            "loss: 0.07646897435188293\n",
            "loss: 0.07646864652633667\n",
            "loss: 0.0764683187007904\n",
            "loss: 0.07646823674440384\n",
            "loss: 0.07646823674440384\n",
            "loss: 0.07646829634904861\n",
            "loss: 0.07646863907575607\n",
            "loss: 0.07646933197975159\n",
            "loss: 0.07647046446800232\n",
            "loss: 0.07647255063056946\n",
            "loss: 0.0764758363366127\n",
            "loss: 0.0764809250831604\n",
            "loss: 0.0764884352684021\n",
            "loss: 0.07650071382522583\n",
            "loss: 0.07651899009943008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check if cos is the same with O.D.E\n",
        "x = torch.unsqueeze(torch.linspace(-3,8, 2000), dim=1) \n",
        "dy = torch.cos(x)\n",
        "\n",
        "method_dy = net1(x)* neural_network(x,weights,bias)\n",
        "method_dy = method_dy.detach().numpy()\n",
        "plt.plot(x.data.numpy(), dy.data.numpy(),'orange', label='ode')\n",
        "plt.plot(x.data.numpy(), method_dy, label='num ode in T')\n",
        "\n",
        "#x = torch.unsqueeze(torch.linspace(-3,3, 2000), dim=1) \n",
        "dy = torch.cos(x)\n",
        "\n",
        "method_dy = net1(x)* neural_network(x,weights,bias) \n",
        "method_dy = method_dy.detach().numpy()\n",
        "plt.plot(x.data.numpy(), dy.data.numpy(), 'orange')\n",
        "plt.plot(x.data.numpy(), method_dy, 'm--', label='num ode out of T T')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "JdsHDesG5LAE",
        "outputId": "115e967d-990b-4474-83a0-20ee87775e7c"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#solution"
      ],
      "metadata": {
        "id": "1_0vzh6v5col"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.Tensor(np.linspace(-3, 3, 300)[:, None]) "
      ],
      "metadata": {
        "id": "qFxR4DF56C1F"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NN = nn.Sequential(nn.Linear(1, 150), nn.Sigmoid(), nn.Linear(150,1))\n",
        "A = neural_network(x[0],weights,bias)\n",
        "Psi_t = lambda x: A + (x+3)*NN(x) #NN is responsible for the solution of O.D.E\n",
        "f = lambda x: net1(x)*neural_network(x,weights,bias) #they are both trained. "
      ],
      "metadata": {
        "id": "_3v883s36au-"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fun(x):\n",
        "\n",
        "    x.requires_grad = True\n",
        "    outputs = Psi_t(x)\n",
        "    Psi_t_x = torch.autograd.grad(outputs, x, grad_outputs=torch.ones_like(outputs),\n",
        "                        create_graph=True)[0]\n",
        "\n",
        "    return  torch.mean( ( Psi_t_x - f(x) )  ** 2)"
      ],
      "metadata": {
        "id": "B3tO1rYN6zLQ"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(NN.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "nBGJbwzO61hf"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def closure():\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    l = loss_fun(x)\n",
        "    print(l)\n",
        "    #l.backward()\n",
        "    l.backward(retain_graph=True)\n",
        "    return l\n",
        "\n",
        "for i in range(5000):\n",
        "    optimizer.step(closure)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tUHfplr637G",
        "outputId": "415fff79-fcf6-4799-ba50-247acc5a2566"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.1066, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1066, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1065, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1065, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1065, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1065, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1065, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1065, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1065, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1065, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1065, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1065, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1065, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1065, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1065, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1065, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1065, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1065, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1065, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1064, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1064, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1064, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1064, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1064, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1064, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1064, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1064, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1064, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1064, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1064, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1064, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1064, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1064, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1064, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1064, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1064, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1064, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1064, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1063, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1063, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1063, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1063, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1063, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1063, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1063, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1063, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1063, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1063, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1063, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1063, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1063, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1063, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1063, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1063, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1063, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1063, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1063, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1062, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1062, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1062, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1062, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1062, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1062, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1062, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1062, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1062, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1062, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1062, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1062, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1062, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1062, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1062, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1062, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1062, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1062, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1062, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1061, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1061, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1061, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1061, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1061, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1061, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1061, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1061, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1061, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1061, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1061, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1061, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1061, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1061, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1061, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1061, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1061, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1061, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1061, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1061, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1062, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1064, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1069, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1079, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1097, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1133, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1198, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1331, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1564, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2030, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2688, grad_fn=<MeanBackward0>)\n",
            "tensor(0.3829, grad_fn=<MeanBackward0>)\n",
            "tensor(0.4443, grad_fn=<MeanBackward0>)\n",
            "tensor(0.4663, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2991, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1402, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1240, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2204, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2590, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1652, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1127, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1678, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1887, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1346, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1180, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1545, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1474, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1142, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1300, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1434, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1160, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1158, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1352, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1194, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1084, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1239, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1203, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1080, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1159, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1176, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1083, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1119, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1155, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1083, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1088, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1134, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1091, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1074, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1109, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1090, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1070, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1095, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1087, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1065, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1083, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1086, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1067, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1073, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1080, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1068, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1069, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1076, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1068, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1065, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1072, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1069, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1064, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1068, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1068, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1064, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1066, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1067, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1064, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1064, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1066, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1064, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1063, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1065, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1064, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1063, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1064, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1064, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1062, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1063, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1063, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1062, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1062, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1063, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1062, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1062, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1062, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1062, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1062, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1062, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1062, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1061, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1061, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1061, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1061, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1061, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1061, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1061, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1061, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1061, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1061, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1061, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1058, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1058, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1058, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1058, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1058, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1058, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1058, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1058, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1058, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1058, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1058, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1058, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1058, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1058, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1058, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1058, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1057, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1057, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1057, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1057, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1057, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1057, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1057, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1057, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1057, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1057, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1057, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1057, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1057, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1057, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1057, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1057, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1057, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1056, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1056, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1056, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1056, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1056, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1056, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1056, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1056, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1056, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1056, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1056, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1056, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1056, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1056, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1056, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1056, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1056, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1056, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1055, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1055, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1055, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1055, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1055, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1055, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1055, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1055, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1055, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1055, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1055, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1055, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1055, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1055, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1055, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1055, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1055, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1055, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1054, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1054, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1054, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1054, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1054, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1054, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1054, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1054, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1054, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1054, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1054, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1054, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1054, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1054, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1054, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1054, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1054, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1054, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1054, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1054, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1053, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1053, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1053, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1053, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1053, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1053, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1053, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1053, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1053, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1053, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1053, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1053, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1053, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1053, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1053, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1053, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1053, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1053, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1053, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1053, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1052, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1052, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1052, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1052, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1052, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1052, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1052, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1052, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1052, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1052, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1052, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1052, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1052, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1052, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1052, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1052, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1052, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1052, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1052, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1053, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1054, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1056, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1064, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1075, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1093, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1126, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1185, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1290, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1476, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1781, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2252, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2811, grad_fn=<MeanBackward0>)\n",
            "tensor(0.3255, grad_fn=<MeanBackward0>)\n",
            "tensor(0.3060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2144, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1206, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1165, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1764, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1900, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1342, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1069, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1415, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1550, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1202, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1085, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1337, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1332, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1092, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1127, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1274, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1164, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1165, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1187, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1074, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1085, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1158, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1104, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1054, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1108, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1116, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1069, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1103, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1074, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1055, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1081, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1080, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1055, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1063, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1076, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1054, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1067, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1064, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1052, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1058, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1063, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1055, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1052, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1057, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1052, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1054, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1057, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1053, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1052, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1055, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1054, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1052, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1053, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1054, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1053, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1052, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1054, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1054, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1054, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1055, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1057, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1061, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1066, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1073, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1083, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1098, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1121, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1155, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1203, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1273, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1362, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1478, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1586, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1679, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1671, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1572, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1363, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1163, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1057, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1084, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1188, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1258, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1238, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1142, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1063, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1112, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1155, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1138, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1087, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1052, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1063, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1095, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1106, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1087, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1058, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1051, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1066, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1081, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1077, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1061, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1050, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1054, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1064, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1068, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1061, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1052, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1050, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1054, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1058, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1054, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1050, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1050, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1053, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1055, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1054, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1051, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1049, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1050, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1051, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1052, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1051, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1050, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1049, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1049, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1050, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1050, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1050, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1049, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1048, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1049, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1049, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1049, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1049, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1049, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1048, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1048, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1048, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1048, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1048, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1048, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1048, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1048, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1048, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1048, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1048, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1048, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1048, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1048, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1047, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1047, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1047, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1047, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1047, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1047, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1047, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1047, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1047, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1047, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1047, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1047, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1047, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1047, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1047, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1047, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1047, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1047, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1047, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1047, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1047, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1046, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1046, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1046, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1046, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1046, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1046, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1046, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1046, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1046, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1046, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1046, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1046, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1046, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1046, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1046, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1046, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1046, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1046, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1046, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1046, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1046, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1045, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1045, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1045, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1045, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1045, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1045, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1045, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1045, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1045, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1045, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1045, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1045, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1045, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1045, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1045, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1045, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1045, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1045, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1045, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1045, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1045, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1045, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1045, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1045, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1046, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1048, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1053, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1062, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1078, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1112, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1174, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1299, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1519, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2574, grad_fn=<MeanBackward0>)\n",
            "tensor(0.3610, grad_fn=<MeanBackward0>)\n",
            "tensor(0.4089, grad_fn=<MeanBackward0>)\n",
            "tensor(0.4153, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2668, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1397, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1338, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2029, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2256, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1621, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1287, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1554, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1622, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1425, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1333, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1321, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1330, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1346, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1257, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1161, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1239, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1288, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1142, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1102, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1239, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1193, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1058, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1140, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1193, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1081, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1080, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1144, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1100, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1071, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1097, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1088, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1077, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1080, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1067, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1071, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1079, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1056, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1074, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1063, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1049, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1062, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1064, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1051, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1053, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1058, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1054, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1051, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1053, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1052, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1052, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1051, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1048, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1050, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1052, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1048, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1048, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1050, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1049, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1047, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1048, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1049, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1047, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1047, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1047, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1047, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1047, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1047, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1046, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1047, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1047, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1046, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1046, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1046, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1046, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1046, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1046, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1046, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1046, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1045, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1045, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1045, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1045, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1045, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1045, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1045, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1045, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1045, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1045, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1045, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1045, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1045, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1043, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1043, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1043, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1043, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1043, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1043, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1043, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1043, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1043, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1043, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1043, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1043, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1043, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1043, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1043, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1043, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1043, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1042, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1042, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1042, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1042, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1042, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1042, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1042, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1042, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1042, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1042, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1042, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1042, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1042, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1042, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1042, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1042, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1042, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1042, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1041, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1041, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1041, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1041, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1041, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1041, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1041, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1041, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1041, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1041, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1041, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1041, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1041, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1041, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1041, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1041, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1041, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1041, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1040, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1040, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1040, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1040, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1040, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1040, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1040, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1040, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1040, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1040, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1040, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1040, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1040, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1040, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1040, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1040, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1040, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1040, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1040, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1039, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1039, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1039, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1039, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1039, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1039, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1039, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1039, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1039, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1039, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1039, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1039, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1039, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1039, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1039, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1039, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1039, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1039, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1039, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1038, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1038, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1038, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1038, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1038, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1038, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1038, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1038, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1038, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1038, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1038, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1038, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1038, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1038, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1038, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1038, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1038, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1038, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1038, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1038, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1037, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1037, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1037, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1037, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1037, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1037, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1037, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1037, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1037, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1037, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1037, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1037, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1037, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1037, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1037, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1037, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1037, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1037, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1037, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1036, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1036, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1036, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1036, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1036, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1036, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1036, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1036, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1036, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1036, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1036, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1036, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1036, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1036, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1036, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1036, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1036, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1036, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1036, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1036, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1035, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1035, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1035, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1035, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1035, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1035, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1035, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1035, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1035, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1035, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1035, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1035, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1035, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1035, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1035, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1035, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1035, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1035, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1035, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1034, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1034, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1034, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1034, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1034, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1034, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1034, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1034, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1034, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1034, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1034, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1034, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1034, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1034, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1034, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1034, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1034, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1034, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1034, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1034, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1034, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1034, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1034, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1035, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1037, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1041, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1047, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1086, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1136, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1237, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1422, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1794, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2387, grad_fn=<MeanBackward0>)\n",
            "tensor(0.3417, grad_fn=<MeanBackward0>)\n",
            "tensor(0.4269, grad_fn=<MeanBackward0>)\n",
            "tensor(0.4776, grad_fn=<MeanBackward0>)\n",
            "tensor(0.3370, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1639, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1166, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2047, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2554, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1790, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1225, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1554, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1783, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1475, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1275, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1401, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1423, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1301, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1258, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1276, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1249, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1219, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1206, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1170, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1168, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1178, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1123, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1119, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1151, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1099, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1093, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1123, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1080, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1083, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1100, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1063, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1074, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1086, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1054, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1062, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1075, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1053, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1050, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1063, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1056, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1042, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1052, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1056, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1041, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1052, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1042, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1041, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1047, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1041, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1040, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1043, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1040, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1040, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1041, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1039, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1038, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1039, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1039, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1038, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1038, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1039, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1037, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1037, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1038, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1037, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1036, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1037, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1036, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1036, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1036, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1036, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1036, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1036, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1036, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1035, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1035, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1035, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1035, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1035, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1035, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1035, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1035, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1035, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1035, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1034, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1034, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1034, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1034, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1034, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1034, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1034, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1034, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1034, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1034, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1034, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1033, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1033, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1033, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1033, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1033, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1033, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1033, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1033, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1033, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1033, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1033, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1033, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1032, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1032, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1032, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1032, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1032, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1032, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1032, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1032, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1032, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1032, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1032, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1032, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1032, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1031, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1031, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1031, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1031, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1031, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1031, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1031, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1031, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1031, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1031, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1031, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1031, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1031, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1030, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1030, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1030, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1030, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1030, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1030, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1030, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1030, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1030, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1030, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1030, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1030, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1030, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1030, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1029, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1029, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1029, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1029, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1029, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1029, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1029, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1029, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1029, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1029, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1029, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1029, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1029, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1028, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1028, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1028, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1028, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1028, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1028, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1028, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1028, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1028, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1028, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1028, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1028, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1028, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1028, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1027, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1027, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1027, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1027, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1027, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1027, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1027, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1027, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1027, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1027, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1027, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1027, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1027, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1026, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1026, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1026, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1026, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1026, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1026, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1026, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1026, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1026, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1026, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1026, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1026, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1026, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1026, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1025, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1025, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1025, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1025, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1025, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1025, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1025, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1025, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1025, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1025, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1025, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1025, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1025, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1025, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1024, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1024, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1024, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1024, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1024, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1024, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1024, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1024, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1024, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1024, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1024, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1024, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1024, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1023, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1023, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1023, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1023, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1023, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1023, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1023, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1023, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1023, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1023, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1023, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1023, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1023, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1023, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1022, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1022, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1022, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1022, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1022, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1022, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1022, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1022, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1022, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1022, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1022, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1022, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1022, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1021, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1021, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1021, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1021, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1021, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1021, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1021, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1021, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1021, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1021, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1021, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1021, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1021, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1020, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1020, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1020, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1020, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1020, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1020, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1020, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1020, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1020, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1020, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1020, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1020, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1020, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1020, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1019, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1019, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1019, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1019, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1019, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1019, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1019, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1020, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1020, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1021, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1022, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1026, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1032, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1068, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1113, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1205, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1371, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1698, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2186, grad_fn=<MeanBackward0>)\n",
            "tensor(0.3022, grad_fn=<MeanBackward0>)\n",
            "tensor(0.3575, grad_fn=<MeanBackward0>)\n",
            "tensor(0.3985, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2917, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1732, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1215, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1585, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2043, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1869, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1468, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1282, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1363, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1538, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1488, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1212, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1146, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1343, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1363, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1145, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1082, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1239, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1255, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1086, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1067, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1179, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1157, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1055, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1063, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1124, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1099, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1043, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1058, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1088, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1065, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1041, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1050, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1053, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1037, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1035, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1048, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1046, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1027, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1030, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1035, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1022, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1030, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1036, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1027, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1023, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1028, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1029, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1025, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1023, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1024, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1026, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1025, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1022, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1022, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1024, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1023, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1020, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1021, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1023, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1021, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1020, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1021, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1021, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1021, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1020, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1020, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1020, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1020, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1020, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1019, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1019, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1020, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1019, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1019, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1019, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1019, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1019, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1018, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1018, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1018, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1018, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1018, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1018, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1018, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1018, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1018, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1017, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1017, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1017, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1017, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1017, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1017, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1017, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1017, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1017, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1017, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1017, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1017, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1016, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1016, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1016, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1016, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1016, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1016, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1016, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1016, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1016, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1016, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1016, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1015, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1015, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1015, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1015, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1015, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1015, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1015, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1015, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1015, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1015, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1015, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1015, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1014, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1014, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1014, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1014, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1014, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1014, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1014, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1014, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1014, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1014, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1014, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1014, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1013, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1013, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1013, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1013, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1013, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1013, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1013, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1013, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1013, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1013, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1013, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1013, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1012, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1012, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1012, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1012, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1012, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1012, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1012, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1012, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1012, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1012, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1012, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1012, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1012, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1011, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1011, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1011, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1011, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1011, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1011, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1011, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1011, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1011, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1011, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1011, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1011, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1010, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1010, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1010, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1010, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1010, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1010, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1010, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1010, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1010, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1010, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1010, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1010, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1010, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1009, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1009, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1009, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1009, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1009, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1009, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1009, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1009, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1009, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1009, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1009, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1009, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1009, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1008, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1008, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1008, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1008, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1008, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1008, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1008, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1008, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1008, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1008, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1008, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1008, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1008, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1007, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1007, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1007, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1007, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1007, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1007, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1007, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1007, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1007, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1007, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1007, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1007, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1007, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1006, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1006, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1006, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1006, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1006, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1006, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1006, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1006, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1006, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1006, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1006, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1006, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1006, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1005, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1005, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1005, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1005, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1005, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1005, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1005, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1005, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1005, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1005, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1005, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1005, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1005, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1005, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1004, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1004, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1004, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1004, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1004, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1004, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1004, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1004, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1004, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1004, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1004, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1004, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1004, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1004, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1004, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1004, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1004, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1004, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1005, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1006, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1008, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1013, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1022, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1041, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1078, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1150, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1300, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1571, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2122, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2911, grad_fn=<MeanBackward0>)\n",
            "tensor(0.4202, grad_fn=<MeanBackward0>)\n",
            "tensor(0.4674, grad_fn=<MeanBackward0>)\n",
            "tensor(0.4322, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2246, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1065, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1749, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2607, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2143, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1162, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1402, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2003, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1566, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1102, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1436, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1558, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1196, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1169, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1367, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1253, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1122, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1204, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1217, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1131, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1117, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1147, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1127, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1085, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1087, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1104, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1073, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1051, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1074, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1066, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1034, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1049, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1058, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1028, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1030, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1048, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1027, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1019, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1038, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1027, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1013, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1029, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1026, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1011, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1021, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1023, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1011, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1016, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1020, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1012, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1012, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1016, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1012, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1011, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1013, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1011, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1010, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1011, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1010, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1010, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1010, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1009, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1009, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1009, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1008, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1008, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1008, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1007, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1007, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1008, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1007, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1007, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1007, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1007, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1006, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1006, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1006, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1006, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1006, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1006, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1006, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1005, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1005, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1005, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1005, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1005, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1005, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1005, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1005, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1005, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1004, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1004, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1004, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1004, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1004, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1004, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1004, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1004, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1004, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1004, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1003, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1003, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1003, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1003, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1003, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1003, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1003, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1003, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1003, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1003, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1003, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1002, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1002, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1002, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1002, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1002, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1002, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1002, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1002, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1002, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1002, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1001, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1001, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1001, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1001, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1001, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1001, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1001, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1001, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1001, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1001, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1001, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1001, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1000, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1000, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1000, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1000, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1000, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1000, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1000, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1000, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1000, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1000, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1000, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1000, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0999, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0999, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0999, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0999, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0999, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0999, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0999, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0999, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0999, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0999, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0999, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0999, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0998, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0998, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0998, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0998, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0998, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0998, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0998, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0998, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0998, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0998, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0998, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0998, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0997, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0997, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0997, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0997, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0997, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0997, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0997, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0997, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0997, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0997, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0997, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0997, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0997, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0996, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0996, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0996, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0996, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0996, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0996, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0996, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0996, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0996, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0996, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0996, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0996, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0996, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0995, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0995, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0995, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0995, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0995, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0995, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0995, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0995, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0995, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0995, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0995, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0995, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0995, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0995, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0994, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0994, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0994, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0994, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0994, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0994, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0994, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0994, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0994, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0994, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0994, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0994, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0994, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0994, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0993, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0993, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0993, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0993, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0993, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0993, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0993, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0993, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0993, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0993, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0993, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0993, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0993, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0993, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0992, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0992, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0992, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0992, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0992, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0992, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0992, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0992, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0992, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0992, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0992, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0992, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0992, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0992, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0992, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0991, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0991, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0991, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0991, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0991, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0991, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0991, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0991, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0991, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0991, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0992, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0993, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0994, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0998, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1006, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1022, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1053, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1112, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1201, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1319, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1373, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1345, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1219, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1141, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1142, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1165, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1137, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1073, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1030, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1024, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1077, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1090, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1067, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1040, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1040, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1046, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1023, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0995, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0998, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1019, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1023, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1015, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1013, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1015, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1013, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1008, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1001, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0994, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0994, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0999, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0999, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0995, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0995, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1000, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1001, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0998, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0995, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0995, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0995, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0994, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0992, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0990, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0990, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0991, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0991, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0990, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0989, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0990, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0991, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0991, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0991, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0991, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0992, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0992, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0992, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0992, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0993, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0993, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0995, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0996, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0998, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1000, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1003, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1008, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1015, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1024, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1038, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1057, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1085, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1122, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1179, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1245, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1343, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1435, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1553, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1599, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1618, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1497, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1329, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1131, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1009, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0998, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1073, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1162, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1188, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1144, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1056, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0996, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0997, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1039, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1078, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1074, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1038, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1000, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0990, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1007, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1030, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1034, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1018, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0997, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0989, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0995, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1007, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1012, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1006, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0996, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0989, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0990, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0995, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1000, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0999, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0994, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0989, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0988, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0989, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0992, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0993, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0992, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0990, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0987, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0987, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0988, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0989, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0990, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0989, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0988, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0987, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0986, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0986, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0987, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0987, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0987, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0987, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0986, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0986, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0986, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0986, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0986, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0986, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0986, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0986, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0985, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0985, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0985, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0985, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0985, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0985, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0985, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0985, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0985, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0985, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0984, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0984, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0984, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0984, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0984, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0984, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0984, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0984, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0984, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0984, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0984, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0984, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0983, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0983, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0983, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0983, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0983, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0983, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0983, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0983, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0983, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0983, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0983, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0983, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0983, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0982, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0982, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0982, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0982, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0982, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0982, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0982, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0982, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0982, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0982, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0982, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0982, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0983, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0983, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0984, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0985, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0987, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0990, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0995, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1003, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1017, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1039, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1077, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1135, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1232, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1361, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1566, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1769, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2057, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2191, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2314, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2087, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1723, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1298, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1083, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1153, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1339, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1430, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1318, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1133, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1053, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1111, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1194, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1181, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1094, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1036, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1049, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1091, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1094, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1053, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1018, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1019, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1049, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1024, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1000, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1003, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1021, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1023, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1002, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0989, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0996, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1009, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1004, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0988, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0984, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0993, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0999, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0992, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0981, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0983, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0990, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0991, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0984, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0979, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0982, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0986, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0985, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0980, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0979, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0981, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0982, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0981, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0979, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0978, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0979, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0980, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0979, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0978, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0978, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0978, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0978, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0977, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0977, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0977, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0977, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0976, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0976, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0976, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0976, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0976, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0975, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0975, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0975, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0975, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0975, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0975, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0974, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0974, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0974, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0974, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0974, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0974, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0974, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0974, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0973, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0973, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0973, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0973, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0973, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0973, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0973, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0973, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0972, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0972, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0972, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0972, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0972, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0972, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0972, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0972, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0972, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0972, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0971, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0971, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0971, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0971, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0971, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0971, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0971, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0971, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0971, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0971, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0971, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0970, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0970, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0970, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0970, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0970, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0970, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0970, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0970, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0970, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0970, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0970, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0970, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0970, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0969, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0969, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0969, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0969, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0969, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0969, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0969, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0969, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0969, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0969, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0969, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0969, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0969, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0969, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0969, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0968, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0968, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0968, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0968, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0968, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0968, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0968, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0968, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0968, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0968, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0968, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0968, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0968, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0968, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0968, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0968, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0967, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0967, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0967, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0967, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0967, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0968, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0968, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0969, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0970, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0973, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0979, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0991, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1014, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1057, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1135, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1243, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1380, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1431, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1402, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1323, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1400, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1625, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1900, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1970, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1904, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1622, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1351, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1124, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1142, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1260, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1306, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1197, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1055, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1009, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1073, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1136, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1111, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1052, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1020, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1013, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1024, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1046, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1048, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1014, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0981, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0988, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1017, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1022, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0997, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0976, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0979, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0995, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0999, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0988, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0976, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0974, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0979, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0984, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0984, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0976, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0969, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0971, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0978, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0979, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0973, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0969, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0970, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0973, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0973, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0971, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0970, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0969, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0969, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0969, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0970, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0969, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0969, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0968, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0968, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0968, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0968, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0968, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0967, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0967, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0967, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0967, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0967, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0967, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0967, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0966, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0966, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0966, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0967, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0967, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0966, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0966, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0966, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0966, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0966, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0966, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0966, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0966, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0966, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0966, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0965, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0965, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0965, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0965, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0965, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0965, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0965, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0965, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0965, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0965, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0965, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0965, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0965, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0965, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0965, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0965, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0965, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0965, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0966, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0969, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0974, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0983, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0999, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1028, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1081, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1171, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1340, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1593, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2040, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2485, grad_fn=<MeanBackward0>)\n",
            "tensor(0.3010, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2815, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2184, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1306, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1046, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1428, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1722, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1528, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1137, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1117, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1350, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1335, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1126, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1079, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1179, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1168, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1058, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1066, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1128, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1068, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1012, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1070, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1007, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0993, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1043, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1038, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0982, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0988, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1027, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1006, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0970, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0985, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1007, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0988, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0969, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0984, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0993, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0976, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0969, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0980, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0980, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0970, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0970, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0976, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0974, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0968, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0970, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0972, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0969, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0967, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0969, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0969, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0967, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0966, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0968, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0968, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0966, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0965, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0967, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0966, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0965, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0965, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0966, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0965, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0965, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0965, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0965, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0966, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0967, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0969, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0973, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0980, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0990, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1005, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1028, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1057, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1094, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1117, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1122, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1085, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1027, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0977, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0987, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1015, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1022, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0999, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0971, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0977, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0991, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0989, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0974, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0966, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0976, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0978, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0971, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0968, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0971, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0967, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0965, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0966, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0965, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0961, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0961, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0961, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0961, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0961, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0961, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0961, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0961, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0961, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0961, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0961, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0966, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0975, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0992, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1026, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1089, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1219, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1447, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1904, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2529, grad_fn=<MeanBackward0>)\n",
            "tensor(0.3540, grad_fn=<MeanBackward0>)\n",
            "tensor(0.3868, grad_fn=<MeanBackward0>)\n",
            "tensor(0.3571, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2002, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1088, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1509, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2077, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1873, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1353, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1313, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1437, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1372, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1399, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1312, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1063, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1215, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1384, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1090, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1023, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1221, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1128, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1058, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1102, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1027, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1068, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1107, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0985, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1007, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1077, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1012, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0995, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1012, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1003, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1015, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0991, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0975, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1004, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0993, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0976, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0984, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0977, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0983, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0983, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0965, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0974, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0982, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0968, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0968, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0972, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0969, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0971, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0967, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0969, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0967, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0965, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0965, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0965, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0965, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0961, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0961, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0961, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0961, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0961, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0961, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0961, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0961, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0961, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0974, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0997, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1142, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1336, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1732, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2396, grad_fn=<MeanBackward0>)\n",
            "tensor(0.3478, grad_fn=<MeanBackward0>)\n",
            "tensor(0.4236, grad_fn=<MeanBackward0>)\n",
            "tensor(0.4121, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2233, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0997, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1684, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2432, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1743, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1037, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1603, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1797, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1141, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1250, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1542, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1156, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1151, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1329, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1138, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1115, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1156, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1119, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1103, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1061, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1081, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1083, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1011, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1064, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0973, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1048, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1033, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0969, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1033, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0998, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0977, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1013, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0983, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0979, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0994, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0976, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0979, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0980, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0973, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0977, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0968, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0975, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0971, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0974, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0966, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0970, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0961, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0967, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0961, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0961, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0961, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0961, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0969, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0986, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1020, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1082, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1199, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1381, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1655, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1899, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2065, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1929, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1660, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1258, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1009, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1069, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1310, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1390, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1167, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0980, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1027, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1163, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1173, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1035, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1038, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1099, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1036, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0991, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1013, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0970, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1004, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1000, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0986, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0986, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0973, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0975, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0968, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0970, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0991, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1036, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1120, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1297, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1600, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2202, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2875, grad_fn=<MeanBackward0>)\n",
            "tensor(0.3743, grad_fn=<MeanBackward0>)\n",
            "tensor(0.3344, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2221, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1139, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1350, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2003, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1700, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1207, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1373, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1451, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1210, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1241, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1312, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1115, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1105, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1241, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1100, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1009, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1152, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1100, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0983, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1081, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1081, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0980, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1038, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1056, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0976, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1009, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1037, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0974, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0986, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1020, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0975, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0970, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1001, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0975, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0986, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0972, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0976, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0967, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0971, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0967, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0961, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0961, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0975, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1003, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1046, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1110, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1161, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1172, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1110, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1066, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1115, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1245, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1346, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1411, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1468, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1591, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1595, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1488, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1268, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1094, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0990, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0976, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1056, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1150, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1155, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0975, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0961, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0995, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1036, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1007, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0982, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1002, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0991, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0966, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0969, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0978, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0968, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0961, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0967, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0985, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1019, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1076, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1184, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1355, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1673, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2062, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2647, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2819, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2658, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1748, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1049, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1099, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1561, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1692, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1280, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1011, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1191, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1349, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1196, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1029, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1093, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1174, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1092, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1019, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1058, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1076, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1030, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1011, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1025, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1023, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1002, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0993, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1000, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0995, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0981, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0981, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0984, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0976, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0970, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0974, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0973, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0970, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0961, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0980, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1008, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1053, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1103, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1142, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1125, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1072, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1040, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1088, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1178, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1249, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1264, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1302, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1346, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1383, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1307, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1194, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1083, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1011, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0965, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1015, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1068, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1072, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1028, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0984, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0969, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0994, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1003, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0984, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0968, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0971, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0973, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0992, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1023, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1071, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1154, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1272, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1472, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1698, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2017, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2148, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2146, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1716, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1226, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0965, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1079, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1337, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1382, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1192, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1003, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1018, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1143, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1159, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0990, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1022, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1066, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1039, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0990, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0988, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1012, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1009, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0980, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0970, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0987, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0991, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0971, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0968, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0980, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0968, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0968, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0965, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0972, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1002, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1060, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1175, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1378, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1770, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2307, grad_fn=<MeanBackward0>)\n",
            "tensor(0.3121, grad_fn=<MeanBackward0>)\n",
            "tensor(0.3393, grad_fn=<MeanBackward0>)\n",
            "tensor(0.3185, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1990, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1235, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1366, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1726, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1807, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1536, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1195, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1180, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1442, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1408, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1066, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1082, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1309, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1175, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1010, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1105, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1143, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1063, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1020, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1042, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1069, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1027, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0976, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1027, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1038, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0961, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0985, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1022, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0968, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0967, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0993, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0973, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0971, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0970, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0965, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0965, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0974, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1013, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1091, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1259, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1563, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2197, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2966, grad_fn=<MeanBackward0>)\n",
            "tensor(0.4030, grad_fn=<MeanBackward0>)\n",
            "tensor(0.3629, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2369, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1143, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1410, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2195, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1921, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1252, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1303, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1599, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1430, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1170, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1262, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1361, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1174, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1099, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1244, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1153, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1018, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1157, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1109, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0984, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1104, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1055, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0975, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1066, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1009, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0979, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1033, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0983, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0980, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1006, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0974, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0972, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0989, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0968, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0967, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0976, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0966, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0961, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0954, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0937, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0937, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0937, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0937, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0937, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0937, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0937, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0937, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0937, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0937, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0937, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0937, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0937, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0937, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0937, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0937, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0937, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0937, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0937, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0937, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0938, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0979, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1020, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1096, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1239, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1472, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1856, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2294, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2846, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2849, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2414, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1407, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1005, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1438, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1782, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1470, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1000, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1136, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1474, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1294, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0966, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1092, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1295, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1118, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1093, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1161, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1008, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0971, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1075, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1057, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0965, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0988, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1037, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0993, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0992, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0999, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0982, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0970, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0966, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0959, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0956, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0957, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0955, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0953, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0949, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0950, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0948, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0947, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0945, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0944, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0943, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0942, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0940, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.unsqueeze(torch.linspace(-4,8, 800), dim=1) \n",
        "y = torch.sin(x)  \n",
        "p_t_second = Psi_t(x)\n",
        "p_t_first = neural_network(x,weights,bias)\n",
        "fig, ax = plt.subplots(dpi=80)\n",
        "ax.scatter(x.data.numpy(),y.data.numpy(), color = \"orange\")\n",
        "ax.plot(x.data.numpy(), p_t_second.data.numpy(), 'g-', lw=3, label ='new method approximation of P(t)')\n",
        "ax.plot(x.data.numpy(), p_t_first.data.numpy(), lw=3, label ='Neural Network approximation of P(t)')\n",
        "\n",
        "ax.vlines(x=5, ymin=-1.5, ymax=1.6, color='black', label='untrained data')\n",
        "\n",
        "\n",
        "#ax.plot(x.data.numpy(), yrk, lw=3, label ='RK approximation of P(t)')\n",
        "\n",
        "plt.legend(loc='best');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "y_JuMqEH66ie",
        "outputId": "bbaadb2d-e0bb-4a82-959f-d295cfc4572a"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 480x320 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RK method\n",
        "x = torch.unsqueeze(torch.linspace(-5,8, 800), dim=1) \n",
        "N = len(x)\n",
        "h = (8+5)/N"
      ],
      "metadata": {
        "id": "S7RsocNF7Rou"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.sin(x)"
      ],
      "metadata": {
        "id": "129X7y5JOZPD"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yrk = np.zeros(int(N))\n",
        "yrk[0]=neural_network(x[0],weights,bias)\n",
        "for i in range(0,int(N-1)):\n",
        "    k1 = net1(x[i])*neural_network(x[i],weights,bias)\n",
        "    k2 = net1(x[i]+(h/2))*neural_network(x[i]+(h/2), weights,bias)\n",
        "    k3 = net1(x[i]+(h/2))*neural_network(x[i]+(h/2), weights,bias)\n",
        "    k4 = net1(x[i]+h)*neural_network(x[i]+h, weights,bias)\n",
        "\n",
        "    yrk[i+1] = yrk[i] + (1/6)*h*(k1+2*k2+2*k3+k4)"
      ],
      "metadata": {
        "id": "ngK8F9MDObE1"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plt.plot(x,yrk, label ='rk', color ='r')\n",
        "plt.plot(x,y, label ='analytical')\n",
        "plt.plot(x.data.numpy(), p_t_second.data.numpy(), 'g-', label ='solution ode with nn')\n",
        "plt.plot(x.data.numpy(), p_t_first.data.numpy(), label ='first NN approximation of P(t)')\n",
        "\n",
        "\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "KzhKkSAYOfKL",
        "outputId": "e7f004c7-1279-4623-c520-a3f74daf1991"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8PHrU6xzPESI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "skake_impl.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}